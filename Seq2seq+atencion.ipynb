{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64fd7bb",
   "metadata": {},
   "source": [
    "## Modelos seq2seq + atención\n",
    "\n",
    "\n",
    "\n",
    "Los modelos de atención han transformado la forma en que abordamos las tareas de traducción automática y otras aplicaciones de procesamiento de lenguaje natural (NLP). Estos modelos permiten que el decodificador se centre en diferentes partes de la secuencia de entrada mientras genera cada palabra de la secuencia de salida.\n",
    "\n",
    "\n",
    "\n",
    "Dos mecanismos importantes en este contexto son la atención global y la atención local. Este cuaderno detalla estos mecanismos, sus implementaciones, y sus aplicaciones prácticas.\n",
    "\n",
    "#### Mecanismo de atención global\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención global, introducido por Bahdanau también conocido como atención suave, permite que el decodificador considere todas las posiciones de la secuencia de entrada al generar cada palabra de la secuencia de salida. Este enfoque asegura que el modelo tenga acceso a toda la información de la entrada en cada paso del proceso de decodificación, mejorando la calidad de la traducción, especialmente en secuencias largas y complejas.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención global se basa en los siguientes pasos y ecuaciones:\n",
    "\n",
    "1. **Cálculo de los puntajes de atención**:\n",
    "   Para cada paso de tiempo del decodificador, se calcula un puntaje de atención $ e_{ij}$ que mide la afinidad entre el estado oculto del decodificador en el paso de tiempo $ j$, denotado como $ s_{j-1}$, y el estado oculto del codificador en el paso de tiempo $ i$, denotado como $ h_i$. Esto se puede calcular usando una red neuronal feedforward con una sola capa oculta (o cualquier otra función de afinidad):\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_{j-1})\n",
    "   $$\n",
    "\n",
    "   donde $W_1$ y $W_2$ son matrices de peso aprendibles y $v$ es un vector de peso aprendible.\n",
    "\n",
    "2. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $ \\alpha_{ij}$, que son distribuciones de probabilidad sobre las posiciones de la secuencia de entrada:\n",
    "\n",
    " $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n",
    "  $$\n",
    "\n",
    "3. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $ c_j$ para cada paso de tiempo del decodificador se calcula como una combinación ponderada de los estados ocultos del codificador:\n",
    "\n",
    " $$\n",
    "   c_j = \\sum_{i=1}^{T_x} \\alpha_{ij} h_i\n",
    " $$\n",
    "\n",
    "4. **Generación de la salida del decodificador**:\n",
    "   Finalmente, el vector de contexto $ c_j$ se combina con el estado oculto del decodificador $ s_j$ para generar la salida $ y_j$:\n",
    "\n",
    " $$\n",
    "   y_j = g(c_j, s_j)\n",
    " $$\n",
    "\n",
    "  donde $ g$ puede ser una función no lineal como una red neuronal.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3787c5",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención global dentro de un modelo seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat([H, encoder_outputs], 2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        energy = torch.bmm(v, energy)\n",
    "        attn_weights = F.softmax(energy.squeeze(1), dim=1)\n",
    "        return attn_weights\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        decoder_outputs, hidden = self.decoder(trg, hidden)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n",
    "        output = torch.cat([hidden.squeeze(0), context.squeeze(1)], 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d178228",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención local\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención local, propuesto por Luong et al. (2015), reduce la complejidad computacional al limitar el alcance de la atención a una ventana local alrededor de cada posición de la secuencia de entrada. Este enfoque es particularmente útil en secuencias largas, donde la atención global puede ser computacionalmente costosa.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención local se define a través de los siguientes pasos:\n",
    "\n",
    "1. **Predicción de la posición de atención**:\n",
    "   Primero, se predice una posición de atención $p_j$ para cada paso de tiempo $j$ del decodificador. Esto puede hacerse mediante una simple función lineal o una red neuronal:\n",
    "   \n",
    "\n",
    "   $$\n",
    "   p_j = S \\cdot \\sigma(W_p s_{j-1})\n",
    "   $$ \n",
    "\n",
    "   donde $S$ es la longitud de la secuencia de entrada, $\\sigma$ es la función sigmoide, $W_p$ es una matriz de peso aprendible, y $s_{j-1}$ es el estado oculto del decodificador en el paso $j-1$.\n",
    "\n",
    "\n",
    "2. **Definición de la ventana local**:\n",
    "   Se define una ventana local de tamaño $2D + 1$ centrada en $p_j$. Los límites de la ventana se calculan como:\n",
    "\n",
    "   $$\n",
    "   [p_j - D, p_j + D]\n",
    "  $$ \n",
    "\n",
    "\n",
    "3. **Cálculo de puntajes de atención dentro de la ventana**:\n",
    "   Los puntajes de atención $e_{ij}$ se calculan solo para las posiciones dentro de la ventana local:\n",
    "\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_{j-1})\n",
    "  $$ \n",
    "\n",
    "\n",
    "4. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $\\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in [p_j - D, p_j + D]} \\exp(e_{ik})}\n",
    "  $$ \n",
    "\n",
    "5. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $c_j$ se calcula como una combinación ponderada de los estados ocultos del codificador dentro de la ventana local:\n",
    "\n",
    "   $$\n",
    "   c_j = \\sum_{i \\in [p_j - D, p_j + D]} \\alpha_{ij} h_i\n",
    "  $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f80f8a",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención local dentro de un modelo seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6146062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, window_size):\n",
    "        super(LocalAttention, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        pos = torch.arange(max_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, max_len, 1)\n",
    "        attn_energies = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        attn_energies = torch.sum(self.v * attn_energies, dim=2)\n",
    "        attn_weights = F.softmax(attn_energies, dim=1)\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "class Seq2SeqWithLocalAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, window_size):\n",
    "        super(Seq2SeqWithLocalAttention, self).__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        self.local_attention = LocalAttention(hidden_dim, window_size)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        decoder_outputs, hidden = self.decoder(trg, hidden)\n",
    "        attn_weights = self.local_attention(hidden, encoder_outputs)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n",
    "        output = torch.cat([hidden.squeeze(0), context.squeeze(1)], 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a0b29",
   "metadata": {},
   "source": [
    "Los mecanismos de atención global y local ofrecen diferentes enfoques para mejorar la generación de secuencias en modelos seq2seq. La atención global proporciona una visión completa de la secuencia de entrada, capturando dependencias a largo plazo, mientras que la atención local mejora la eficiencia computacional al enfocarse en ventanas locales. La elección entre estos mecanismos depende de la naturaleza de la tarea y las prioridades del modelo en términos de precisión y eficiencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81333cd4",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención jerárquica\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "La atención jerárquica se utiliza para manejar estructuras de datos complejas y de múltiples niveles, como documentos largos divididos en párrafos, párrafos divididos en oraciones y oraciones divididas en palabras. Este mecanismo aplica la atención en dos niveles: a nivel de palabra dentro de cada oración y a nivel de oración dentro del documento. Esta estructura permite capturar dependencias tanto locales como globales de manera eficiente.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de atención jerárquica se puede dividir en dos fases principales:\n",
    "\n",
    "1. **Atención a nivel de palabra**:\n",
    "   Primero, se aplica la atención para cada palabra dentro de cada oración. Supongamos que una oración $ \\text{sentence}_i $ contiene $ T_i $ palabras y sus representaciones de palabra son $ \\{h_{i1}, h_{i2}, \\ldots, h_{iT_i}\\} $. La atención a nivel de palabra se calcula de la siguiente manera:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v_1^T \\tanh(W_1 h_{ij} + b_1)\n",
    "   $$\n",
    "\n",
    "   donde $ W_1 $ y $ v_1 $ son parámetros aprendibles, y $ b_1 $ es un vector de sesgo.\n",
    "\n",
    "   Los pesos de atención se obtienen aplicando la función softmax a los puntajes $ e_{ij} $:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_i} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "   El vector de contexto para la oración $ i $ se calcula como una combinación ponderada de las representaciones de palabra:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j=1}^{T_i} \\alpha_{ij} h_{ij}\n",
    "   $$\n",
    "\n",
    "2. **Atención a nivel de oración**:\n",
    "   Una vez obtenidos los vectores de contexto $ \\{c_1, c_2, \\ldots, c_N\\} $ para todas las oraciones en un documento (donde $ N $ es el número de oraciones en el documento), se aplica la atención a nivel de oración:\n",
    "\n",
    "   $$\n",
    "   e_i = v_2^T \\tanh(W_2 c_i + b_2)\n",
    "   $$\n",
    "\n",
    "   Los pesos de atención a nivel de oración se obtienen aplicando la función softmax a los puntajes $ e_i $:\n",
    "\n",
    "   $$\n",
    "   \\beta_i = \\frac{\\exp(e_i)}{\\sum_{k=1}^{N} \\exp(e_k)}\n",
    "   $$\n",
    "\n",
    "   El vector de contexto para el documento se calcula como una combinación ponderada de los vectores de contexto de las oraciones:\n",
    "\n",
    "   $$\n",
    "   d = \\sum_{i=1}^{N} \\beta_i c_i\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb7f8b",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención jerárquica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd757ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalAttention(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sentence_hidden_size, vocab_size, word_embedding_dim):\n",
    "        super(HierarchicalAttention, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.word_encoder = nn.GRU(word_embedding_dim, word_hidden_size, batch_first=True)\n",
    "        self.word_attention = nn.Linear(word_hidden_size, word_hidden_size)\n",
    "        self.sentence_encoder = nn.GRU(word_hidden_size, sentence_hidden_size, batch_first=True)\n",
    "        self.sentence_attention = nn.Linear(sentence_hidden_size, sentence_hidden_size)\n",
    "        self.fc = nn.Linear(sentence_hidden_size, num_classes)  # num_classes depende de la tarea específica\n",
    "\n",
    "    def forward(self, documents):\n",
    "        sentence_vectors = []\n",
    "        for sentences in documents:  # Iterar sobre documentos\n",
    "            word_vectors = []\n",
    "            for sentence in sentences:  # Iterar sobre oraciones\n",
    "                embedded_words = self.word_embedding(sentence)\n",
    "                word_enc_outputs, _ = self.word_encoder(embedded_words)\n",
    "                word_att_weights = F.softmax(self.word_attention(word_enc_outputs), dim=1)\n",
    "                word_vector = torch.sum(word_att_weights * word_enc_outputs, dim=1)\n",
    "                word_vectors.append(word_vector)\n",
    "            sentence_vectors.append(torch.stack(word_vectors))\n",
    "        sentence_enc_outputs, _ = self.sentence_encoder(torch.stack(sentence_vectors))\n",
    "        sentence_att_weights = F.softmax(self.sentence_attention(sentence_enc_outputs), dim=1)\n",
    "        doc_vector = torch.sum(sentence_att_weights * sentence_enc_outputs, dim=1)\n",
    "        output = self.fc(doc_vector)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e7277",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención basada en consultas\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "La atención basada en consultas, utilizada en modelos como el Transformer, utiliza tres componentes principales: consultas (queries), claves (keys) y valores (values). Este mecanismo permite calcular la atención como una función de similitud entre las consultas y las claves, aplicándola a los valores para obtener una representación ponderada. Este enfoque es altamente eficiente y escalable.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención basada en consultas se define a través de los siguientes pasos:\n",
    "\n",
    "1. **Cálculo de consultas, claves y valores**:\n",
    "   Las consultas $ Q$, las claves $ K$ y los valores $ V$ se obtienen mediante proyecciones lineales de la entrada:\n",
    "\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "\n",
    "   donde $ W_Q$, $ W_K$ y $ W_V$ son matrices de peso aprendibles.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Los puntajes de atención se calculan como el producto punto escalado entre las consultas y las claves:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   donde $ d_k$ es la dimensión de las claves.\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $ \\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $ c_i$ se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j} \\alpha_{ij} V_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14003c",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención basada en consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBasedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(QueryBasedAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        Q = self.linear_q(query)\n",
    "        K = self.linear_k(key)\n",
    "        V = self.linear_v(value)\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        output = self.fc_out(attn_output)\n",
    "        return output\n",
    "\n",
    "class Seq2SeqWithQueryBasedAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Seq2SeqWithQueryBasedAttention, self).__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        self.query_attn = QueryBasedAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        enc_output, hidden = self.encoder(src)\n",
    "        dec_output, _ = self.decoder(trg, hidden)\n",
    "        attn_output = self.query_attn(dec_output, enc_output, enc_output)\n",
    "        output = self.fc(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41960b",
   "metadata": {},
   "source": [
    "Los mecanismos de atención jerárquica y basada en consultas ofrecen enfoques avanzados y efectivos para mejorar el rendimiento de los modelos de secuencia a secuencia en diversas tareas de procesamiento de lenguaje natural. La atención jerárquica es ideal para manejar datos estructurados jerárquicamente y capturar dependencias a múltiples niveles, mientras que la atención basada en consultas es altamente eficiente y escalable, lo que la hace fundamental para modelos modernos como el Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bfb10",
   "metadata": {},
   "source": [
    "#### Mecanismo de auto-atención (Self-Attention)\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de auto-atención, o self-attention, permite que cada elemento de la secuencia preste atención a todos los demás elementos de la misma secuencia. Esto es fundamental para capturar las dependencias a largo plazo en las secuencias y es un componente clave en los modelos Transformer.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de auto-atención se puede describir mediante los siguientes pasos:\n",
    "\n",
    "1. **Proyección lineal**:\n",
    "   Al igual que en la atención multi-cabecera, se proyectan las consultas $Q$, las claves $K$ y los valores $V$:\n",
    "\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "\n",
    "   donde $W_Q$, $W_K$ y $W_V$ son matrices de peso aprendibles.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Los puntajes de atención se calculan utilizando el producto punto escalado:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención  $\\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $c_i$ se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j} \\alpha_{ij} V_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2fc8f",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de auto-atención:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.linear_q(x)\n",
    "        K = self.linear_k(x)\n",
    "        V = self.linear_v(x)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = self.fc_out(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d22d58",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención multi-cabecera (Multi-Head Attention)\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención multi-cabecera, introducido por Vaswani en el modelo Transformer, extiende la idea de la auto-atención al permitir que el modelo se concentre en diferentes partes de la secuencia de entrada de manera simultánea y desde múltiples perspectivas. Esto se logra al tener múltiples \"cabeceras\" de atención, cada una de las cuales realiza una operación de atención independiente.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de atención multi-cabecera se puede dividir en varios pasos:\n",
    "\n",
    "1. **Proyección lineal**:\n",
    "   Se proyectan las consultas  $Q$, las claves $K$ y los valores $V$ en subespacios diferentes para cada cabecera de atención. Supongamos que tenemos $h$ cabecera de atención y una dimensión de modelo $d_{\\text{model}}$. La proyección se realiza de la siguiente manera:\n",
    "\n",
    "   $$\n",
    "   Q_h = X W_Q^h, \\quad K_h = X W_K^h, \\quad V_h = X W_V^h\n",
    "   $$\n",
    "\n",
    "   donde $W_Q^h$, $W_K^h$ y $W_V^h$ son matrices de peso específicas para la cabecera  $h$.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Para cada cabecera de atención, se calcula el puntaje de atención utilizando el producto punto escalado:\n",
    "\n",
    "   $$\n",
    "   e_{ij}^h = \\frac{Q_i^h (K_j^h)^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   donde $d_k$ es la dimensión de las claves.\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $\\alpha_{ij}^h$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij}^h = \\frac{\\exp(e_{ij}^h)}{\\sum_{k} \\exp(e_{ik}^h)}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto para cada cabecera de atención se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i^h = \\sum_{j} \\alpha_{ij}^h V_j^h\n",
    "   $$\n",
    "\n",
    "5. **Concatenación y proyección final**:\n",
    "   Los vectores de contexto de todas las cabeceras se concatenan y se proyectan de nuevo en el espacio original:\n",
    "\n",
    "   $$\n",
    "   \\text{MultiHead}(Q, K, V) = \\text{Concat}(c_i^1, c_i^2, \\ldots, c_i^h) W_O\n",
    "   $$\n",
    "\n",
    "   donde  $W_O$ es la matriz de peso de proyección final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d27616",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención multi-cabecera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046437fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Proyección lineal\n",
    "        Q = self.linear_q(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.linear_k(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.linear_v(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Cálculo de puntajes de atención\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        # Normalización de puntajes de atención\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Cálculo del vector de contexto\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Proyección final\n",
    "        output = self.fc_out(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63bbf6",
   "metadata": {},
   "source": [
    "Estos mecanismos son fundamentales para el funcionamiento de los modelos Transformer, permitiendo capturar dependencias a largo plazo y manejar grandes cantidades de datos de manera eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f534",
   "metadata": {},
   "source": [
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d6ef2",
   "metadata": {},
   "source": [
    "1 .Implementa un modelo de atención jerárquica donde primero se aplique atención a nivel de palabra y luego a nivel de frase para tareas de resumen de texto o clasificación de documentos.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Divide un documento en oraciones y cada oración en palabras.\n",
    "- Implementa atención a nivel de palabra para obtener una representación de cada oración.\n",
    "- Implementa atención a nivel de frase para obtener una representación del documento.\n",
    "- Usa la representación del documento para realizar una tarea específica (por ejemplo, clasificación de documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d80f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be317e",
   "metadata": {},
   "source": [
    "2 . Implementa un mecanismo de atención multi-cabecera similar al utilizado en los modelos de transformadores, para permitir que el modelo enfoque en diferentes partes de la entrada de manera simultánea.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa un bloque de atención con múltiples cabeceras.\n",
    "- Integra este bloque en un modelo seq2seq.\n",
    "- Evalua el rendimiento del modelo en una tarea de traducción automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55639b",
   "metadata": {},
   "source": [
    "3 . Implementa un modelo de atención local que solo considere una ventana alrededor de la posición actual en lugar de toda la secuencia de entrada, reduciendo la complejidad computacional.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "* Define una ventana de atención fija.\n",
    "* Implementa el cálculo de los pesos de atención solo dentro de esta ventana.\n",
    "* Integra este mecanismo en un modelo seq2seq y evaluar su rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1112631",
   "metadata": {},
   "source": [
    "4 . Implementa un modelo de atención basada en consultas similar al mecanismo utilizado en el Transformador, donde las consultas, las claves y los valores provienen de proyecciones de la entrada.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa las proyecciones lineales para consultas, claves y valores.\n",
    "- Calcula los pesos de atención usando productos escalares entre las consultas y las claves.\n",
    "- Aplica estos pesos a los valores para obtener la representación de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617dc25",
   "metadata": {},
   "source": [
    "5 . Implementa la auto-atención donde cada elemento de la secuencia presta atención a todos los demás elementos de la misma secuencia. Esto es útil para tareas de traducción automática y clasificación de secuencias.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa el cálculo de auto-atención utilizando proyecciones lineales para claves, consultas y valores.\n",
    "- Integra el mecanismo de auto-atención en un modelo seq2seq.\n",
    "- Evalua el rendimiento del modelo en una tarea de traducción automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
