{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64fd7bb",
   "metadata": {},
   "source": [
    "## Modelos seq2seq + atención\n",
    "\n",
    "\n",
    "Los modelos de atención han transformado la forma en que abordamos las tareas de traducción automática y otras aplicaciones de procesamiento de lenguaje natural (NLP). Estos modelos permiten que el decodificador se centre en diferentes partes de la secuencia de entrada mientras genera cada palabra de la secuencia de salida.\n",
    "\n",
    "\n",
    "\n",
    "Dos mecanismos importantes en este contexto son la atención global y la atención local. Este cuaderno detalla estos mecanismos, sus implementaciones, y sus aplicaciones prácticas.\n",
    "\n",
    "#### Mecanismo de atención global\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/encoder-decoder-attention.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención global, introducido por Bahdanau también conocido como atención suave, permite que el decodificador considere todas las posiciones de la secuencia de entrada al generar cada palabra de la secuencia de salida. Este enfoque asegura que el modelo tenga acceso a toda la información de la entrada en cada paso del proceso de decodificación, mejorando la calidad de la traducción, especialmente en secuencias largas y complejas.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención global se basa en los siguientes pasos y ecuaciones:\n",
    "\n",
    "1. **Cálculo de los puntajes de atención**:\n",
    "   Para cada paso de tiempo del decodificador, se calcula un puntaje de atención $ e_{ij}$ que mide la afinidad entre el estado oculto del decodificador en el paso de tiempo $ j$, denotado como $ s_{j-1}$, y el estado oculto del codificador en el paso de tiempo $ i$, denotado como $ h_i$. Esto se puede calcular usando una red neuronal feedforward con una sola capa oculta (o cualquier otra función de afinidad):\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_{j-1})\n",
    "   $$\n",
    "\n",
    "   donde $W_1$ y $W_2$ son matrices de peso aprendibles y $v$ es un vector de peso aprendible.\n",
    "\n",
    "2. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $ \\alpha_{ij}$, que son distribuciones de probabilidad sobre las posiciones de la secuencia de entrada:\n",
    "\n",
    " $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n",
    "  $$\n",
    "\n",
    "3. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $ c_j$ para cada paso de tiempo del decodificador se calcula como una combinación ponderada de los estados ocultos del codificador:\n",
    "\n",
    " $$\n",
    "   c_j = \\sum_{i=1}^{T_x} \\alpha_{ij} h_i\n",
    " $$\n",
    "\n",
    "4. **Generación de la salida del decodificador**:\n",
    "   Finalmente, el vector de contexto $ c_j$ se combina con el estado oculto del decodificador $ s_j$ para generar la salida $ y_j$:\n",
    "\n",
    " $$\n",
    "   y_j = g(c_j, s_j)\n",
    " $$\n",
    "\n",
    "  donde $ g$ puede ser una función no lineal como una red neuronal.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3787c5",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención global dentro de un modelo seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        # Capa lineal para calcular la energía de la atención\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Vector de parámetros de atención\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(1)  # Longitud de la secuencia de entrada\n",
    "        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)  # Expande el estado oculto del decodificador\n",
    "        \n",
    "        # Calcula la energía de atención\n",
    "        energy = torch.tanh(self.attn(torch.cat([H, encoder_outputs], 2)))\n",
    "        energy = energy.transpose(1, 2)  # Ajusta la dimensión para la multiplicación\n",
    "        \n",
    "        # Calcula los pesos de atención\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # Expande el vector de atención\n",
    "        energy = torch.bmm(v, energy)  # Multiplicación de matrices batch\n",
    "        attn_weights = F.softmax(energy.squeeze(1), dim=1)  # Aplica softmax para normalizar\n",
    "        return attn_weights\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        \n",
    "        # Definición del codificador con GRU\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Definición del decodificador con GRU\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Módulo de atención\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # Capa lineal para generar la salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # Codificación de la secuencia de entrada\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # Decodificación de la secuencia de salida\n",
    "        decoder_outputs, hidden = self.decoder(trg, hidden)\n",
    "        \n",
    "        # Aplicación de la atención\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)  # Multiplicación de los pesos de atención con las salidas del codificador\n",
    "        \n",
    "        # Concatenación del contexto y el estado oculto\n",
    "        output = torch.cat([hidden.squeeze(0), context.squeeze(1)], 1)\n",
    "        output = self.fc(output)  # Capa final para generar la salida\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d178228",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención local\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención local, propuesto por Luong (2015), reduce la complejidad computacional al limitar el alcance de la atención a una ventana local alrededor de cada posición de la secuencia de entrada. Este enfoque es particularmente útil en secuencias largas, donde la atención global puede ser computacionalmente costosa.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención local se define a través de los siguientes pasos:\n",
    "\n",
    "1. **Predicción de la posición de atención**:\n",
    "   Primero, se predice una posición de atención $p_j$ para cada paso de tiempo $j$ del decodificador. Esto puede hacerse mediante una simple función lineal o una red neuronal:\n",
    "   \n",
    "\n",
    "   $$\n",
    "   p_j = S \\cdot \\sigma(W_p s_{j-1})\n",
    "   $$ \n",
    "\n",
    "   donde $S$ es la longitud de la secuencia de entrada, $\\sigma$ es la función sigmoide, $W_p$ es una matriz de peso aprendible, y $s_{j-1}$ es el estado oculto del decodificador en el paso $j-1$.\n",
    "\n",
    "\n",
    "2. **Definición de la ventana local**:\n",
    "   Se define una ventana local de tamaño $2D + 1$ centrada en $p_j$. Los límites de la ventana se calculan como:\n",
    "\n",
    "   $$\n",
    "   [p_j - D, p_j + D]\n",
    "  $$ \n",
    "\n",
    "\n",
    "3. **Cálculo de puntajes de atención dentro de la ventana**:\n",
    "   Los puntajes de atención $e_{ij}$ se calculan solo para las posiciones dentro de la ventana local:\n",
    "\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_{j-1})\n",
    "  $$ \n",
    "\n",
    "\n",
    "4. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $\\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in [p_j - D, p_j + D]} \\exp(e_{ik})}\n",
    "  $$ \n",
    "\n",
    "5. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $c_j$ se calcula como una combinación ponderada de los estados ocultos del codificador dentro de la ventana local:\n",
    "\n",
    "   $$\n",
    "   c_j = \\sum_{i \\in [p_j - D, p_j + D]} \\alpha_{ij} h_i\n",
    "  $$ \n",
    "\n",
    "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/luong2015-fig2-3.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f80f8a",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención local dentro de un modelo seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6146062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, window_size):\n",
    "        super(LocalAttention, self).__init__()\n",
    "        self.window_size = window_size  # Tamaño de la ventana de atención local\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  # Capa lineal para calcular energía de atención\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))  # Vector de parámetros de atención\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)  # Tamaño del batch\n",
    "        max_len = encoder_outputs.size(1)  # Longitud máxima de la secuencia de entrada\n",
    "        hidden = hidden.squeeze(0)  # Ajusta la dimensión del estado oculto\n",
    "        \n",
    "        # Posiciones de las palabras en la secuencia\n",
    "        pos = torch.arange(max_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        # Expande el estado oculto del decodificador\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, max_len, 1)\n",
    "        \n",
    "        # Calcula la energía de la atención\n",
    "        attn_energies = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        attn_energies = torch.sum(self.v * attn_energies, dim=2)  # Ponderación con el vector de atención\n",
    "        \n",
    "        # Calcula los pesos de atención aplicando softmax\n",
    "        attn_weights = F.softmax(attn_energies, dim=1)\n",
    "        \n",
    "        return attn_weights\n",
    "\n",
    "class Seq2SeqWithLocalAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, window_size):\n",
    "        super(Seq2SeqWithLocalAttention, self).__init__()\n",
    "        \n",
    "        # Definición del codificador con GRU\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Definición del decodificador con GRU\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Módulo de atención local\n",
    "        self.local_attention = LocalAttention(hidden_dim, window_size)\n",
    "        \n",
    "        # Capa lineal para generar la salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # Codificación de la secuencia de entrada\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # Decodificación de la secuencia de salida\n",
    "        decoder_outputs, hidden = self.decoder(trg, hidden)\n",
    "        \n",
    "        # Aplicación de la atención local\n",
    "        attn_weights = self.local_attention(hidden, encoder_outputs)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)  # Multiplicación de los pesos de atención con las salidas del codificador\n",
    "        \n",
    "        # Concatenación del contexto y el estado oculto\n",
    "        output = torch.cat([hidden.squeeze(0), context.squeeze(1)], 1)\n",
    "        output = self.fc(output)  # Capa final para generar la salida\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81333cd4",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención jerárquica\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "La atención jerárquica se utiliza para manejar estructuras de datos complejas y de múltiples niveles, como documentos largos divididos en párrafos, párrafos divididos en oraciones y oraciones divididas en palabras. Este mecanismo aplica la atención en dos niveles: a nivel de palabra dentro de cada oración y a nivel de oración dentro del documento. Esta estructura permite capturar dependencias tanto locales como globales de manera eficiente.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de atención jerárquica se puede dividir en dos fases principales:\n",
    "\n",
    "1. **Atención a nivel de palabra**:\n",
    "   Primero, se aplica la atención para cada palabra dentro de cada oración. Supongamos que una oración $ \\text{sentence}_i $ contiene $ T_i $ palabras y sus representaciones de palabra son $ \\{h_{i1}, h_{i2}, \\ldots, h_{iT_i}\\} $. La atención a nivel de palabra se calcula de la siguiente manera:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v_1^T \\tanh(W_1 h_{ij} + b_1)\n",
    "   $$\n",
    "\n",
    "   donde $ W_1 $ y $ v_1 $ son parámetros aprendibles, y $ b_1 $ es un vector de sesgo.\n",
    "\n",
    "   Los pesos de atención se obtienen aplicando la función softmax a los puntajes $ e_{ij} $:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_i} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "   El vector de contexto para la oración $ i $ se calcula como una combinación ponderada de las representaciones de palabra:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j=1}^{T_i} \\alpha_{ij} h_{ij}\n",
    "   $$\n",
    "\n",
    "2. **Atención a nivel de oración**:\n",
    "   Una vez obtenidos los vectores de contexto $ \\{c_1, c_2, \\ldots, c_N\\} $ para todas las oraciones en un documento (donde $ N $ es el número de oraciones en el documento), se aplica la atención a nivel de oración:\n",
    "\n",
    "   $$\n",
    "   e_i = v_2^T \\tanh(W_2 c_i + b_2)\n",
    "   $$\n",
    "\n",
    "   Los pesos de atención a nivel de oración se obtienen aplicando la función softmax a los puntajes $ e_i $:\n",
    "\n",
    "   $$\n",
    "   \\beta_i = \\frac{\\exp(e_i)}{\\sum_{k=1}^{N} \\exp(e_k)}\n",
    "   $$\n",
    "\n",
    "   El vector de contexto para el documento se calcula como una combinación ponderada de los vectores de contexto de las oraciones:\n",
    "\n",
    "   $$\n",
    "   d = \\sum_{i=1}^{N} \\beta_i c_i\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb7f8b",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención jerárquica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd757ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HierarchicalAttention(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sentence_hidden_size, vocab_size, word_embedding_dim, num_classes):\n",
    "        super(HierarchicalAttention, self).__init__()\n",
    "        \n",
    "        # Capa de embeddings para representar palabras en un espacio de menor dimensión\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        \n",
    "        # Codificador a nivel de palabra con una GRU\n",
    "        self.word_encoder = nn.GRU(word_embedding_dim, word_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Módulo de atención para dar peso a cada palabra\n",
    "        self.word_attention = nn.Linear(word_hidden_size, word_hidden_size)\n",
    "        \n",
    "        # Codificador a nivel de oración con una GRU\n",
    "        self.sentence_encoder = nn.GRU(word_hidden_size, sentence_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Módulo de atención para dar peso a cada oración\n",
    "        self.sentence_attention = nn.Linear(sentence_hidden_size, sentence_hidden_size)\n",
    "        \n",
    "        # Capa lineal final para clasificación u otras tareas\n",
    "        self.fc = nn.Linear(sentence_hidden_size, num_classes)  # num_classes depende de la tarea específica\n",
    "\n",
    "    def forward(self, documents):\n",
    "        sentence_vectors = []  # Lista para almacenar los vectores de cada oración\n",
    "        \n",
    "        for sentences in documents:  # Iterar sobre cada documento\n",
    "            word_vectors = []  # Lista para almacenar los vectores de palabras dentro de una oración\n",
    "            \n",
    "            for sentence in sentences:  # Iterar sobre cada oración en el documento\n",
    "                \n",
    "                # Obtener las representaciones de las palabras mediante embeddings\n",
    "                embedded_words = self.word_embedding(sentence)\n",
    "                \n",
    "                # Aplicar la GRU a nivel de palabra\n",
    "                word_enc_outputs, _ = self.word_encoder(embedded_words)\n",
    "                \n",
    "                # Calcular pesos de atención a nivel de palabra\n",
    "                word_att_weights = F.softmax(self.word_attention(word_enc_outputs), dim=1)\n",
    "                \n",
    "                # Obtener el vector de la oración ponderando las palabras\n",
    "                word_vector = torch.sum(word_att_weights * word_enc_outputs, dim=1)\n",
    "                word_vectors.append(word_vector)\n",
    "            \n",
    "            # Almacenar los vectores de oraciones en la lista de vectores de oraciones\n",
    "            sentence_vectors.append(torch.stack(word_vectors))\n",
    "        \n",
    "        # Aplicar la GRU a nivel de oración\n",
    "        sentence_enc_outputs, _ = self.sentence_encoder(torch.stack(sentence_vectors))\n",
    "        \n",
    "        # Calcular pesos de atención a nivel de oración\n",
    "        sentence_att_weights = F.softmax(self.sentence_attention(sentence_enc_outputs), dim=1)\n",
    "        \n",
    "        # Obtener el vector del documento ponderando las oraciones\n",
    "        doc_vector = torch.sum(sentence_att_weights * sentence_enc_outputs, dim=1)\n",
    "        \n",
    "        # Aplicar la capa final para obtener la salida\n",
    "        output = self.fc(doc_vector)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e7277",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención basada en consultas\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "La atención basada en consultas, utilizada en modelos como el Transformer, utiliza tres componentes principales: consultas (queries), claves (keys) y valores (values). Este mecanismo permite calcular la atención como una función de similitud entre las consultas y las claves, aplicándola a los valores para obtener una representación ponderada. Este enfoque es altamente eficiente y escalable.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención basada en consultas se define a través de los siguientes pasos:\n",
    "\n",
    "1. **Cálculo de consultas, claves y valores**:\n",
    "   Las consultas $ Q$, las claves $ K$ y los valores $ V$ se obtienen mediante proyecciones lineales de la entrada:\n",
    "\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "\n",
    "   donde $ W_Q$, $ W_K$ y $ W_V$ son matrices de peso aprendibles.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Los puntajes de atención se calculan como el producto punto escalado entre las consultas y las claves:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   donde $ d_k$ es la dimensión de las claves.\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $ \\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $ c_i$ se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j} \\alpha_{ij} V_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14003c",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención basada en consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class QueryBasedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(QueryBasedAttention, self).__init__()\n",
    "        \n",
    "        # Proyecciones lineales para query, key y value\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)  # Proyección de la consulta (query)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)  # Proyección de la clave (key)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)  # Proyección del valor (value)\n",
    "        \n",
    "        # Capa lineal final para procesar la salida de la atención\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Proyectar query, key y value en espacios latentes\n",
    "        Q = self.linear_q(query)\n",
    "        K = self.linear_k(key)\n",
    "        V = self.linear_v(value)\n",
    "        \n",
    "        # Calcular los puntajes de atención mediante el producto punto escalado\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        \n",
    "        # Aplicar softmax para obtener los pesos de atención\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiplicar los pesos de atención con los valores (V)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Aplicar la capa final para generar la salida\n",
    "        output = self.fc_out(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Seq2SeqWithQueryBasedAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Seq2SeqWithQueryBasedAttention, self).__init__()\n",
    "        \n",
    "        # Definición del codificador con GRU\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Definición del decodificador con GRU\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Módulo de atención basado en query\n",
    "        self.query_attn = QueryBasedAttention(hidden_dim)\n",
    "        \n",
    "        # Capa lineal final para generar la salida\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # Codificación de la secuencia de entrada\n",
    "        enc_output, hidden = self.encoder(src)\n",
    "        \n",
    "        # Decodificación de la secuencia de salida\n",
    "        dec_output, _ = self.decoder(trg, hidden)\n",
    "        \n",
    "        # Aplicación de la atención basada en query\n",
    "        attn_output = self.query_attn(dec_output, enc_output, enc_output)\n",
    "        \n",
    "        # Generar la salida final con la capa lineal\n",
    "        output = self.fc(attn_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bfb10",
   "metadata": {},
   "source": [
    "#### Mecanismo de auto-atención (Self-Attention)\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de auto-atención, o self-attention, permite que cada elemento de la secuencia preste atención a todos los demás elementos de la misma secuencia. Esto es fundamental para capturar las dependencias a largo plazo en las secuencias y es un componente clave en los modelos Transformer.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de auto-atención se puede describir mediante los siguientes pasos:\n",
    "\n",
    "1. **Proyección lineal**:\n",
    "   Al igual que en la atención multi-cabecera, se proyectan las consultas $Q$, las claves $K$ y los valores $V$:\n",
    "\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "\n",
    "   donde $W_Q$, $W_K$ y $W_V$ son matrices de peso aprendibles.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Los puntajes de atención se calculan utilizando el producto punto escalado:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención  $\\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $c_i$ se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j} \\alpha_{ij} V_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2fc8f",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de auto-atención:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Proyecciones lineales para query (Q), key (K) y value (V)\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)  # Proyección de la consulta (query)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)  # Proyección de la clave (key)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)  # Proyección del valor (value)\n",
    "        \n",
    "        # Capa lineal final para procesar la salida de la atención\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Proyectar la entrada en query (Q), key (K) y value (V)\n",
    "        Q = self.linear_q(x)\n",
    "        K = self.linear_k(x)\n",
    "        V = self.linear_v(x)\n",
    "\n",
    "        # Calcular los puntajes de atención usando el producto punto escalado\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        \n",
    "        # Aplicar softmax para obtener los pesos de atención\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiplicar los pesos de atención con los valores (V)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Aplicar la capa lineal final para generar la salida\n",
    "        output = self.fc_out(attn_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d22d58",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención multi-cabecera (Multi-Head Attention)\n",
    "\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png\" width=\"600\">\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención multi-cabecera, introducido por Vaswani en el modelo Transformer, extiende la idea de la auto-atención al permitir que el modelo se concentre en diferentes partes de la secuencia de entrada de manera simultánea y desde múltiples perspectivas. Esto se logra al tener múltiples \"cabeceras\" de atención, cada una de las cuales realiza una operación de atención independiente.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de atención multi-cabecera se puede dividir en varios pasos:\n",
    "\n",
    "1. **Proyección lineal**:\n",
    "   Se proyectan las consultas  $Q$, las claves $K$ y los valores $V$ en subespacios diferentes para cada cabecera de atención. Supongamos que tenemos $h$ cabecera de atención y una dimensión de modelo $d_{\\text{model}}$. La proyección se realiza de la siguiente manera:\n",
    "\n",
    "   $$\n",
    "   Q_h = X W_Q^h, \\quad K_h = X W_K^h, \\quad V_h = X W_V^h\n",
    "   $$\n",
    "\n",
    "   donde $W_Q^h$, $W_K^h$ y $W_V^h$ son matrices de peso específicas para la cabecera  $h$.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Para cada cabecera de atención, se calcula el puntaje de atención utilizando el producto punto escalado:\n",
    "\n",
    "   $$\n",
    "   e_{ij}^h = \\frac{Q_i^h (K_j^h)^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   donde $d_k$ es la dimensión de las claves.\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $\\alpha_{ij}^h$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij}^h = \\frac{\\exp(e_{ij}^h)}{\\sum_{k} \\exp(e_{ik}^h)}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto para cada cabecera de atención se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i^h = \\sum_{j} \\alpha_{ij}^h V_j^h\n",
    "   $$\n",
    "\n",
    "5. **Concatenación y proyección final**:\n",
    "   Los vectores de contexto de todas las cabeceras se concatenan y se proyectan de nuevo en el espacio original:\n",
    "\n",
    "   $$\n",
    "   \\text{MultiHead}(Q, K, V) = \\text{Concat}(c_i^1, c_i^2, \\ldots, c_i^h) W_O\n",
    "   $$\n",
    "\n",
    "   donde  $W_O$ es la matriz de peso de proyección final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d27616",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención multi-cabecera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046437fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # Verifica que el tamaño oculto sea divisible entre el número de cabezas\n",
    "        assert hidden_size % num_heads == 0\n",
    "        \n",
    "        self.num_heads = num_heads  # Número de cabezas de atención\n",
    "        self.head_dim = hidden_size // num_heads  # Dimensión de cada cabeza\n",
    "\n",
    "        # Proyecciones lineales para query (Q), key (K) y value (V)\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)  # Proyección de la consulta\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)  # Proyección de la clave\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)  # Proyección del valor\n",
    "\n",
    "        # Capa lineal final después de la atención multi-cabeza\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)  # Obtiene el tamaño del batch\n",
    "\n",
    "        # Aplicar proyección lineal y dividir en múltiples cabezas\n",
    "        Q = self.linear_q(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.linear_k(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.linear_v(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Calcular puntajes de atención escalados\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        # Aplicar softmax para normalizar los puntajes de atención\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiplicar los pesos de atención con los valores (V) para calcular el vector de contexto\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reorganizar las dimensiones y concatenar las cabezas de atención\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Aplicar la proyección final para combinar la información de todas las cabezas\n",
    "        output = self.fc_out(attn_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63bbf6",
   "metadata": {},
   "source": [
    "Estos mecanismos son fundamentales para el funcionamiento de los modelos Transformer, permitiendo capturar dependencias a largo plazo y manejar grandes cantidades de datos de manera eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f534",
   "metadata": {},
   "source": [
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d6ef2",
   "metadata": {},
   "source": [
    "1 .Implementa un modelo de atención jerárquica donde primero se aplique atención a nivel de palabra y luego a nivel de frase para tareas de resumen de texto o clasificación de documentos.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Divide un documento en oraciones y cada oración en palabras.\n",
    "- Implementa atención a nivel de palabra para obtener una representación de cada oración.\n",
    "- Implementa atención a nivel de frase para obtener una representación del documento.\n",
    "- Usa la representación del documento para realizar una tarea específica (por ejemplo, clasificación de documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d80f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be317e",
   "metadata": {},
   "source": [
    "2 . Implementa un mecanismo de atención multi-cabecera similar al utilizado en los modelos de transformadores, para permitir que el modelo enfoque en diferentes partes de la entrada de manera simultánea.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa un bloque de atención con múltiples cabeceras.\n",
    "- Integra este bloque en un modelo seq2seq.\n",
    "- Evalua el rendimiento del modelo en una tarea de traducción automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55639b",
   "metadata": {},
   "source": [
    "3 . Implementa un modelo de atención local que solo considere una ventana alrededor de la posición actual en lugar de toda la secuencia de entrada, reduciendo la complejidad computacional.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "* Define una ventana de atención fija.\n",
    "* Implementa el cálculo de los pesos de atención solo dentro de esta ventana.\n",
    "* Integra este mecanismo en un modelo seq2seq y evaluar su rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1112631",
   "metadata": {},
   "source": [
    "4 . Implementa un modelo de atención basada en consultas similar al mecanismo utilizado en el Transformador, donde las consultas, las claves y los valores provienen de proyecciones de la entrada.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa las proyecciones lineales para consultas, claves y valores.\n",
    "- Calcula los pesos de atención usando productos escalares entre las consultas y las claves.\n",
    "- Aplica estos pesos a los valores para obtener la representación de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617dc25",
   "metadata": {},
   "source": [
    "5 . Implementa la auto-atención donde cada elemento de la secuencia presta atención a todos los demás elementos de la misma secuencia. Esto es útil para tareas de traducción automática y clasificación de secuencias.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa el cálculo de auto-atención utilizando proyecciones lineales para claves, consultas y valores.\n",
    "- Integra el mecanismo de auto-atención en un modelo seq2seq.\n",
    "- Evalua el rendimiento del modelo en una tarea de traducción automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
