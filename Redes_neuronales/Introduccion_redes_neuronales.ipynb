{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKOq7nJ6WcCd"
   },
   "source": [
    "## Redes neuronales\n",
    "\n",
    "Una familia de algoritmos conocida como **redes neuronales** ha experimentado recientemente un renacimiento bajo el nombre de **aprendizaje profundo**. Si bien el aprendizaje profundo ha demostrado un gran potencial en muchas aplicaciones de *machine learning*, sus algoritmos suelen diseñarse con mucho cuidado para casos de uso específicos.\n",
    "\n",
    "En este contexto, utilizaremos **perceptrones multicapa** para tareas de clasificación y regresión, los cuales pueden servir como punto de partida para métodos de aprendizaje profundo más especializados. Los perceptrones multicapa (*Multilayer Perceptrons*, MLP) también se conocen como **redes neuronales de retroalimentación** o, en algunos casos, simplemente como **redes neuronales**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiFDBbV9WcCh"
   },
   "source": [
    "### El modelo de redes neuronales\n",
    "\n",
    "Los **perceptrones multicapa** (*MLP*) pueden verse como una generalización de los modelos lineales, en los que se realizan múltiples etapas de procesamiento antes de tomar una decisión.\n",
    "\n",
    "Recordemos que la predicción en un regresor lineal se expresa como:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w[0] \\cdot x[0] + w[1] \\cdot x[1] + \\dots + w[p] \\cdot x[p] + b\n",
    "$$\n",
    "\n",
    "En otras palabras, $\\hat{y}$ es una suma ponderada de las características de entrada, `x[0]` a `x[p]`, multiplicadas por los coeficientes aprendidos, `w[0]` a `w[p]`, más un sesgo `b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dibuja_grafo_regresion_logistica.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-13ht_NxWcC3",
    "outputId": "7c86a67d-499a-4c92-be3c-4d644bff5419"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "display(dibuja_grafo_regresion_logistica())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiTsZ6L6WcDH"
   },
   "source": [
    "En la imagen, cada nodo de la izquierda representa una **característica de entrada** (`x[0]` a `x[3]`). Las **líneas de conexión** simbolizan los **pesos aprendidos** (`w[0]` a `w[3]`), que ponderan la contribución de cada característica. Finalmente, el nodo de la derecha representa la **salida** (`y`), que se obtiene como una **suma ponderada** de las entradas más un posible término de sesgo (*bias*).\n",
    "\n",
    "En un perceptrón multicapa (*MLP*), este proceso se extiende mediante **capas ocultas**, donde las unidades ocultas reciben entradas ponderadas, aplican funciones de activación y transmiten sus resultados a la siguiente capa. Este encadenamiento de operaciones permite que la red aprenda representaciones más complejas antes de producir la salida final.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dibuja_grafo_unica_capa_oculta.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mY9TilIpWcDa",
    "outputId": "abef7981-9398-4046-cd2c-c45b3976d446"
   },
   "outputs": [],
   "source": [
    "dibuja_grafo_unica_capa_oculta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztGgVaeqWcDp"
   },
   "source": [
    "Este modelo tiene **más coeficientes** (también llamados **pesos**) que un modelo lineal simple: hay un peso para cada conexión entre las **entradas** (`x[0]` a `x[3]`) y cada **neurona de la capa oculta** (`h[0]` a `h[2]`), así como otro conjunto de pesos entre la **capa oculta** y la **salida** (`y`).\n",
    "\n",
    "Matemáticamente, calcular múltiples **sumas ponderadas** en cada capa sigue siendo un cálculo lineal. Para que este modelo sea realmente **más expresivo** que un modelo lineal, se necesita un elemento adicional: **una función de activación no lineal**. Una vez que se calcula la suma ponderada en cada neurona oculta, se aplica una **función de activación**, que introduce **no linealidad** en la red.\n",
    "\n",
    "Las funciones de activación más comunes son:\n",
    "- **Rectified Linear Unit (ReLU)**: Define $\\text{ReLU}(z) = \\max(0, z)$. **Anula valores negativos** estableciéndolos en cero, lo que mejora la capacidad de la red para aprender relaciones complejas sin saturarse.\n",
    "- **Tangente hiperbólica (tanh)**: Define $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$. **Satura** los valores de entrada, llevándolos a -1 para valores bajos y +1 para valores altos.\n",
    "\n",
    "El resultado de la **función de activación** en la capa oculta se utiliza posteriormente en la siguiente **suma ponderada**, que finalmente calcula la salida $\\hat{y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mfNWKUCWcDq",
    "outputId": "61ef1fd5-58fd-4948-88e7-dc6e73df3abf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "linea = np.linspace(-3, 3, 100)\n",
    "plt.plot(linea, np.tanh(linea), label=\"tanh\")\n",
    "plt.plot(linea, np.maximum(linea, 0), label=\"relu\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"relu(x), tanh(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz38PsUmWcD1"
   },
   "source": [
    "\n",
    "Para la pequeña red neuronal representada, la fórmula completa para calcular $\\hat{y}$ en el caso de **regresión** es:\n",
    "\n",
    "$$\n",
    "h[0] = \\tanh(w[0,0] \\cdot x[0] + w[1,0] \\cdot x[1] + w[2,0] \\cdot x[2] + w[3,0] \\cdot x[3] + b_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h[1] = \\tanh(w[0,1] \\cdot x[0] + w[1,1] \\cdot x[1] + w[2,1] \\cdot x[2] + w[3,1] \\cdot x[3] + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h[2] = \\tanh(w[0,2] \\cdot x[0] + w[1,2] \\cdot x[1] + w[2,2] \\cdot x[2] + w[3,2] \\cdot x[3] + b_2)\n",
    "$$\n",
    "\n",
    "Por lo que:\n",
    "\n",
    "$$\n",
    "\\hat{y} = v[0] \\cdot h[0] + v[1] \\cdot h[1] + v[2] \\cdot h[2] + b_y\n",
    "$$\n",
    "\n",
    "Aquí:\n",
    "- **`w[i,j]`** representa los pesos entre la entrada `x[i]` y la neurona oculta `h[j]`.\n",
    "- **`b_j`** es el término de sesgo asociado a la neurona oculta `h[j]`.\n",
    "- **`v[j]`** representa los pesos entre la capa oculta `h[j]` y la salida $\\hat{y}$.\n",
    "- **`b_y`** es el término de sesgo en la capa de salida.\n",
    "- **`x[i]`** son las características de entrada.\n",
    "- **$\\hat{y}$** es la salida calculada.\n",
    "- **`h[j]`** representa los valores intermedios de la capa oculta después de aplicar la activación `tanh`.\n",
    "\n",
    "Un parámetro importante que debe establecer el usuario es la **cantidad de nodos en la capa oculta**. Este valor puede ser tan pequeño como `10` para conjuntos de datos simples o pequeños, y llegar hasta `10,000` para datos más complejos. Además, es posible agregar **capas ocultas adicionales**, aumentando así la capacidad del modelo para capturar relaciones más complejas en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dibuja_grafo_dos_capa_oculta.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r3CJk0AWcEB",
    "outputId": "29cba518-99e9-4ad9-e518-90a5d2b43859"
   },
   "outputs": [],
   "source": [
    "dibuja_grafo_dos_capa_oculta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTZYHn4iWcEI"
   },
   "source": [
    "La imagen representa una **red neuronal profunda** con dos capas ocultas. A continuación, se describen sus componentes:\n",
    "\n",
    "1. **Capa de entrada (verde)**:\n",
    "   - Contiene **4 neuronas de entrada** (`x[0]`, `x[1]`, `x[2]`, `x[3]`), que representan las características del conjunto de datos de entrada.\n",
    "   - Cada una de estas neuronas está conectada a todas las neuronas de la primera capa oculta.\n",
    "\n",
    "2. **Primera capa oculta (naranja, \"capa oculta 1\")**:\n",
    "   - Contiene **3 neuronas ocultas** (`h1[0]`, `h1[1]`, `h1[2]`).\n",
    "   - Cada neurona recibe una **suma ponderada** de todas las entradas y aplica una **función de activación** (como ReLU o tanh).\n",
    "   - Sus valores activados se transmiten como entrada a la segunda capa oculta.\n",
    "\n",
    "3. **Segunda capa oculta (amarilla, \"capa oculta 2\")**:\n",
    "   - También tiene **3 neuronas ocultas** (`h2[0]`, `h2[1]`, `h2[2]`).\n",
    "   - Cada neurona recibe información de todas las neuronas de la **primera capa oculta**.\n",
    "   - También aplica una función de activación y transmite los valores resultantes a la capa de salida.\n",
    "\n",
    "4. **Capa de salida (azul, \"salida\")**:\n",
    "   - Contiene **una única neurona de salida** (`y`).\n",
    "   - Recibe una suma ponderada de las activaciones de la **segunda capa oculta**.\n",
    "   - En un caso de regresión, `y` puede representar un valor numérico. En clasificación, podría representar probabilidades para diferentes clases después de aplicar una función como `softmax` o `sigmoide`.\n",
    "\n",
    "#### **Expresión matemática del cálculo en la red**\n",
    "Cada neurona en la **primera capa oculta** se calcula como:\n",
    "\n",
    "$$\n",
    "h1[j] = \\text{Activación} \\left( \\sum_{i=0}^{3} w1[i,j] \\cdot x[i] + b1[j] \\right), \\quad j = 0,1,2\n",
    "$$\n",
    "\n",
    "Cada neurona en la **segunda capa oculta** se calcula como:\n",
    "\n",
    "$$\n",
    "h2[k] = \\text{Activación} \\left( \\sum_{j=0}^{2} w2[j,k] \\cdot h1[j] + b2[k] \\right), \\quad k = 0,1,2\n",
    "$$\n",
    "\n",
    "Finalmente, la **salida** se obtiene con:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{k=0}^{2} v[k] \\cdot h2[k] + b_y\n",
    "$$\n",
    "\n",
    "Tener grandes redes neuronales formadas por muchas de estas capas de computación es lo que inspiró el término `aprendizaje profundo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40C4HWlTWcEJ"
   },
   "source": [
    "### Ajustando redes neuronales\n",
    "\n",
    "Utilicemos las funciones definidas anteriormente, para entender la aplicación de redes neuronales con scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dibuja_dispersion_discreta.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dibuja_separador_2d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih9TStpMWcEd"
   },
   "source": [
    "Veamos el funcionamiento del MLP aplicando el `MLPClassifier` al conjunto de datos `two_ moon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-O3wJw5WcEe",
    "outputId": "a8a57535-4a65-4f7e-d2bb-08da74ac3ea8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, stratify=y,random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_entrenamiento, y_entrenamiento)\n",
    "dibuja_separador_2d(mlp, X_entrenamiento, relleno=True, alfa=.3)\n",
    "\n",
    "dibuja_dispersion_discreta(X_entrenamiento[:, 0], X_entrenamiento[:, 1], y_entrenamiento)\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qD7onBNWcEn"
   },
   "source": [
    "Este gráfico representa la **frontera de decisión** aprendida por una **red neuronal multicapa (MLPClassifier de Scikit-Learn)** para clasificar dos clases en un problema de clasificación no lineal.\n",
    "\n",
    "1. **Ejes**:\n",
    "   - El eje **X** (\"Característica 0\") y el eje **Y** (\"Característica 1\") representan las dos características de entrada del dataset.\n",
    "   \n",
    "2. **Regiones coloreadas**:\n",
    "   - La región **azul** representa el área donde la red neuronal predice una clase (círculos azules).\n",
    "   - La región **roja** representa el área donde la red neuronal predice la otra clase (triángulos naranjas).\n",
    "   - La **frontera entre las regiones** representa la separación aprendida por la red neuronal.\n",
    "\n",
    "3. **Puntos de datos**:\n",
    "   - **Círculos azules**: Representan instancias de la clase 0.\n",
    "   - **Triángulos naranjas**: Representan instancias de la clase 1.\n",
    "\n",
    "\n",
    "Se ilustra cómo una **red neuronal multicapa (MLP)** **aprende una frontera de decisión no lineal**:\n",
    "\n",
    "1. **Modelo utilizado**:\n",
    "   - Se utilizó un **Perceptrón Multicapa (MLP)** con al menos **una capa oculta** para capturar la relación no lineal entre las características.\n",
    "   - El algoritmo de optimización usado (`lbfgs`) ajusta los pesos de la red para minimizar el error de clasificación.\n",
    "\n",
    "2. **Frontera de decisión aprendida**:\n",
    "   - La línea que separa las regiones **no es lineal**, lo que indica que la red ha aprendido una **transformación no lineal** de los datos.\n",
    "   - Esto es posible gracias a las **funciones de activación** aplicadas en las **capas ocultas**.\n",
    "\n",
    "3. **Comparación con modelos lineales**:\n",
    "   - Un **modelo lineal (como regresión logística o un perceptrón simple)** generaría una frontera recta, lo que podría no ser suficiente para separar correctamente los datos.\n",
    "   - La MLP puede **aprender representaciones más complejas** de los datos, permitiendo que la frontera de decisión se adapte a la estructura del dataset.\n",
    "\n",
    "De forma predeterminada, MLP usa 100 nodos ocultos, lo cual es bastante para este pequeño conjunto de datos. Podemos reducir el número (lo que reduce la complejidad del modelo) y obtener un buen resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6AOMkl_WcEo",
    "outputId": "c1a322df-fac4-4e57-abaf-2f03f9b5d45f"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10]).fit(X_entrenamiento, y_entrenamiento)\n",
    "dibuja_separador_2d(mlp, X_entrenamiento, relleno=True, alfa=.3)\n",
    "\n",
    "dibuja_dispersion_discreta(X_entrenamiento[:, 0], X_entrenamiento[:, 1], y_entrenamiento)\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQpNAV-nWcEz"
   },
   "source": [
    "Con solo 10 unidades ocultas, la frontera de decisión parece un poco más irregular. La no linealidad predeterminada es relu. Con una sola capa oculta, esto significa que la función de decisión estará formada por 10 segmentos de línea recta. Si queremos una frontera  de decisión más uniforme, podríamos agregar más unidades ocultas, agregando una segunda capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mbfs1GDWcE0",
    "outputId": "f18c03fe-885f-45a9-d57f-2b865287339b"
   },
   "outputs": [],
   "source": [
    "# Usando dos capas ocultas, con 10 unidades cada una\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10, 10]).fit(X_entrenamiento, y_entrenamiento)\n",
    "dibuja_separador_2d(mlp, X_entrenamiento, relleno=True, alfa=.3)\n",
    "\n",
    "dibuja_dispersion_discreta(X_entrenamiento[:, 0], X_entrenamiento[:, 1], y_entrenamiento)\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvX8g3qIWcE8"
   },
   "source": [
    "Hagamos lo mismo utilizando la nonlinealidad de `tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Gz57cgyWcE9",
    "outputId": "dd4e71ee-15dd-4002-f0d4-98a4a0bb1bc0"
   },
   "outputs": [],
   "source": [
    "# Usando dos capas ocultas, con 10 unidades cada una\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', activation='tanh', random_state=0, hidden_layer_sizes=[10, 10],  max_iter=1000)\n",
    "mlp.fit(X_entrenamiento, y_entrenamiento)\n",
    "\n",
    "dibuja_separador_2d(mlp, X_entrenamiento, relleno=True, alfa=.3)\n",
    "dibuja_dispersion_discreta(X_entrenamiento[:, 0], X_entrenamiento[:, 1], y_entrenamiento)\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YINB4qmNWcFF"
   },
   "source": [
    "Finalmente, también podemos controlar la complejidad de una red neuronal utilizando una penalización `l2` para reducir los pesos hacia cero, como se hizo  en la regresión ridge y los clasificadores lineales. El parámetro para esto en MLPClassifier es `alpha` (como en los modelos de regresión lineal) y se establece en un valor muy bajo (poca regularización) de forma predeterminada. \n",
    "\n",
    "El siguiente gráfico muestra el efecto de diferentes valores de `alpha` en el conjunto de datos `two_moons`, usando dos capas ocultas de `10` o `100` unidades cada una.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YauGLMRWcFH",
    "outputId": "a1ee84eb-3355-4946-ac9b-d56c357ef977"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for axx, n_nodos_ocultos in zip(axes, [10, 100]):\n",
    "    for ax, alfa in zip(axx, [0.0001, 0.01, 0.1, 1]):\n",
    "        mlp = MLPClassifier(solver='adam', random_state=0,\n",
    "                            hidden_layer_sizes=[n_nodos_ocultos, n_nodos_ocultos],alpha=alfa, max_iter=1000)\n",
    "        mlp.fit(X_entrenamiento, y_entrenamiento)\n",
    "        dibuja_separador_2d(mlp, X_entrenamiento, relleno=True, alfa=.3, ax=ax)\n",
    "        dibuja_dispersion_discreta(X_entrenamiento[:, 0], X_entrenamiento[:, 1], y_entrenamiento, ax=ax)\n",
    "        ax.set_title(\"n_ocultas=[{}, {}]\\nalfa={:.4f}\".format( n_nodos_ocultos, n_nodos_ocultos, alfa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-sucVnJWcFO"
   },
   "source": [
    "\n",
    "Una propiedad importante de las redes neuronales es que sus **pesos iniciales se establecen aleatoriamente** antes de comenzar el proceso de aprendizaje. Esta inicialización aleatoria **afecta el modelo final**, lo que significa que incluso utilizando exactamente los mismos parámetros, los modelos pueden ser diferentes si se emplean distintas semillas aleatorias.\n",
    "\n",
    "Si la red neuronal es **grande y su complejidad está bien ajustada**, la variabilidad en la inicialización no debería afectar significativamente la **precisión** del modelo. Sin embargo, en redes más pequeñas, esta variabilidad puede tener un impacto mayor, por lo que es un factor a considerar.\n",
    "\n",
    "La siguiente figura muestra gráficos de varios modelos, todos entrenados con la **misma configuración de parámetros**, pero con **diferentes inicializaciones aleatorias**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN-Ei3lPWcFP",
    "outputId": "bab79be9-84a8-40bd-d900-f0b505c8f0f0"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    mlp = MLPClassifier(solver='adam', random_state=i, hidden_layer_sizes=[100, 100],max_iter=1000)\n",
    "    mlp.fit(X_entrenamiento, y_entrenamiento)\n",
    "    \n",
    "    dibuja_separador_2d(mlp, X_entrenamiento, relleno=True, alfa=.3, ax=ax)\n",
    "    dibuja_dispersion_discreta(X_entrenamiento[:, 0], X_entrenamiento[:, 1], y_entrenamiento, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyAbs5JIWcFX"
   },
   "source": [
    "Para obtener una mejor comprensión de las redes neuronales en datos del mundo real, apliquemos MLPClassifier al conjunto de datos `Breast Cancer`. Comenzamos con los parámetros predeterminados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd-ppfweWcFX",
    "outputId": "bdf0e4f7-10ec-4034-d293-a636e3e49d85"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "#np.set_printoptions(suppress=True)\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "print(\"Datos de cancer por caracteristica maxima:\\n{}\".format(cancer.data.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukw3qF2DWcFg",
    "outputId": "b6ce7578-983e-43cc-f3d4-64ca53631102"
   },
   "outputs": [],
   "source": [
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split( cancer.data, cancer.target, random_state=0)\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(X_entrenamiento, y_entrenamiento)\n",
    "print(\"Precision del conjunto de entrenamiento: {:.3f}\".format(mlp.score(X_entrenamiento, y_entrenamiento)))\n",
    "print(\"Precision del conjunto de pruebas: {:.3f}\".format(mlp.score(X_prueba, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f0dSNBfWcFm"
   },
   "source": [
    "La precisión del MLP es bastante buena, pero no tan buena como los otros modelos. Como en el ejemplo de SVC anterior, es probable que esto se deba al escalado de los datos. Las redes neuronales también esperan que todas las características de entrada varíen de manera similar, e idealmente tengan una media de 0 y una varianza de 1. Debemos cambiar la escala de nuestros datos para que cumplan con estos requisitos.\n",
    "\n",
    "Nuevamente, hacemos esto a mano aquí, ya que se puede utilizar  `StandardScaler` para hacer esto automáticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhzGnCwiWcFn",
    "outputId": "8bb20891-318a-448d-f5bc-6a3782343141"
   },
   "outputs": [],
   "source": [
    "# Calculamos la media por caracteristica en el conjunto de entrenamiento\n",
    "media_conjunto_entrenamiento = X_entrenamiento.mean(axis=0)\n",
    "\n",
    "# calculamos la desviacion estandar de cada caracteristica en el conjunto de entrenamiento\n",
    "std_conjunto_entrenamiento = X_entrenamiento.std(axis=0)\n",
    "\n",
    "# sustraemos la media y escalamos la inversa de la desviacion estandar\n",
    "# despues mean=0, std =1\n",
    "X_entrenamiento_escalado = (X_entrenamiento -media_conjunto_entrenamiento)/std_conjunto_entrenamiento\n",
    "\n",
    "# usamos la misma transformacion sobre el conjunto de prueba\n",
    "X_prueba_escalado = (X_prueba - media_conjunto_entrenamiento)/std_conjunto_entrenamiento\n",
    "\n",
    "mlp = MLPClassifier(random_state=0, max_iter=400)\n",
    "mlp.fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "\n",
    "print(\"Precision del conjunto de entrenamiento: {:.3f}\".format(mlp.score(X_entrenamiento_escalado, y_entrenamiento)))\n",
    "print(\"Precision del conjunto de pruebas: {:.3f}\".format(mlp.score(X_prueba_escalado, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7BLVhbjWcFv"
   },
   "source": [
    "Los resultados son mucho mejores después de escalar  y ya son bastante competitivos. Sin embargo, recibimos una advertencia del modelo que nos dice que se ha alcanzado el número máximo de iteraciones. Esto es parte del algoritmo de `Adam` para aprender el modelo y nos dice que debemos aumentar el número de iteraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk2k_LGJWcFv",
    "outputId": "ae99097e-a93c-458a-d5fe-52b90cf7f6f7"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "mlp.fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "\n",
    "print(\"Precision del conjunto de entrenamiento: {:.3f}\".format(mlp.score(X_entrenamiento_escalado, y_entrenamiento)))\n",
    "print(\"Precision del conjunto de pruebas: {:.3f}\".format(mlp.score(X_prueba_escalado, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfjHavrQWcF2"
   },
   "source": [
    "Aumentar el número de iteraciones solo aumentó el rendimiento del conjunto de entrenamiento, no el rendimiento de generalización. Aún así, el modelo está funcionando bastante bien. Como hay una brecha entre el rendimiento de los conjuntos de  entrenamiento y de  prueba, podemos tratar de disminuir la complejidad del modelo para obtener un mejor rendimiento de generalización. Aquí, optamos por aumentar el parámetro `alpha` (de 0.0001 a 1) para agregar una regularización más fuerte de los pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0Vsi-P3WcF3",
    "outputId": "9ff0e178-53fe-4984-9cbe-9d4d3810e508"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000,alpha=1, random_state=0)\n",
    "mlp.fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "\n",
    "print(\"Precision del conjunto de entrenamiento: {:.3f}\".format(mlp.score(X_entrenamiento_escalado, y_entrenamiento)))\n",
    "print(\"Precision del conjunto de pruebas: {:.3f}\".format(mlp.score(X_prueba_escalado, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQcH6Tb1WcF9"
   },
   "source": [
    "Si bien es posible analizar lo que ha aprendido una **red neuronal**, esto suele ser **mucho más complejo** que interpretar un **modelo lineal** o un **modelo basado en árboles de decisión**. Una forma de obtener información sobre lo aprendido por la red es **examinar los pesos del modelo**. Puedes ver un ejemplo de esto en la [galería de ejemplos de scikit-learn](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html).\n",
    "\n",
    "En el caso del conjunto de datos `Breast Cancer`, interpretar estos pesos puede ser un desafío. El siguiente gráfico muestra los pesos aprendidos en las conexiones entre la **capa de entrada** y la **primera capa oculta**. \n",
    "\n",
    "- **Las filas** del gráfico representan las **30 características de entrada**.  \n",
    "- **Las columnas** representan las **100 unidades ocultas**.  \n",
    "- **Colores claros** indican **valores positivos grandes**, mientras que **colores oscuros** representan **valores negativos**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K-vIOnuWcF9",
    "outputId": "9398ce86-d4d7-4e2c-eb3c-b6931ef12c71"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
    "plt.yticks(range(30), cancer.feature_names)\n",
    "plt.xlabel(\"Columnas en matriz de peso\")\n",
    "plt.ylabel(\"Caracteristicas de entrada\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyoF0hdMWcGI"
   },
   "source": [
    "A partir de la figura, podemos inferir que las características cuyos **pesos son muy pequeños en todas las unidades ocultas** pueden ser **menos relevantes** para el modelo. En particular, se observa que `mean smoothness` y `mean compactness`, junto con varias características entre `smoothness error` y `fractal dimension error`, tienen **pesos relativamente bajos** en comparación con otras características.\n",
    "\n",
    "Esto podría indicar que:\n",
    "1. **Estas características tienen menor impacto en la predicción del modelo**.\n",
    "2. **No están bien representadas de una manera que la red neuronal pueda aprovechar eficazmente**.\n",
    "\n",
    "También podríamos analizar los pesos que conectan la **capa oculta con la capa de salida**, aunque su interpretación suele ser **aún más compleja**, ya que reflejan cómo cada unidad oculta contribuye a la decisión final del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwX-uh5iWcGJ"
   },
   "source": [
    "#### **Estimando la complejidad en una red neuronal**\n",
    "\n",
    "Los parámetros más importantes en una red neuronal son el **número de capas ocultas** y el **número de unidades por capa oculta**. Es recomendable **comenzar con una o dos capas ocultas** y expandirlas si es necesario. La cantidad de **nodos por capa oculta** a menudo es similar a la cantidad de características de entrada, pero rara vez es **mucho mayor** en niveles bajos o intermedios de la red.\n",
    "\n",
    "Una forma útil de estimar la **complejidad de una red neuronal** es considerar el número total de **pesos o coeficientes aprendidos**. \n",
    "\n",
    "Por ejemplo, si se tiene un **conjunto de datos de clasificación binaria** con `100` características y se usa una red con **100 unidades ocultas en una capa**, entonces:\n",
    "\n",
    "- **Pesos entre la entrada y la primera capa oculta:**  \n",
    "  $$\n",
    "  100 \\times 100 = 10,000\n",
    "  $$\n",
    "- **Pesos entre la capa oculta y la capa de salida:**  \n",
    "  $$\n",
    "  100 \\times 1 = 100\n",
    "  $$\n",
    "- **Total:**  \n",
    "  $$\n",
    "  10,100 \\text{ pesos}\n",
    "  $$\n",
    "\n",
    "Si se **agrega una segunda capa oculta** con `100` unidades ocultas:\n",
    "\n",
    "- **Pesos entre la primera y segunda capa oculta:**  \n",
    "  $$\n",
    "  100 \\times 100 = 10,000\n",
    "  $$\n",
    "- **Nuevo total:**  \n",
    "  $$\n",
    "  20,100 \\text{ pesos}\n",
    "  $$\n",
    "\n",
    "Si, en cambio, se usa **una sola capa oculta con 1,000 unidades**, entonces:\n",
    "\n",
    "- **Pesos entre la entrada y la capa oculta:**  \n",
    "  $$\n",
    "  100 \\times 1,000 = 100,000\n",
    "  $$\n",
    "- **Pesos entre la capa oculta y la salida:**  \n",
    "  $$\n",
    "  1,000 \\times 1 = 1,000\n",
    "  $$\n",
    "- **Total:**  \n",
    "  $$\n",
    "  101,000 \\text{ pesos}\n",
    "  $$\n",
    "\n",
    "Si se **agrega una segunda capa oculta con 1,000 unidades**, el número de pesos adicionales será:\n",
    "\n",
    "- **Pesos entre la primera y segunda capa oculta:**  \n",
    "  $$\n",
    "  1,000 \\times 1,000 = 1,000,000\n",
    "  $$\n",
    "- **Nuevo total:**  \n",
    "  $$\n",
    "  1,101,000 \\text{ pesos}\n",
    "  $$\n",
    "\n",
    "Este último modelo tiene **50 veces más parámetros** que la red con **dos capas ocultas de tamaño 100**, lo que aumenta drásticamente la **capacidad de modelado**, pero también el riesgo de **sobreajuste** y el costo computacional.\n",
    "\n",
    "Una estrategia común para ajustar los parámetros en una red neuronal es:\n",
    "\n",
    "1. **Crear una red lo suficientemente grande como para sobreajustar**  \n",
    "   - Se entrena un modelo grande para asegurarse de que la tarea puede ser **aprendida por la red**.\n",
    "2. **Reducir la red o aplicar regularización**  \n",
    "   - Una vez confirmado que la red puede aprender los datos de entrenamiento, se reduce el tamaño de la red o se **aumenta `alpha`** (regularización) para mejorar la **generalización**.\n",
    "\n",
    "Al entrenar un **MLPClassifier**, se recomienda:\n",
    "\n",
    "- **Usar `adam`**: Funciona bien en la mayoría de las situaciones, pero es **sensible a la escala de los datos**.  \n",
    "- **Escalar siempre los datos**: Se recomienda normalizarlos a **media `0` y varianza `1`** (`StandardScaler`).\n",
    "- **Usar `lbfgs` si se necesita robustez**, aunque puede ser **lento en modelos grandes o con conjuntos de datos grandes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3GfQVsKWcGN"
   },
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### **Ejercicio 1: Explorando el impacto de la inicialización aleatoria en MLP**\n",
    "\n",
    "**Objetivo:** Evaluar cómo la inicialización aleatoria de los pesos afecta el modelo.\n",
    "\n",
    "1. **Entrena** un `MLPClassifier` con **una capa oculta de 50 neuronas** y función de activación `tanh` en el dataset `make_moons`.\n",
    "2. **Usa diferentes semillas aleatorias (`random_state`)**, manteniendo fijos todos los demás hiperparámetros.\n",
    "3. **Grafica las fronteras de decisión** para al menos 5 inicializaciones diferentes.\n",
    "4. **Explica** por qué las fronteras de decisión varían a pesar de que los datos y parámetros son los mismos.\n",
    "\n",
    "*Pista:* ¿La inicialización aleatoria de los pesos es relevante para redes neuronales profundas? ¿Cómo afecta la convergencia del modelo?.\n",
    "\n",
    "#### **Ejercicio 2: Comparando la regularización con `alpha`**\n",
    "\n",
    "**Objetivo:** Analizar cómo la regularización afecta la complejidad del modelo y el sobreajuste.\n",
    "\n",
    "1. Entrena **un mismo modelo** (`MLPClassifier`) con:\n",
    "   - **Dos capas ocultas** (`hidden_layer_sizes=[50, 50]`)\n",
    "   - **Función de activación `relu`**\n",
    "   - **Valores de `alpha` de {0.0001, 0.01, 0.1, 1, 10}**.\n",
    "2. **Evalúa la precisión en entrenamiento y prueba** para cada valor de `alpha`.\n",
    "3. **Grafica las fronteras de decisión** y analiza cómo cambia con `alpha`.\n",
    "4. **Explica** cómo la regularización afecta la complejidad del modelo y el sobreajuste.\n",
    "\n",
    "*Pista:* ¿Qué ocurre con `alpha` muy pequeño o muy grande? ¿Cómo influye en la suavidad de la frontera de decisión?.\n",
    "\n",
    "\n",
    "#### **Ejercicio 3: Análisis de la complejidad del modelo**\n",
    "\n",
    "**Objetivo:** Evaluar el impacto del número de neuronas y capas en el rendimiento.\n",
    "\n",
    "1. Entrena un `MLPClassifier` con:\n",
    "   - **Una sola capa oculta** con `10`, `100` y `1000` neuronas.\n",
    "   - **Dos capas ocultas** con `10,10`, `100,100` y `1000,1000` neuronas.\n",
    "2. **Mide la cantidad de parámetros (pesos)** en cada configuración.\n",
    "3. **Evalúa la precisión en entrenamiento y prueba** y observa si hay sobreajuste.\n",
    "4. **Explica** qué arquitectura es la mejor en términos de rendimiento y complejidad.\n",
    "\n",
    "*Pista:* ¿Cómo se relaciona la cantidad de parámetros con la posibilidad de sobreajuste? ¿En qué casos un modelo con más capas es beneficioso?.\n",
    "\n",
    "#### **Ejercicio 4: Interpretación de pesos en la primera capa oculta**\n",
    "\n",
    "**Objetivo:** Visualizar e interpretar los pesos aprendidos en la primera capa de una red neuronal.\n",
    "\n",
    "1. **Entrena un `MLPClassifier`** con `hidden_layer_sizes=[100]` en el dataset `Breast Cancer`.\n",
    "2. **Extrae la matriz de pesos** entre la entrada y la primera capa oculta (`mlp.coefs_[0]`).\n",
    "3. **Visualiza los pesos** en un `heatmap` (usando `seaborn` o `matplotlib`).\n",
    "4. **Identifica características con pesos pequeños** y razona sobre su posible baja importancia en la clasificación.\n",
    "\n",
    "*Pista:* ¿Cómo podrías confirmar si las características con pesos bajos realmente son irrelevantes?\n",
    "\n",
    "\n",
    "#### **Ejercicio 5: Comparando solvers (`adam` vs `lbfgs`)**\n",
    "**Objetivo:** Evaluar cómo el solver afecta la convergencia y el tiempo de entrenamiento.\n",
    "\n",
    "1. Entrena un `MLPClassifier` con:\n",
    "   - **`solver='adam'`**\n",
    "   - **`solver='lbfgs'`**\n",
    "   - **Diferentes valores de `max_iter`** (200, 500, 1000)\n",
    "2. **Mide el tiempo de entrenamiento** con `time.time()` y la precisión en prueba.\n",
    "3. **Explica cuál solver converge más rápido** y por qué.\n",
    "\n",
    "*Pista:* `adam` es más rápido para datos grandes, pero `lbfgs` puede ser más preciso en ciertos casos.\n",
    "\n",
    "\n",
    "#### **Ejercicio 6: Optimización de hiperparámetros con `GridSearchCV`**\n",
    "\n",
    "**Objetivo:** Encontrar la mejor arquitectura para `MLPClassifier`.\n",
    "\n",
    "1. **Usa `GridSearchCV`** para optimizar:\n",
    "   - `hidden_layer_sizes`: { (50,), (100,), (50,50), (100,100) }\n",
    "   - `alpha`: {0.0001, 0.01, 0.1}\n",
    "   - `solver`: {'adam', 'lbfgs'}\n",
    "2. **Entrena y evalúa en el dataset `make_moons`**.\n",
    "3. **Encuentra la mejor combinación de hiperparámetros** y justifica los resultados.\n",
    "\n",
    "*Pista:* `GridSearchCV` evalúa varias combinaciones y selecciona la mejor.\n",
    "\n",
    "\n",
    "#### **Ejercicio 7: Evaluación de estabilidad en entrenamientos**\n",
    "\n",
    "**Objetivo:** Analizar cómo la inicialización y optimización afectan los resultados.\n",
    "\n",
    "1. **Entrena el mismo modelo (`MLPClassifier`) 10 veces** con:\n",
    "   - `random_state` diferente en cada ejecución.\n",
    "   - Misma arquitectura y parámetros.\n",
    "2. **Registra la precisión en entrenamiento y prueba** en cada ejecución.\n",
    "3. **Calcula la desviación estándar** de las precisiones y razona sobre la estabilidad del entrenamiento.\n",
    "\n",
    "*Pista:* ¿Qué modelo tiene menor variabilidad en sus resultados?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Redes-neuronales.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
