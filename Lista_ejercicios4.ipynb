{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08daa78-f8bd-4702-9db9-40589bd8b4bf",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "\n",
    "**1. Implementación de un autoencoder básico para compresión de datos**  \n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Construye un autoencoder simple para comprimir y reconstruir imágenes (por ejemplo, utilizando el dataset MNIST). La red debe estar compuesta por dos partes:  \n",
    "- **Encoder:** Reduce la dimensión de la imagen a un espacio latente de menor dimensionalidad.  \n",
    "- **Decoder:** Reconstruye la imagen original a partir de la representación latente.  \n",
    "\n",
    "El ejercicio implica ajustar la arquitectura (número de capas y neuronas), elegir funciones de activación (como ReLU o Sigmoid) y entrenar el modelo para minimizar la diferencia entre la imagen original y la reconstruida (por ejemplo, usando MSELoss).\n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: de 28x28 a un vector latente de tamaño 32\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder: de vector latente de tamaño 32 a 28x28\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Sigmoid()  # Para que la salida esté en el rango [0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Aplanar la imagen de entrada\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        # Reconstruir la imagen original\n",
    "        out = out.view(x.size(0), 1, 28, 28)\n",
    "        return out\n",
    "\n",
    "# Ejemplo de creación del modelo\n",
    "modelo = Autoencoder()\n",
    "print(modelo)\n",
    "```\n",
    "\n",
    "\n",
    "**2. Desarrollo de un autoencoder variacional (VAE) para generación de imágenes**\n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Implementa un VAE que, además de comprimir las imágenes, aprenda una distribución en el espacio latente. El ejercicio se centra en:  \n",
    "- Definir un encoder que genere los parámetros (media y log-varianza) de la distribución latente.  \n",
    "- Realizar el muestreo utilizando la \"reparametrización\" para permitir el flujo de gradiente.  \n",
    "- Diseñar un decoder que genere imágenes a partir de muestras del espacio latente.  \n",
    "\n",
    "Evalúa la capacidad del VAE para generar nuevas imágenes a partir de la distribución latente aprendida.\n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(28*28, 400)\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, 400)\n",
    "        self.fc3 = nn.Linear(400, 28*28)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decode(z)\n",
    "        reconstruction = reconstruction.view(-1, 1, 28, 28)\n",
    "        return reconstruction, mu, logvar\n",
    "\n",
    "# Ejemplo de creación del modelo\n",
    "vae = VAE(latent_dim=20)\n",
    "print(vae)\n",
    "```\n",
    "\n",
    "**3. Aplicación de autoencoders en reducción de dimensionalidad y visualización** \n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Utiliza un autoencoder (similar al del ejercicio 1) para reducir la dimensionalidad de un dataset de imágenes o datos de alta dimensión. Una vez entrenado, extrae la representación latente y utiliza técnicas de visualización (por ejemplo, t-SNE o PCA) para observar la agrupación y la estructura interna de los datos.  \n",
    "El ejercicio permite explorar cómo el autoencoder aprende representaciones compactas y cómo estas pueden ayudar a identificar clusters o patrones.\n",
    "\n",
    "*Código de referencia (extracción de la representación latente):*\n",
    "```python\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponiendo que 'model' es el autoencoder entrenado del ejercicio 1 y 'data_loader' contiene imágenes\n",
    "def get_latent_representations(model, data_loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, lbl in data_loader:\n",
    "            images = images.to(next(model.parameters()).device)\n",
    "            # Aplanar imágenes y pasar por el encoder\n",
    "            images = images.view(images.size(0), -1)\n",
    "            latent = model.encoder(images)\n",
    "            latents.append(latent.cpu())\n",
    "            labels.extend(lbl.numpy())\n",
    "    return torch.cat(latents, dim=0), labels\n",
    "\n",
    "# Una vez obtenidas las representaciones latentes:\n",
    "# latents, labels = get_latent_representations(model, data_loader)\n",
    "# tsne = TSNE(n_components=2)\n",
    "# latents_2d = tsne.fit_transform(latents)\n",
    "# plt.scatter(latents_2d[:, 0], latents_2d[:, 1], c=labels, cmap='viridis')\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "**4. Implementación del mecanismo de atención \"scaled dot-product\"**\n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Desarrolla una función en PyTorch que implemente el mecanismo de atención \"scaled dot-product\". La función debe recibir como entrada tensores de consulta (query), llave (key) y valor (value), calcular la puntuación mediante el producto escalar, aplicar el escalado por la raíz cuadrada de la dimensión y, finalmente, usar softmax para obtener los pesos de atención.  \n",
    "Este ejercicio es clave para entender cómo se ponderan las relaciones entre diferentes elementos en una secuencia.\n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    # query, key, value: [batch_size, num_heads, seq_len, d_k]\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n",
    "\n",
    "# Ejemplo de uso:\n",
    "batch_size, num_heads, seq_len, d_k = 2, 4, 10, 16\n",
    "query = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "key   = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "value = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "output, attention = scaled_dot_product_attention(query, key, value)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention shape:\", attention.shape)\n",
    "```\n",
    "\n",
    "**5. Construcción de un módulo de atención multi-cabecera en un modelo transformer**  \n",
    "*Enunciado detallado:*  \n",
    "Implementa un módulo de atención multi-cabecera que divida la entrada en varias \"cabeceras\", aplique el mecanismo de atención en cada una y luego combine los resultados.  \n",
    "\n",
    "Aspectos a considerar:  \n",
    "- Proyectar las entradas a espacios de query, key y value para cada cabecera.  \n",
    "- Concatenar las salidas de cada cabecera y aplicar una proyección final.  \n",
    "Este módulo es esencial para entender cómo el transformer procesa información de forma paralela en diferentes subespacios.\n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"El embedding debe ser divisible por el número de cabecera\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        # Proyección lineal\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "        \n",
    "        # Dividir en múltiples cabecera y reorganizar dimensiones\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Aplicar atención escalada (utilizando la función del ejercicio 4)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Reorganizar la salida\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        output = self.out_linear(context)\n",
    "        return output, attn\n",
    "\n",
    "# Ejemplo de uso:\n",
    "batch_size, seq_len, embed_dim, num_heads = 2, 15, 64, 8\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "output, attn = mha(x)\n",
    "print(\"Output shape:\", output.shape)\n",
    "```\n",
    "\n",
    "**6. Entrenamiento y escalado de un modelo transformer para modelado de lenguaje**  \n",
    "*Enunciado detallado:*  \n",
    "Construye un modelo transformer para modelado de lenguaje (por ejemplo, predicción de la siguiente palabra o traducción). Este ejercicio involucra:  \n",
    "- Definir un encoder (y/o decoder) basado en bloques de atención multicabecera, capas feedforward y mecanismos de normalización.  \n",
    "- Entrenar el modelo con un corpus de texto, experimentando con distintos tamaños (número de capas, cabeceras de atención, etc.) para observar cómo afecta la capacidad del modelo.  \n",
    "- Evaluar el rendimiento mediante métricas comunes en NLP, como la pérdida de entropía cruzada.\n",
    "\n",
    "*Código de referencia (usando el módulo Transformer de PyTorch):*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parámetros del modelo\n",
    "vocab_size = 10000\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "seq_length = 20\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length]\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = x.transpose(0,1)  # Transformer espera [seq_length, batch_size, embed_dim]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0,1)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "# Ejemplo de uso:\n",
    "modelo = TransformerModel(vocab_size, embed_dim, num_heads, num_layers)\n",
    "dummy_input = torch.randint(0, vocab_size, (8, seq_length))\n",
    "logits = modelo(dummy_input)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "```\n",
    "\n",
    "**7. Comparación de variantes del mecanismo de atención en tareas de NLP**  \n",
    "*Enunciado detallado:*  \n",
    "Diseña e implementa al menos dos variantes del mecanismo de atención (por ejemplo, atención escalada dot-product vs. atención local o autoregresiva) y aplícalas a una tarea de NLP como clasificación de texto o análisis de sentimientos.  \n",
    "El ejercicio debe incluir:  \n",
    "- La implementación de las variantes, preferiblemente en módulos reutilizables.  \n",
    "- Un estudio comparativo donde se entrene cada variante bajo las mismas condiciones y se analicen métricas de desempeño (p. ej., precisión, pérdida).  \n",
    "- Reflexiones sobre cómo cada variante maneja la dependencia entre elementos de la secuencia.\n",
    "\n",
    "*Código de referencia (esquemático de dos variantes de atención):*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Variante 1: Atención escalada dot-product (igual que en el ejercicio 4)\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n",
    "\n",
    "# Variante 2: Atención local (limitando la atención a una ventana)\n",
    "def local_attention(query, key, value, window_size=5):\n",
    "    batch_size, num_heads, seq_len, d_k = query.size()\n",
    "    outputs = []\n",
    "    all_attn = []\n",
    "    for i in range(seq_len):\n",
    "        # Definir límites de la ventana\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(seq_len, i + window_size + 1)\n",
    "        # Extraer las partes relevantes\n",
    "        q = query[:, :, i:i+1, :]\n",
    "        k_slice = key[:, :, start:end, :]\n",
    "        v_slice = value[:, :, start:end, :]\n",
    "        # Calcular atención para la posición i\n",
    "        scores = torch.matmul(q, k_slice.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, v_slice)\n",
    "        outputs.append(output)\n",
    "        all_attn.append(attn)\n",
    "    outputs = torch.cat(outputs, dim=2)\n",
    "    all_attn = torch.cat(all_attn, dim=2)\n",
    "    return outputs, all_attn\n",
    "\n",
    "# Ejemplo de uso para ambas variantes:\n",
    "batch_size, num_heads, seq_len, d_k = 2, 4, 10, 16\n",
    "query = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "key   = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "value = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "out1, attn1 = scaled_dot_product_attention(query, key, value)\n",
    "out2, attn2 = local_attention(query, key, value, window_size=3)\n",
    "\n",
    "print(\"Scaled dot-product output shape:\", out1.shape)\n",
    "print(\"Local attention output shape:\", out2.shape)\n",
    "```\n",
    "\n",
    "\n",
    "**8. Implementación de positional encoding en PyTorch**  \n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Desarrolla una función que implemente la codificación posicional sinusoidal, tal como se describe en el paper original de transformers. Esta codificación deberá sumarse a las embeddings de entrada para inyectar información posicional en el modelo.  \n",
    "*Objetivos:*  \n",
    "- Comprender la importancia de la codificación posicional.  \n",
    "- Implementar el cálculo de senos y cosenos en diferentes frecuencias.  \n",
    "- Integrar la codificación posicional a un pipeline de embeddings.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# Ejemplo de uso:\n",
    "seq_len = 50\n",
    "embed_dim = 512\n",
    "pe = positional_encoding(seq_len, embed_dim)\n",
    "print(\"Positional Encoding shape:\", pe.shape)\n",
    "```\n",
    "\n",
    "\n",
    "**9. Implementación de un bloque de transformer encoder desde cero**  \n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Construye un bloque de encoder de transformer que incluya los siguientes componentes:  \n",
    "\n",
    "- Módulo de atención multicabecera (con proyecciones lineales y atención escalada dot-product).  \n",
    "- Capa feedforward con al menos dos capas lineales y función de activación (por ejemplo, ReLU).  \n",
    "- Conexiones residuales y normalización de capas (LayerNorm) en ambos sub-bloques.  \n",
    "\n",
    "*Objetivos:*  \n",
    "- Entender la estructura interna de un bloque encoder.  \n",
    "- Implementar el mecanismo de residual connection y normalización para estabilizar el entrenamiento.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"El embedding debe ser divisible por el número de cabecera\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "        # Reorganizar para múltiples cabeceras\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        output = self.out_linear(context)\n",
    "        return output\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Capa feedforward\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Primer sub-bloque: atención multicabecera + residual + normalización\n",
    "        attn_output = self.mha(x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # Segundo sub-bloque: capa feedforward + residual + normalización\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# Ejemplo de uso:\n",
    "batch_size, seq_len, embed_dim = 2, 10, 64\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 256\n",
    "dummy_input = torch.randn(batch_size, seq_len, embed_dim)\n",
    "encoder_block = TransformerEncoderBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "output = encoder_block(dummy_input)\n",
    "print(\"Encoder block output shape:\", output.shape)\n",
    "```\n",
    "\n",
    "\n",
    "**10. Construcción de un modelo transformer completo para traducción**  \n",
    "\n",
    "*Enunciado detallado:*\n",
    "\n",
    "Utilizando el módulo `nn.Transformer` de PyTorch, crea un modelo completo de encoder-decoder para una tarea de traducción automática. El ejercicio incluye:  \n",
    "- Definir embeddings para el idioma de origen y destino.  \n",
    "- Aplicar codificación posicional a ambas entradas.  \n",
    "- Configurar el transformer con parámetros ajustables (número de capas, cabeceras, etc.).  \n",
    "- Implementar la función de entrenamiento que utilice la entropía cruzada como pérdida.  \n",
    "\n",
    "*Objetivos:*  \n",
    "- Integrar encoder y decoder en un modelo end-to-end.  \n",
    "- Entender cómo se gestionan las máscaras para el decoder en tareas de traducción.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, dropout=0.1, max_seq_len=50):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_dim)\n",
    "        self.positional_encoding = positional_encoding(max_seq_len, embed_dim).unsqueeze(0)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # src, tgt: [batch_size, seq_len]\n",
    "        src_emb = self.src_embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.tgt_embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        \n",
    "        # Transformer espera entrada [seq_len, batch_size, embed_dim]\n",
    "        src_emb = src_emb.transpose(0,1)\n",
    "        tgt_emb = tgt_emb.transpose(0,1)\n",
    "        \n",
    "        memory = self.encoder(src_emb)\n",
    "        output = self.decoder(tgt_emb, memory)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Ejemplo de uso:\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "modelo = SimpleTransformer(src_vocab_size, tgt_vocab_size, embed_dim=256, num_heads=8, num_layers=3)\n",
    "dummy_src = torch.randint(0, src_vocab_size, (8, 20))\n",
    "dummy_tgt = torch.randint(0, tgt_vocab_size, (8, 20))\n",
    "output = modelo(dummy_src, dummy_tgt)\n",
    "print(\"Transformer output shape:\", output.shape)\n",
    "```\n",
    "\n",
    "\n",
    "**11. Ajuste fino de un modelo transformer preentrenado para clasificación de texto**  \n",
    "*Enunciado detallado:*  \n",
    "Utiliza un modelo Transformer preentrenado (por ejemplo, BERT o similar disponible en Hugging Face) y realiza el ajuste fino para una tarea de clasificación de texto. El ejercicio debe cubrir:  \n",
    "- Carga del modelo preentrenado.  \n",
    "- Adición de una capa de salida para clasificación.  \n",
    "- Preparación de un dataset de texto y tokenización adecuada.  \n",
    "- Entrenamiento y evaluación del modelo en la tarea objetivo.  \n",
    "\n",
    "*Objetivos:*  \n",
    "- Aprender a transferir conocimiento de un modelo preentrenado a tareas específicas.  \n",
    "- Comprender las técnicas de ajuste fino (fine-tuning) en modelos de lenguaje.  \n",
    "\n",
    "*Código de referencia (esquemático):*\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Cargar el tokenizador y modelo preentrenado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "modelo = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Ejemplo de preparación de datos\n",
    "sentences = [\"This is a positive example.\", \"This is a negative example.\"]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n",
    "\n",
    "# Ejemplo de paso de entrenamiento\n",
    "labels = torch.tensor([1, 0]).unsqueeze(0)  # Ajustar dimensiones según sea necesario\n",
    "outputs = modelo(**inputs, labels=labels.squeeze())\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "print(\"Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "**12. Estudio de la escalabilidad de transformers en tareas de modelado de lenguaje**  \n",
    "\n",
    "*Enunciado detallado:*  \n",
    "Investiga cómo varían el rendimiento y el tiempo de entrenamiento de un modelo transformer al modificar su arquitectura. Para ello, diseña un experimento en el que:  \n",
    "- Se entrene un modelo transformer para modelado de lenguaje (por ejemplo, predicción de la siguiente palabra).  \n",
    "- Se varíen parámetros como el número de capas del encoder, la cantidad de cabeceras en la atención y el tamaño de las capas feedforward.  \n",
    "- Se registren métricas de desempeño (pérdida, perplexity) y tiempos de entrenamiento, para comparar la escalabilidad y eficiencia del modelo.  \n",
    "\n",
    "*Objetivos:*  \n",
    "- Analizar el trade-off entre la complejidad del modelo y su desempeño en tareas de NLP.  \n",
    "- Experimentar con hiperparámetros y comprender su impacto en el entrenamiento.  \n",
    "\n",
    "*Código de referencia (estructura básica de experimento):*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Aquí se esquematiza la carga de datos, tokenización y definición de un modelo transformer\n",
    "# La idea es entrenar con diferentes configuraciones y registrar resultados.\n",
    "\n",
    "def train_model(model, data_loader, epochs=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Supongamos que 'src' es la secuencia de entrada y 'tgt' la secuencia a predecir\n",
    "            src, tgt = batch['src'], batch['tgt']\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoca {epoch+1}, Loss: {total_loss/len(data_loader)}\")\n",
    "\n",
    "# Ejemplo: cambiar número de capas y cabezas\n",
    "# Se deben definir distintos modelos con configuraciones variantes y evaluar su desempeño.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c1d76-f0e4-412d-b7e7-c7fd65e3173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
