{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46581f61-76d7-4122-a947-d1e8b24bf1de",
   "metadata": {},
   "source": [
    "### Examen final\n",
    "\n",
    "La siguiente lista de ejercicios (1 cada pregunta) constituye el **examen final** junto con una exposición. Cada uno aborda aspectos clave en **modelado**, **funciones de pérdida**, **arquitecturas profundas**, **atención** y **optimizaciones** no triviales. \n",
    "\n",
    "**Importante**:  \n",
    "- El docente **requiere** que toda entrega esté acompañada de bitácoras, diagramas y explicaciones **originales**.  \n",
    "- Se empleará un sistema de verificación que detecta automáticamente contenido **copiado** de herramientas como ChatGPT.  \n",
    "- Incluir de forma obligatoria una sección al final titulada ***\"Notas de autoría\"***, donde el estudiante describa **su** proceso de razonamiento, errores encontrados y correcciones realizadas.\n",
    "- Presentación del cuaderno con respuestas: 8 puntos y 12 exposición de ejercicios. **Fecha de presentación 17 de marzo hasta las 6pm**.\n",
    "\n",
    "**Sugerencia**:  \n",
    "1. **No** copies literalmente el enunciado ni los fragmentos de respuesta en herramientas de LLM.  \n",
    "2. Genera tus propios ejemplos numéricos y agrega comentarios detallados en el código.  \n",
    "3. Prepara tu presentación para la **defensa oral**,  requerida por el profesor. \n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Regularización mixta y datos sintéticos\n",
    "\n",
    "**Ejercicio 1.1**  \n",
    "- Diseña un **conjunto de datos sintéticos** bidimensional (ej. 2D) en el que dos clases se solapan parcialmente en un plano. Explica cómo se generó los datos, indicando las distribuciones y los parámetros empleados (muestra ecuaciones o pseudocódigo, **no** uses librerías externas).  \n",
    "- Entrena un **modelo de regresión logística** con regularización L2 y L1 al mismo tiempo (*Elastic Net*, $\\alpha \\|\\mathbf{w}\\|_1 + \\frac{1-\\alpha}{2}\\|\\mathbf{w}\\|^2_2$).  \n",
    "- Realiza un **análisis teórico** de por qué la función de costo resultante **no es convexa** en relación con ambos términos simultáneamente (L1 + L2) si se incluyen también términos no lineales (e.g., polinomiales).  \n",
    "- Muestre curvas de entrenamiento (p. ej., error vs. época) y comenta por qué la combinación de L1 y L2 dificulta la optimización.\n",
    "\n",
    "**Pista**: Para que no sea trivial, escribe una forma de introducir *features* polinomiales (o una red neuronal pequeña) que rompa la convexidad.\n",
    "\n",
    "#### 2. Retropropagación con función de activación personalizada\n",
    "\n",
    "**Ejercicio 2.1**  \n",
    "- Define una **nueva función de activación** (ejemplo: $\\phi(z) = z \\,\\text{sigmoid}(z)$, que es similar a *swish*) e incluya un **término de corrección**: $\\phi(z) = z \\,\\sigma(z) + \\beta \\cdot \\sin(z)$, con $\\beta$ un parámetro entrenable adicional.  \n",
    "- Formula la **derivada** de esta activación respecto a $z$ y respecto a $\\beta$.  \n",
    "- Construye una red neuronal muy simple (por ejemplo, 1 sola capa oculta) que use esta activación. Muestra un mini-ejemplo de **forward y backward pass** con 2 neuronas de entrada, 2 neuronas ocultas y 1 de salida, dejando indicado cómo se calculan los gradientes **sin** depender de marcos de auto-diferenciación.\n",
    "\n",
    "**Ejercicio 2.2**  \n",
    "- Discuta si este tipo de activación puede causar **explosión** o **desvanecimiento** del gradiente según los valores de $\\beta$. Proponga una estrategia de inicialización de $\\beta$.\n",
    "\n",
    "#### 3. Pérdidas combinadas: cross entropy + KL + reconstrucción\n",
    "\n",
    "**Ejercicio 3.1**  \n",
    "- Supongamos que estás diseñando un modelo que combina:  \n",
    "  1. *Cross Entropy* para clasificación,  \n",
    "  2. *Kullback-Leibler divergence* para alinear distribuciones de salidas,  \n",
    "  3. *Reconstruction loss* (p. ej., MSE) para forzar a una capa interna a \"autoencodificar\" parte de la representación.  \n",
    "- Escribe la **función de pérdida total** (usa ponderaciones distintas para cada término) y explica el orden en que se retropropagan los gradientes, indicando si existe riesgo de **incompatibilidad** entre estos términos.\n",
    "\n",
    "**Ejercicio 3.2**  \n",
    "- Ilustra un ejemplo numérico (inventado, aunque sea sencillo) en el que la parte de KL 2desvía\" el mínimo que se alcanzaría usando solamente CE + MSE. Explica los resultados.\n",
    "\n",
    "\n",
    "#### 4. Visualización de gradientes en 2D y *saturación*\n",
    "\n",
    "**Ejercicio 4.1**  \n",
    "- Genera un **pequeño dataset** 2D con dos clases, entrene un *perceptrón multicapa* (de al menos 2 capas ocultas) y represente en un diagrama de contorno de la región de decisión.  \n",
    "- En cada punto de una malla `(x, y)`, calcula la magnitud del gradiente de la *cross entropy* (respecto a los pesos principales) y **muestra** un mapa de calor superpuesto en el plano.  \n",
    "- Analiza en qué zonas del plano los gradientes se vuelven muy pequeños (*saturación*) y en qué zonas son grandes.\n",
    "\n",
    "**Ejercicio 4.2**  \n",
    "- Discute cómo la inicialización de los pesos y la elección de la función de activación modifican el patrón del mapa de calor de gradientes.\n",
    "\n",
    "#### 5. Auto-atención en datos multimodales\n",
    "\n",
    "**Ejercicio 5.1**  \n",
    "- Diseña un escenario en el que tengamos secuencias de texto (por ejemplo, oraciones cortas) y **características visuales** (por ejemplo, descriptores de imágenes en forma de vectores) que deban fusionarse.  \n",
    "- Proponga cómo usar *self-attention* para cada parte, y luego un **mecanismo cruzado** (*cross-attention*) para la interacción entre texto e imagen.  \n",
    "- Muestra **las ecuaciones** de queries, keys y values para ambos modos (texto e imagen) y explica las dimensiones involucradas (¿tienen el mismo $d_k$? ¿Por qué?).\n",
    "\n",
    "**Ejercicio 5.2**  \n",
    "- Argumenta por qué, en algunos casos, el factor $\\sqrt{d_k}$ podría ajustarse de forma distinta según el modo (texto o imagen). ¿Bajo qué circunstancias se justifica este ajuste?\n",
    "\n",
    "#### 6. Evaluación de *ciclos dinámicos* en grafos computacionales\n",
    "\n",
    "**Ejercicio 6.1**  \n",
    "- Proponga una **arquitectura de red recurrente** que tenga una conexión adicional (no estándar) que introduzca un \"ciclo\" en el grafo computacional. Ilustra con un diagrama de bloques.  \n",
    "- Explica por qué en un *framework* de grafo estático este tipo de conexión es más difícil de manejar, mientras que en un **grafos dinámicos** (tipo *PyTorch eager mode* o *JAX*) es más factible programarlo.\n",
    "\n",
    "**Ejercicio 6.2**  \n",
    "- Menciona **dos** inconvenientes o riesgos (numéricos o de complejidad) de mantener estos ciclos dinámicos. Propone estrategias de atenuación (por ejemplo, recorte de gradientes, steps limitados, etc.).\n",
    "\n",
    "\n",
    "#### 7. *Knowledge distillation* y *mini-ejercicio* de memoria\n",
    "\n",
    "**Ejercicio 7.1**  \n",
    "- Describe el procedimiento de *Knowledge Distillation* entre un modelo grande (teacher) y uno pequeño (student) cuando:  \n",
    "  1. El teacher regresa **distribuciones de probabilidad** sobre 10 clases, y  \n",
    "  2. El student aprende a imitar esas distribuciones con un factor $\\tau$ (temperatura).  \n",
    "- Explica cómo entra en juego la *Cross Entropy* con el vector suave (soft labels) y muestre la **fórmula** exacta de la pérdida.\n",
    "\n",
    "**Ejercicio 7.2**  \n",
    "- Asume que el modelo *student* tiene $(M_s$ parámetros y se evalúa en *minibatches* de tamaño $B$. Estima la memoria para:  \n",
    "  - El forward pass (activaciones).  \n",
    "  - El backward pass (gradientes).  \n",
    "- Compara con el caso en que usamos el modelo *teacher*, que puede tener $M_t \\gg M_s$ parámetros. Explica la **relación** entre la reducción de memoria y las prestaciones de inferencia.\n",
    "\n",
    "#### 8. **Seq2Seq** con mecanismo de atención\n",
    "\n",
    "**Ejercicio 8.1**\n",
    "1. Diseña un **modelo Seq2Seq** (por ejemplo, para traducción de oraciones simples) que utilice una red recurrente (RNN, LSTM o GRU) como codificador (*encoder*) y otra como decodificador (*decoder*).  \n",
    "2. Implementa un **mecanismo de atención** (puede ser *Luong* o *Bahdanau*) entre el *decoder* y los estados del *encoder*.  \n",
    "3. Muestra las **fórmulas** de cálculo de la puntuación de atención y cómo se normaliza para obtener la distribución atencional. Explica paso a paso cómo se integra en el proceso de decodificación.  \n",
    "\n",
    "**Ejercicio 8.2**\n",
    "- Reflexiona: ¿En qué casos el **mecanismo de atención** reduce el desvanecimiento de gradiente en redes recurrentes largas? Menciona dos ventajas y una limitación concreta de usar este esquema en **tareas multimodales** (p. ej., descripción de imágenes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52318566-d08f-4c10-85a1-142695fdb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
