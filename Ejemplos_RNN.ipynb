{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4mUCbP9qCLv"
   },
   "source": [
    "### RNN y texto\n",
    "\n",
    "Basado en el paper de Jeffrey L. Elman: **[Finding Structure in Time](https://condor.depaul.edu/dallbrit/extra/hon207/readings/elman-chapter10-finding-structure-in-time.pdf)**.\n",
    "\n",
    "El artículo se basa en representar el tiempo en modelos de redes neuronales (conexionistas) es muy importante.que implica el uso de enlaces recurrentes para dotar a las redes de una memoria dinámica. \n",
    "\n",
    "En este enfoque, los patrones de unidades ocultas se retroalimentan a sí mismos, es decir las representaciones internas que se desarrollan reflejan las demandas de la tarea en el contexto de estados internos previos. Estas representaciones revelan una estructura interesante, que les permite ser altamente dependientes del contexto, al mismo tiempo que expresan generalizaciones a través de clases de elementos.\n",
    "\n",
    "En el código siguiente:\n",
    "\n",
    "* Función make_batch:\n",
    "  - Genera los datos de entrada y salida para el entrenamiento de la RNN.\n",
    "  - Convierte las oraciones en índices utilizando un diccionario (word_dict).\n",
    "  - Los datos de entrada (input_batch) se crean usando codificación one-hot para representar las palabras excepto la última de cada oración.\n",
    "  - El objetivo (target_batch) es la última palabra de cada oración.\n",
    "\n",
    "* Clase TextRNN:\n",
    "  - Define un modelo de RNN con una capa lineal y un sesgo para generar predicciones.\n",
    "  - La función forward procesa los datos a través de la RNN y la capa lineal para obtener la predicción final.\n",
    "\n",
    "* Entrenamiento del modelo:\n",
    "\n",
    "  -  Se inicializan los pesos, se define la función de pérdida (CrossEntropyLoss) y el optimizador (Adam).\n",
    "  -  En cada época, se procesan los lotes de datos a través del modelo, se calcula la pérdida, y se actualizan los pesos.\n",
    "\n",
    "* Predicción y evaluación:\n",
    "\n",
    " - Al final del entrenamiento, se predice la última palabra de cada oración basada en las primeras dos palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3203,
     "status": "ok",
     "timestamp": 1624550562069,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "-RuOaUKUp2WH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()  \n",
    "        input = [word_dict[n] for n in word[:-1]]  \n",
    "        target = word_dict[word[-1]] \n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "\n",
    "    # Convierte la lista de arrays de NumPy a un único array de NumPy antes de convertirlo a un tensor\n",
    "    input_batch = np.array(input_batch, dtype=np.float32)\n",
    "    return torch.FloatTensor(input_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextRNN, self).__init__()\n",
    "    self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
    "    self.W = nn.Linear(n_hidden, n_class, bias=False)\n",
    "    self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "  def forward(self, hidden, X):\n",
    "    X = X.transpose(0, 1) \n",
    "    salidas, hidden = self.rnn(X, hidden)\n",
    "    salidas = salidas[-1] \n",
    "    model = self.W(salidas) + self.b \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMBZLdI0qZVy"
   },
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3373,
     "status": "ok",
     "timestamp": 1624550565429,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "rcndGvqVqaBD",
    "outputId": "b4d48c31-4c38-4bdd-ef5e-aa5d08afdb46"
   },
   "outputs": [],
   "source": [
    "n_step = 2 \n",
    "n_hidden = 5 \n",
    "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)\n",
    "batch_size = len(sentences)\n",
    "model = TextRNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_batch, target_batch = make_batch()\n",
    "input_batch = torch.FloatTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "for epoch in range(5000):\n",
    "  optimizer.zero_grad()\n",
    "  hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "  output = model(hidden, input_batch)\n",
    "\n",
    "  loss = criterion(output, target_batch)\n",
    "  if (epoch + 1) % 1000 == 0:\n",
    "    print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "input = [sen.split()[:2] for sen in sentences]\n",
    "hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios:\n",
    "\n",
    "- Implementa métodos de regularización como dropout en la RNN para prevenir el sobreajuste.\n",
    "- Además de la pérdida durante el entrenamiento, evalúa el modelo en un conjunto de datos de validación para monitorear el sobreajuste.\n",
    "- Implementa métricas de rendimiento adicionales como precisión o recall.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de capas ocultas, y números de capas en la RNN.\n",
    "- Usa técnicas como búsqueda de cuadrícula o búsqueda aleatoria para encontrar los mejores hiperparámetros.\n",
    "- Normaliza las oraciones, como convertir todo a minúsculas, para hacer el modelo menos sensible a variaciones en el formato de entrada.\n",
    "- Usa embeddings de palabras preentrenados como Word2Vec o GloVe en lugar de codificación one-hot para representar las palabras.\n",
    "- Aumenta el conjunto de oraciones para entrenar el modelo y observa cómo mejora o empeora el rendimiento.\n",
    "- Modifica la clase TextRNN para incluir dropout en la RNN y observa cómo afecta al overfitting.\n",
    "- Reemplaza nn.RNN por nn.LSTM o nn.GRU para ver si mejora el rendimiento del modelo.\n",
    "- Cambia el tamaño de n_hidden, el número de épocas, y la tasa de aprendizaje. Documenta cómo cada cambio afecta la pérdida y la precisión del modelo.\n",
    "- Implementa un gráfico para visualizar la pérdida después de cada época usando matplotlib o seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tu respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg4b78nBr9Ra"
   },
   "source": [
    "### LSTM y texto\n",
    "\n",
    "Aprender a almacenar información en intervalos de tiempo prolongados a través de la retropropagación recurrente lleva mucho tiempo. Las **LSTM (Long Short-Term Memory)** . LSTM es local en el espacio y  tiempo, su complejidad computacional por paso de tiempo y peso es *O(1)*. \n",
    "\n",
    "Este código implementa una red neuronal utilizando módulos de PyTorch para predecir la última letra de una palabra dada en base a las primeras letras de la misma. La red se basa en una arquitectura LSTM (Long Short-Term Memory), que es una variante de las redes neuronales recurrentes (RNN), adecuada para manejar dependencias a largo plazo en secuencias de datos.\n",
    "\n",
    "Función make_batch:\n",
    " - Crea lotes de datos de entrada y salida para el entrenamiento.\n",
    " - Los datos de entrada (input_batch) se generan usando codificación one-hot para todas las letras de la palabra excepto la última.\n",
    " - El objetivo (target_batch) es el índice de la última letra de cada palabra en seq_data.\n",
    "\n",
    "Clase TextLSTM:\n",
    "\n",
    " - Define el modelo LSTM con una capa lineal y un sesgo para la predicción de la letra final de cada palabra.\n",
    " - La función forward procesa los datos a través de la LSTM y la capa lineal para producir la predicción.\n",
    "\n",
    "Entrenamiento del modelo:\n",
    "\n",
    " - Configura el optimizador (Adam) y la función de pérdida (CrossEntropyLoss).\n",
    " - Durante cada época, el modelo predice la salida, calcula la pérdida, realiza backpropagation, y actualiza los pesos del modelo.\n",
    "\n",
    "Predicción:\n",
    " - Al final del entrenamiento, el modelo intenta predecir la última letra de las palabras en seq_data basándose en las tres primeras letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1624550565431,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "u-iMpZ89qPK9"
   },
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "  input_batch, target_batch = [], []\n",
    "  for seq in seq_data:\n",
    "    input = [word_dict[n] for n in seq[:-1]] \n",
    "    target = word_dict[seq[-1]] \n",
    "    input_batch.append(np.eye(n_class)[input])\n",
    "    target_batch.append(target)\n",
    "\n",
    "  return input_batch, target_batch\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextLSTM, self).__init__()\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n",
    "    self.W = nn.Linear(n_hidden, n_class, bias=False)\n",
    "    self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "  def forward(self, X):\n",
    "    input = X.transpose(0, 1)  \n",
    "\n",
    "    hidden_state = torch.zeros(1, len(X), n_hidden)  \n",
    "    cell_state = torch.zeros(1, len(X), n_hidden)   \n",
    "\n",
    "    salidas, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "    salidas = salidas[-1]  \n",
    "    model = self.W(salidas) + self.b  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7G8mVBis8zE"
   },
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2061,
     "status": "ok",
     "timestamp": 1624550567483,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "hpfP2gMvs90d",
    "outputId": "cffa9383-e6c7-4b2c-a3f1-05c3fe58e568"
   },
   "outputs": [],
   "source": [
    "n_hidden = 128 \n",
    "\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
    "n_class = len(word_dict)  \n",
    "\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n",
    "\n",
    "model = TextLSTM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_batch, target_batch = make_batch()\n",
    "input_batch = torch.FloatTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    \n",
    "for epoch in range(1000):\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  output = model(input_batch)\n",
    "  loss = criterion(output, target_batch)\n",
    "  if (epoch + 1) % 100 == 0:\n",
    "    print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "inputs = [sen[:3] for sen in seq_data]\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKJjB0IZuHC4"
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "Repite el proceso en el caso de una red GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1624550567485,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "cco2ORjqtuEe"
   },
   "outputs": [],
   "source": [
    "## Tu solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más ejercicios\n",
    "\n",
    "* Convierte input_batch a un tensor de PyTorch de manera más eficiente, tal como se discutió en la respuesta anterior, convirtiendo primero la lista de matrices a un único numpy.ndarray.\n",
    "* Además de la pérdida, evaluar el modelo en un conjunto de validación para verificar si generaliza bien a datos no vistos.\n",
    "* Usa métricas como la precisión para evaluar el rendimiento del modelo en la tarea de clasificación.\n",
    "* Experimenta con diferentes configuraciones para n_hidden, la tasa de aprendizaje, y el número de capas LSTM.\n",
    "* Implementa búsqueda de hiperparámetros automática para encontrar la mejor configuración.\n",
    "* Añade dropout a la LSTM para reducir el overfitting si es necesario.\n",
    "* Modifica la arquitectura del modelo para añadir más capas LSTM o capas lineales y observar cómo afecta el rendimiento.\n",
    "* Reemplaza la codificación one-hot por embeddings de letras para ver si mejora la capacidad del modelo para captar relaciones más complejas entre caracteres.\n",
    "* Aumenta seq_data con más palabras o utilizar un conjunto de datos más grande para entrenar el modelo y evaluar cómo escala con más datos.\n",
    "* Implementa visualizaciones durante el entrenamiento para monitorear la pérdida y la precisión a lo largo de las épocas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder recurrente\n",
    "\n",
    "Un autoencoder recurrente es una variante especial de autoencoder diseñada para manejar secuencias de datos, como series temporales o secuencias de texto. Está compuesto por dos partes principales: un encoder y un decoder, ambos utilizando arquitecturas de red neuronal recurrente, como LSTM o GRU.\n",
    "\n",
    "**Encoder**: Toma una secuencia de entrada y la procesa para producir un estado oculto o un conjunto de estados que representan la secuencia. Este estado actúa como una \"compresión\" de la información contenida en la secuencia de entrada.\n",
    "\n",
    "**Decoder:** Utiliza el estado o estados ocultos generados por el encoder para reconstruir la secuencia de entrada. El objetivo es que la secuencia reconstruida sea lo más parecida posible a la entrada original.\n",
    "\n",
    "El uso de autoencoders recurrentes es particularmente útil para tareas como el aprendizaje de características de secuencias sin supervisión, denoising de secuencias, y reducción de la dimensionalidad de datos secuenciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Definición de la clase del Autoencoder Recurrente\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(input_size=hidden_size, hidden_size=input_size, batch_first=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Encoder\n",
    "        _, (hidden, _) = self.encoder(X)\n",
    "\n",
    "        # Decoder: repetimos el estado oculto para cada paso de tiempo\n",
    "        repeated_hidden = hidden.repeat(X.size(1), 1, 1).transpose(0, 1)\n",
    "        decoded, _ = self.decoder(repeated_hidden)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "# Datos de ejemplo\n",
    "seq_data = ['hello world', 'deep learning', 'machine learning', 'hello there']\n",
    "word_list = list(set(' '.join(seq_data).split()))\n",
    "word_dict = {word: i for i, word in enumerate(word_list)}\n",
    "n_class = len(word_dict)\n",
    "\n",
    "# Crear lotes de datos\n",
    "input_batch = []\n",
    "for seq in seq_data:\n",
    "    words = seq.split()\n",
    "    input_vector = [np.eye(n_class)[word_dict[word]] for word in words]\n",
    "    input_batch.append(input_vector)\n",
    "\n",
    "# Convertir a tensores\n",
    "input_batch = torch.FloatTensor(input_batch)\n",
    "\n",
    "# Parámetros\n",
    "n_hidden = 128\n",
    "\n",
    "# Modelo\n",
    "model = RecurrentAutoencoder(input_size=n_class, hidden_size=n_hidden)\n",
    "\n",
    "# Definición de la función de pérdida y optimizador\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_batch)\n",
    "\n",
    "    # Calcular la pérdida\n",
    "    loss = criterion(outputs, input_batch)\n",
    "\n",
    "    # Backward pass y optimización\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Verificación de la reconstrucción\n",
    "model.eval()\n",
    "test_output = model(input_batch)\n",
    "print(\"Original:\", seq_data)\n",
    "print(\"Reconstruido:\", test_output.argmax(dim=2).numpy())  # Asumiendo uso de one-hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_output Tensor: Al final del ciclo de entrenamiento, se pasa input_batch (el tensor que contiene las secuencias originales codificadas) a través del modelo para obtener test_output. Este tensor contiene los vectores de salida generados por el decoder del autoencoder. Cada vector de salida tiene dimensiones correspondientes al número de clases (palabras en word_dict), y cada componente del vector representa la probabilidad (o una cantidad proporcional a la probabilidad) de que una palabra específica sea la correcta para esa posición de la secuencia.\n",
    "\n",
    "argmax(dim=2): Este método se aplica al tensor de salida para obtener los índices del valor máximo a lo largo de la dimensión específica (dim=2, que corresponde a la dimensión de las palabras en el diccionario). El resultado es un tensor donde cada elemento es el índice de la palabra con la mayor \"probabilidad\" (según el modelo) para cada posición en cada secuencia.\n",
    "\n",
    "Por ejemplo, si test_output en una posición dada es `[0.1, 0.2, 0.7]` y estos valores corresponden a las probabilidades de que las palabras 'hello', 'world', 'deep' sean la palabra correcta, argmax seleccionará '2' (índice de 'deep') porque 0.7 es el valor máximo.\n",
    "\n",
    "test_output.argmax(dim=2).numpy(): Convierte el tensor de índices a un array de NumPy para facilitar la visualización y manipulación.\n",
    "Luego, el script imprime la correspondencia entre las secuencias originales y las secuencias reconstruidas según los índices predichos. En este punto, los índices se traducen nuevamente a palabras usando el diccionario inverso number_dict para mostrar las palabras reconstruidas.\n",
    "\n",
    "Supongamos que seq_data es `['hello world', 'deep learning']` y word_dict tiene 'hello', 'world', 'deep', 'learning' mapeados a 0, 1, 2, 3 respectivamente. Si la salida es algo como:\n",
    "\n",
    "```\n",
    "Original: ['hello world', 'deep learning']\n",
    "Reconstruido: [[0, 1], [2, 3]]\n",
    "```\n",
    "\n",
    "Esto indica que el modelo ha reconstruido correctamente las secuencias. Si la salida hubiese sido `[[0, 0], [2, 2]]`, esto indicaría que el modelo predijo 'hello' en lugar de 'world' y 'deep' en lugar de 'learning'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "* En lugar de utilizar codificación one-hot, que es de alta dimensionalidad y dispersa, utiliza embeddings de palabras como GloVe o Word2Vec para representar las palabras. Esto puede ayudar a capturar mejor las relaciones semánticas entre palabras y mejorar la capacidad de reconstrucción del modelo.\n",
    "* Aumenta el número de capas o la cantidad de neuronas en las capas de LSTM. También puedes experimentar con diferentes arquitecturas como GRU en lugar de LSTM.\n",
    "* Considera el uso de una estructura de encoder y decoder más compleja, incluyendo capas densas adicionales o técnicas de regularización como dropout.\n",
    "* Experimenta con diferentes tasas de aprendizaje, algoritmos de optimización y tamaños de batch. El ajuste de estos hiperparámetros puede tener un impacto significativo en el rendimiento del modelo.\n",
    "* Utiliza LSTM o GRU bidireccionales en el encoder para capturar contextos tanto futuros como pasados de la secuencia. Esto es particularmente útil para mejorar la reconstrucción en tareas donde el contexto en ambas direcciones es importante.\n",
    "* Implementa técnicas de regularización como L1/L2 para reducir el sobreajuste. También puedes explorar autoencoders para denoising, donde el modelo aprende a reconstruir las secuencias originales a partir de versiones ruidosas de las mismas.\n",
    "* Entrena el autoencoder con un corpus de texto más grande y evalúa su capacidad para reconstruir frases o párrafos completos. Analiza los errores de reconstrucción para entender qué tipos de estructuras o palabras son más difíciles de reconstruir.\n",
    "* Modifica el dataset para introducir ruido (por ejemplo, errores tipográficos o palabras aleatorias) y entrena el autoencoder para limpiar estas secuencias. Esto puede ayudar a entender mejor cómo el modelo maneja la información esencial y cómo filtra el ruido.\n",
    "* Usa el decoder del autoencoder para generar nuevas secuencias de texto. Puedes hacer esto alimentando el estado oculto del encoder con diferentes semillas o incluso estados ocultos interpolados entre múltiples ejemplos.\n",
    "* Visualiza los estados ocultos generados por el encoder usando técnicas como t-SNE o PCA. Esto puede proporcionar insights sobre cómo el modelo está representando las secuencias de datos internamente.\n",
    "* Implementa y compara diferentes arquitecturas de autoencoders recurrentes, como aquellos con LSTM, GRU, o capas bidireccionales, y evalúa sus diferencias en términos de rendimiento y tipo de reconstrucción que logran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANVSb2emuUEv"
   },
   "source": [
    "### Bi-LSTM y texto\n",
    "\n",
    "Una red neuronal recurrente regular (RNN) se extiende a una red neuronal recurrente bidireccional (BRNN). El BRNN se puede entrenar sin la limitación de usar información de entrada solo hasta un framework futuro preestablecido. Esto se logra entrenándolo simultáneamente en la dirección del tiempo hacía adelante y atrás. \n",
    "\n",
    "La estructura bidireccional se puede modificar fácilmente para permitir una estimación eficiente de la probabilidad posterior condicional de secuencias de símbolos completas sin hacer ninguna suposición explícita sobre la forma de la distribución. \n",
    "\n",
    "El código que has proporcionado implementa una red neuronal recurrente bidireccional (BiLSTM) usando PyTorch para predecir la siguiente palabra en una secuencia basada en las palabras anteriores de una oración dada. \n",
    "\n",
    "Función make_batch:\n",
    " - Prepara los datos de entrada y salida para el modelo.\n",
    " - Recorre cada palabra en la oración, excepto la última, y crea un lote de entrada donde cada entrada es una secuencia de todas las palabras anteriores hasta la actual, codificadas mediante one-hot encoding y con padding de ceros para asegurar que todas las secuencias tengan la misma longitud (max_len).\n",
    " - La salida (target) es la palabra siguiente en la oración.\n",
    "\n",
    "Clase BiLSTM:\n",
    " - Define un modelo de LSTM bidireccional. La bidireccionalidad permite al modelo aprender dependencias en ambas direcciones de la secuencia (hacia adelante y hacia atrás).\n",
    " - La capa LSTM es seguida por una capa lineal que reduce la dimensión de salida al número de clases posibles (palabras en el vocabulario).\n",
    " - La función forward procesa las entradas a través del LSTM y la capa lineal para generar predicciones.\n",
    "\n",
    "Entrenamiento del modelo:\n",
    " - Se utiliza una función de pérdida de entropía cruzada y un optimizador Adam.\n",
    " - El modelo es entrenado por 10000 épocas, imprimiendo la pérdida cada 1000 épocas.\n",
    "\n",
    "Predicción y salida:\n",
    " - Después del entrenamiento, el modelo intenta predecir la siguiente palabra para cada palabra de entrada en la oración, y luego se imprimen estas predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1624550567487,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "ulXDoUMUuMbi"
   },
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "  input_batch = []\n",
    "  target_batch = []\n",
    "\n",
    "  words = sentence.split()\n",
    "  for i, word in enumerate(words[:-1]):\n",
    "    input = [word_dict[n] for n in words[:(i + 1)]]\n",
    "    input = input + [0] * (max_len - len(input))\n",
    "    target = word_dict[words[i + 1]]\n",
    "    input_batch.append(np.eye(n_class)[input])\n",
    "    target_batch.append(target)\n",
    "\n",
    "  return input_batch, target_batch\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BiLSTM, self).__init__()\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, bidirectional=True)\n",
    "    self.W = nn.Linear(n_hidden * 2, n_class, bias=False)\n",
    "    self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "  def forward(self, X):\n",
    "    input = X.transpose(0, 1)  \n",
    "\n",
    "    hidden_state = torch.zeros(1*2, len(X), n_hidden)   \n",
    "    cell_state = torch.zeros(1*2, len(X), n_hidden)    \n",
    "\n",
    "    salidas, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "    salidas = salidas[-1]  \n",
    "    model = self.W(salidas) + self.b \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWLv6NYvdXM"
   },
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95305,
     "status": "ok",
     "timestamp": 1624550662781,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "photoUrl": "",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "U_UHDSKKvb94",
    "outputId": "eb0b8bec-1073-4232-8f22-61f5b5fd123a"
   },
   "outputs": [],
   "source": [
    "n_hidden = 5 \n",
    "sentence = (\n",
    "        'Artificial intelligence  refers to the simulation of human intelligence '\n",
    "        'is its ability to take actions that have the best chance of achieving a specific goal '\n",
    "        'Deep learning techniques enable this automatic learning '\n",
    "    )\n",
    "\n",
    "word_dict = {w: i for i, w in enumerate(list(set(sentence.split())))}\n",
    "number_dict = {i: w for i, w in enumerate(list(set(sentence.split())))}\n",
    "n_class = len(word_dict)\n",
    "max_len = len(sentence.split())\n",
    "\n",
    "model = BiLSTM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_batch, target_batch = make_batch()\n",
    "input_batch = torch.FloatTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "for epoch in range(10000):\n",
    "  optimizer.zero_grad()\n",
    "  output = model(input_batch)\n",
    "  loss = criterion(output, target_batch)\n",
    "  if (epoch + 1) % 1000 == 0:\n",
    "    print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(sentence)\n",
    "print([number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios:\n",
    "\n",
    "* Implementa un conjunto de validación para evaluar el modelo durante el entrenamiento y ajustar los parámetros si es necesario para evitar el sobreajuste.\n",
    "* Considera utilizar embeddings preentrenados para las palabras en lugar de codificación one-hot, lo que podría ayudar a mejorar el rendimiento capturando mejor las relaciones semánticas entre palabras.\n",
    "* Experimenta con diferentes tamaños para n_hidden, tasas de aprendizaje y estructuras de red (añadiendo más capas LSTM o capas completamente conectadas).\n",
    "* Incluye capas de dropout dentro de la red para ayudar a mitigar el sobreajuste, especialmente dado el número relativamente alto de épocas de entrenamiento.\n",
    "* Modifica la red para incluir más o menos capas LSTM o cambia entre LSTM y GRU para ver cómo afectan al rendimiento.\n",
    "* Añade Dropout y Batch Normalization a la red para observar si hay mejoras en la generalización y en la estabilidad durante el entrenamiento.\n",
    "* Implementa visualizaciones de la pérdida durante el entrenamiento y las métricas de evaluación como la precisión o el recall para un conjunto de validación.\n",
    "* Utiliza un corpus más grande para el entrenamiento y compara el rendimiento del modelo entrenado con más datos frente al entrenado con menos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOgnujylZrWgq1jZocozrGs",
   "collapsed_sections": [],
   "name": "RNN-texto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
