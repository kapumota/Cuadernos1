{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introducción a Spark usando Python** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La API de Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spark está escrito en Scala, que se compila a bytecode de Java, pero puedes escribir código en Python para comunicarte con la máquina virtual de Java a través de una biblioteca llamada `py4j`. Python tiene la API más completa, pero puede ser algo limitante si necesitas utilizar un método que no está disponible o si necesitas escribir un código especializado. La latencia asociada con la comunicación entre Python y la JVM a veces puede hacer que el código se ejecute más lentamente.  \n",
    "\n",
    "Una excepción a esto es la biblioteca SparkSQL, que tiene un motor de planificación de ejecución que precompila las consultas. Incluso con esta optimización, hay casos en los que el código puede ejecutarse más lentamente que la versión nativa en Scala.  \n",
    "La recomendación general para el código en PySpark es utilizar los métodos integrados tanto como sea posible y evitar llamadas excesivamente frecuentes (iterativas) a los métodos de Spark. Si necesitas escribir código de alto rendimiento o especializado, intenta hacerlo en Scala.  \n",
    "\n",
    "Pero bueno, sabemos que Python es increíble, y sus bibliotecas de visualización son mucho mejores. ¡Así que la decisión es tuya!. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, revisaremos los conceptos básicos de Apache Spark y PySpark. Comenzaremos creando el `SparkContext` y el `SparkSession`. Luego, crearemos un `RDD` y aplicaremos algunas transformaciones y acciones básicas. Finalmente, demostraremos los conceptos básicos de `DataFrames` y `SparkSQL`.  \n",
    "\n",
    "Después de terminar este cuaderno, serás capaz de:  \n",
    "\n",
    "* Crear el `SparkContext` y el `SparkSession`  \n",
    "* Crear un `RDD` y aplicar algunas transformaciones y acciones básicas a los `RDDs`  \n",
    "* Demostrar el uso básico de `DataFrames` y `SparkSQL`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para este cuaderno, usaremos Python y Spark (PySpark). Estas bibliotecas deberían estar instaladas en tu entorno de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando paquetes requeridos\n",
    "#!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark es la API de Spark para Python. En este cuaderno, usamos PySpark para inicializar el contexto de Spark. \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1 - Spark Context y Spark Session  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este ejercicio, crearás el contexto de Spark e inicializarás la sesión de Spark necesaria para `SparkSQL` y `DataFrames`.  \n",
    "`SparkContext` es el punto de entrada para las aplicaciones de Spark y contiene funciones para crear `RDDs`, como `parallelize()`.  \n",
    "`SparkSession` es necesario para `SparkSQL` y operaciones con `DataFrame`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1: Creando la sesión y contexto de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de una sesión de Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ejemplo básico de DataFrames con Python Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de una clase de contexto de Spark\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 2: Inicializar la sesión de Spark  \n",
    "Para trabajar con *DataFrames*, solo necesitamos verificar que la instancia de la sesión de Spark ha sido creada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession está activa y lista para usar.\")\n",
    "else:\n",
    "    print(\"SparkSession no está activa. Por favor, crea una SparkSession.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Ejercicio 2: RDDs  \n",
    "En este ejercicio trabajaremos con *Resilient Distributed Datasets* (RDDs). Los RDDs son la abstracción de datos primitiva de Spark y utilizamos conceptos de programación funcional para crearlos y manipularlos.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1: Crear un RDD  \n",
    "Con fines de demostración, creamos un RDD llamando a `sc.parallelize()`. Creamos un RDD que contiene enteros del 1 al 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(range(1, 31))  # Convertimos range en una lista para compatibilidad con PySpark\n",
    "\n",
    "# Primer elemento del iterador\n",
    "print(f\"Primer elemento: {data[0]}\")\n",
    "print(f\"Tamaño de la lista: {len(data)}\")\n",
    "\n",
    "# Crear el RDD con 4 particiones\n",
    "xrangeRDD = sc.parallelize(data, numSlices=4)\n",
    "\n",
    "# Mostrar información del RDD\n",
    "print(f\"Número de particiones: {xrangeRDD.getNumPartitions()}\")\n",
    "print(\"Primeros 10 elementos del RDD:\", xrangeRDD.take(10))\n",
    "\n",
    "# Mostrar el contenido completo del RDD (solo si el conjunto de datos es pequeño)\n",
    "print(\"Contenido del RDD:\", xrangeRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2: Transformaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una transformación es una operación sobre un RDD que da como resultado un nuevo RDD. El RDD transformado se genera rápidamente porque se evalúa de manera *perezosa* (*lazy evaluation*), lo que significa que el cálculo no se realiza inmediatamente cuando se genera el nuevo RDD.  \n",
    "El RDD contendrá una serie de transformaciones o instrucciones de cálculo, que solo se ejecutarán cuando se llame a una acción.  \n",
    "\n",
    "En esta transformación, reducimos cada elemento del RDD en 1. Nota el uso de la función *lambda*. Luego filtramos el RDD para que solo contenga elementos menores a 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(\"Primeros 10 elementos de subRDD (después de map):\", subRDD.take(10))\n",
    "print(\"Elementos de filteredRDD (después de filter):\", filteredRDD.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 3: Acciones \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una transformación devuelve un resultado al *driver*. Ahora aplicamos la acción `collect()` para obtener la salida de la transformación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tarea 4: Almacenamiento en caché de datos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este simple ejemplo muestra cómo crear un RDD y almacenarlo en caché. ¡Observa la mejora de velocidad **10x**!  \n",
    "Si deseas ver el tiempo de cómputo real, accede a la interfaz de Spark UI en `host:4040`. Notarás que el segundo cálculo tomó mucho menos tiempo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "test = sc.parallelize(range(1,50000),4)\n",
    "test.cache()\n",
    "\n",
    "t1 = time.time()\n",
    "# el primer count activará la evaluación del count *y* almacenará en caché\n",
    "count1 = test.count()\n",
    "dt1 = time.time() - t1\n",
    "print(\"dt1: \", dt1)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "# el segundo count opera solo sobre los datos en caché\n",
    "count2 = test.count()\n",
    "dt2 = time.time() - t2\n",
    "print(\"dt2: \", dt2)\n",
    "\n",
    "#test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: DataFrames y SparkSQL \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con el potente motor SQL de Apache Spark, necesitas una *Spark Session*.  \n",
    "Ya la creamos en el primer ejercicio, así que verifiquemos que la sesión sigue activa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1: Crea tu primer Dataframe!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Puedes crear un conjunto de datos estructurado (similar a una tabla de base de datos) en Spark.  \n",
    "Una vez hecho esto, puedes utilizar herramientas SQL avanzadas para consultar y unir tus *DataFrames*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga los datos primero en un archivo local `people.json`\n",
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el conjunto de datos en un dataframe de Spark usando la función `read.json()`\n",
    "df = spark.read.json(\"people.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el DataFrame y su esquema de datos\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registra el DataFrame como una vista SQL temporal\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2: Explorar los datos usando funciones de DataFrame y SparkSQL  \n",
    "En esta sección, exploramos los conjuntos de datos utilizando funciones tanto de *DataFrames* como de consultas SQL correspondientes con *SparkSQL*.  \n",
    "Nota las diferentes formas de lograr la misma tarea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar y mostrar columnas básicas de datos\n",
    "\n",
    "df.select(\"name\").show()\n",
    "df.select(df[\"name\"]).show()\n",
    "spark.sql(\"SELECT name FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar filtrado básico\n",
    "\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar agregación básica de datos\n",
    "\n",
    "df.groupBy(\"age\").count().show()\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1 - RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un RDD con enteros del 1 al 50.  \n",
    "Aplica una transformación para multiplicar cada número por 2, obteniendo un RDD que contenga los primeros 50 números pares.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo inicial\n",
    "# numbers = range(1, 50)\n",
    "# numbers_RDD = ...\n",
    "# even_numbers_RDD = numbers_RDD.map(lambda x: ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doble-click **aquí** para la solución.\n",
    "\n",
    "<!-- La respuesta esta abajo:\n",
    "numbers = range(1, 50) \n",
    "numbers_RDD = sc.parallelize(numbers) \n",
    "even_numbers_RDD = numbers_RDD.map(lambda x: x * 2)\n",
    "print( even_numbers_RDD.collect()) \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 2 - DataFrames y SparkSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar al archivo `people.json`, ahora lee el archivo `people2.json` en el cuaderno, cárgalo en un *DataFrame* y aplica operaciones SQL para determinar la edad promedio en nuestro archivo `people2`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codigo inicial\n",
    "# !curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n",
    "# df = spark.read...\n",
    "# df.createTempView..\n",
    "# spark.sql(\"SELECT ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz doble clic **aquí** para ver una pista.\n",
    "\n",
    "<!-- La pista está abajo:\n",
    "\n",
    "1. La consulta SQL \"SELECT AVG(nombre_columna) FROM...\" se puede utilizar para encontrar el valor promedio de una columna.  \n",
    "2. Otra posible forma es utilizar las operaciones de *DataFrame* `select()` y `mean()`.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz doble clic **aquí** para ver la solución.\n",
    "\n",
    "<!-- La respuesta está abajo:\n",
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n",
    "df = spark.read.json(\"people2.json\").cache()\n",
    "df.createTempView(\"people2\")\n",
    "result = spark.sql(\"SELECT AVG(age) from people2\")\n",
    "result.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3 - SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cierra la *SparkSession* que creamos para este cuaderno.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz doble clic **aquí** para ver la solución.\n",
    "\n",
    "<!-- La respuesta está abajo:\n",
    "\n",
    "spark.stop() detendrá la sesión de Spark.\n",
    "\n",
    "-->  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "prev_pub_hash": "e1ad62faa424d34e707cec17aeb0f9861646fd1eb6856d7c0047335c6ed4463f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
