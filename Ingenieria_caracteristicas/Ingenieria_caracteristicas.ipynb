{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh7A_ImF0zVC"
   },
   "source": [
    "## Ingeniería de características\n",
    "La **ingeniería de características** es mucho más que un simple paso en el preprocesamiento de datos; es el arte y la ciencia de transformar datos brutos en información útil y significativa para los algoritmos de **machine learning**. A menudo, el éxito de un modelo depende en gran medida de la calidad y pertinencia de las características utilizadas, superando incluso la elección del modelo o el ajuste de hiperparámetros. Este proceso es iterativo y requiere una comprensión profunda tanto de la naturaleza de los datos como del problema a resolver.\n",
    "\n",
    "\n",
    "#### 1. Tipos de características\n",
    "\n",
    "Los datos pueden presentarse en diversos formatos, y conocer el tipo de cada característica es esencial para aplicar las técnicas adecuadas:\n",
    "\n",
    "##### **a) Características continuas**\n",
    "\n",
    "- **Definición:** Variables que pueden asumir un rango infinito o muy amplio de valores numéricos dentro de un intervalo.\n",
    "- **Ejemplos:**  \n",
    "  - Temperatura en grados Celsius.\n",
    "  - Brillo de píxeles en imágenes.\n",
    "  - Tiempo en segundos.\n",
    "- **Consideraciones:**  \n",
    "  - Estas variables pueden requerir escalado o normalización para evitar que magnitudes muy dispares afecten el entrenamiento de modelos basados en distancia (por ejemplo, KNN o regresiones lineales).\n",
    "\n",
    "##### **b) Características categóricas**\n",
    "\n",
    "- **Definición:** Variables que representan categorías o grupos discretos.\n",
    "- **Ejemplos:**  \n",
    "  - Color de un producto (rojo, azul, verde).\n",
    "  - Tipo de trabajo (docente, ingeniero, médico).\n",
    "  - País de residencia (EE.UU., España, Perú).\n",
    "- **Consideraciones:**  \n",
    "  - La conversión de estas variables a un formato numérico es imprescindible para la mayoría de los modelos. Además, es importante evaluar la cardinalidad (cantidad de categorías) para evitar problemas como la explosión de dimensiones en técnicas como One-Hot Encoding.\n",
    "  \n",
    "> **Nota adicional:** En algunos casos se consideran variables ordinales, que son categóricas pero poseen un orden natural (por ejemplo, niveles de satisfacción: bajo, medio, alto).\n",
    "\n",
    "#### 2. Técnicas de ingeniería de características\n",
    "\n",
    "La transformación y enriquecimiento de los datos se puede lograr mediante diversas técnicas, cada una con sus ventajas y desafíos. A continuación, se profundiza en algunas de las más relevantes:\n",
    "\n",
    "##### **a) Codificación de características categóricas**\n",
    "\n",
    "Debido a que los algoritmos de machine learning requieren entradas numéricas, es esencial transformar las variables categóricas. Entre las técnicas más utilizadas se encuentran:\n",
    "\n",
    "- **One-Hot encoding:**  \n",
    "  - **Funcionamiento:** Crea una columna binaria para cada categoría presente.  \n",
    "  - **Ejemplo:** Una variable \"Color\" con valores {Rojo, Azul, Verde} se transforma en tres columnas: `Color_Rojo`, `Color_Azul` y `Color_Verde`.  \n",
    "  - **Ventajas y desventajas:**  \n",
    "    - *Ventaja:* No asume un orden implícito entre categorías.  \n",
    "    - *Desventaja:* Puede generar un gran número de columnas si la variable tiene alta cardinalidad.\n",
    "  \n",
    "- **Label encoding:**  \n",
    "  - **Funcionamiento:** Asigna un entero a cada categoría (por ejemplo, Rojo → 0, Azul → 1, Verde → 2).  \n",
    "  - **Consideración importante:** Útil cuando las categorías tienen un orden natural, pero puede inducir relaciones espurias cuando no existe orden.\n",
    "  \n",
    "- **Target (Mean) encoding:**  \n",
    "  - **Funcionamiento:** Reemplaza cada categoría por el valor medio de la variable objetivo para ese grupo.  \n",
    "  - **Precaución:** Se debe evitar el sobreajuste y la fuga de datos (target leakage) mediante técnicas de validación cruzada durante la codificación.\n",
    "  \n",
    "- **Binary encoding:**  \n",
    "  - **Funcionamiento:** Convierte las categorías en representaciones binarias comprimidas, reduciendo la dimensionalidad en comparación con One-Hot Encoding.  \n",
    "  - **Aplicación:** Es particularmente útil para variables con alta cardinalidad.\n",
    "\n",
    "- **Otras técnicas:**  \n",
    "  - *Frequency Encoding:* Sustituye cada categoría por su frecuencia de aparición en los datos, lo que puede ser útil cuando la prevalencia de una categoría es relevante para la predicción.\n",
    "\n",
    "\n",
    "##### **b) Escalado y normalización de características numéricas**\n",
    "\n",
    "Los algoritmos que utilizan métodos basados en distancias o que se basan en gradientes (como la regresión logística o las redes neuronales) suelen requerir datos en escalas similares para garantizar una convergencia adecuada:\n",
    "\n",
    "- **Min-Max Scaling:**  \n",
    "  - **Fórmula:**  \n",
    "    $$\n",
    "    X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "    $$\n",
    "  - **Resultado:** Escala las variables a un rango específico, usualmente `[0, 1]`.\n",
    "  \n",
    "- **Z-Score normalization (Estandarización):**  \n",
    "  - **Fórmula:**  \n",
    "    $$\n",
    "    X' = \\frac{X - \\mu}{\\sigma}\n",
    "    $$\n",
    "  - **Resultado:** Transforma los datos para que tengan media 0 y desviación estándar 1.\n",
    "  \n",
    "- **Log Transformation:**  \n",
    "  - **Uso:** Se aplica en variables con distribución sesgada para aproximar una distribución normal, lo que puede mejorar el desempeño de ciertos modelos.\n",
    "  \n",
    "- **Robust scaling:**  \n",
    "  - **Característica:** Utiliza la mediana y los cuartiles, siendo menos sensible a valores atípicos.\n",
    "\n",
    "\n",
    "##### **c) Generación de nuevas características**\n",
    "\n",
    "La creación de nuevas variables a partir de las existentes puede revelar relaciones ocultas en los datos:\n",
    "\n",
    "- **Interacciones y transformaciones no lineales:**  \n",
    "  - **Ejemplos:**  \n",
    "    - Producto o división de variables: $\\text{feature}_1 \\times \\text{feature}_2$ o $\\frac{\\text{feature}_1}{\\text{feature}_2}$.  \n",
    "    - Transformaciones no lineales: $\\text{feature}_1^2$, $\\sqrt{\\text{feature}_2}$ o funciones logarítmicas.\n",
    "  \n",
    "- **Características temporales:**  \n",
    "  - **Extracción de información:** A partir de fechas se pueden extraer el día, mes, año, hora, o incluso identificar estacionalidades y tendencias.\n",
    "  - **Ejemplo:** En series de tiempo, calcular la diferencia en días o el promedio móvil puede ser muy útil.\n",
    "  \n",
    "- **Reducción de dimensionalidad:**  \n",
    "  - **PCA (Análisis de componentes principales):** Permite transformar las variables originales en un conjunto menor de componentes que explican la mayor parte de la varianza.\n",
    "  - **Otros métodos:** Técnicas como t-SNE o UMAP se utilizan para la visualización y análisis exploratorio, aunque no necesariamente para entrenamiento.\n",
    "  \n",
    "- **Características derivadas con modelos:**  \n",
    "  - **Autoencoders:** Redes neuronales entrenadas para aprender una representación comprimida y eficiente de los datos, útiles para descubrir estructuras latentes.\n",
    "  \n",
    "- **Ingeniería automática de características:**  \n",
    "  - **Herramientas como Featuretools:** Automatizan la generación de características a partir de relaciones entre tablas y datos temporales, facilitando el descubrimiento de patrones complejos.\n",
    "\n",
    "\n",
    "##### **d) Manejo de datos faltantes**\n",
    "\n",
    "Los valores ausentes pueden degradar la calidad del modelo, por lo que su tratamiento es fundamental:\n",
    "\n",
    "- **Eliminación de registros:**  \n",
    "  - Útil cuando la proporción de datos faltantes es mínima y no se pierde información valiosa.\n",
    "  \n",
    "- **Imputación de valores:**  \n",
    "  - **Métodos simples:** Reemplazo por la media, mediana o moda.  \n",
    "  - **Métodos avanzados:**  \n",
    "    - Imputación basada en modelos, como KNN Imputation o técnicas de regresión.  \n",
    "    - Imputación múltiple, que genera varias estimaciones para capturar la incertidumbre en los datos faltantes.\n",
    "  \n",
    "- **Uso de indicadores de faltantes:**  \n",
    "  - A veces, la ausencia de datos en sí puede ser informativa. Se pueden crear variables binarias que indiquen si un valor estaba ausente.\n",
    "\n",
    "##### **e) Selección de características**\n",
    "\n",
    "No todas las variables disponibles aportan valor al modelo. La selección de características ayuda a enfocarse en las variables más relevantes, mejorando la eficiencia y reduciendo el riesgo de sobreajuste:\n",
    "\n",
    "- **Métodos basados en filtros:**  \n",
    "  - Se utilizan métricas estadísticas para evaluar la relación entre cada característica y la variable objetivo (por ejemplo, coeficientes de correlación, pruebas Chi-cuadrado o ANOVA).\n",
    "  \n",
    "- **Métodos basados en wrappers:**  \n",
    "  - **Recursive feature elimination (RFE):** Iterativamente elimina las características menos importantes evaluando el rendimiento del modelo.\n",
    "  - **Búsqueda con validación cruzada:** Se evalúa el conjunto óptimo de características mediante técnicas de validación.\n",
    "  \n",
    "- **Métodos basados en modelos (Embedded):**  \n",
    "  - Algoritmos como LASSO (regularización L1) o modelos basados en árboles (Random Forests, Gradient Boosting) pueden asignar pesos o importancias a cada variable durante el entrenamiento, facilitando la selección.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Algoritmos y herramientas utilizados en ingeniería de características\n",
    "\n",
    "La aplicación de algoritmos en el proceso de ingeniería de características no solo se limita a la selección y transformación, sino que también puede ayudar a extraer patrones complejos:\n",
    "\n",
    "- **Árboles de decisión y ensambles:**  \n",
    "  - Además de servir para la predicción, estos algoritmos permiten identificar automáticamente la importancia relativa de las características. Técnicas como Random Forests o Gradient Boosting son ampliamente utilizadas para este propósito.\n",
    "  \n",
    "- **Análisis de componentes principales (PCA):**  \n",
    "  - Herramienta estadística que ayuda a reducir la dimensionalidad, eliminando la redundancia en los datos y mejorando la eficiencia computacional.\n",
    "  \n",
    "- **Autoencoders:**  \n",
    "  - Redes neuronales diseñadas para aprender representaciones compactas de datos, útiles en escenarios de reducción de ruido y detección de anomalías.\n",
    "  \n",
    "- **Embeddings y técnicas de representación:**  \n",
    "  - En el procesamiento de lenguaje natural, técnicas como Word2Vec, FastText o TF-IDF permiten convertir texto en vectores numéricos de forma que se capturen las relaciones semánticas.\n",
    "  \n",
    "- **Ingeniería automática de características:**  \n",
    "  - Herramientas como **Featuretools** permiten automatizar la generación y combinación de características, lo que es especialmente útil cuando se trabaja con bases de datos relacionales y series temporales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpVk-cOg0zVG"
   },
   "source": [
    "#### **Variables categóricas**\n",
    "\n",
    "Como ejemplo, utilizaremos el conjunto de datos de ingresos de adultos en los Estados Unidos, derivado de la base de datos del censo de 1994. La tarea de este conjunto de datos es predecir si un trabajador tiene un ingreso **superior a $50,000$** o **inferior o igual a $50,000$**. Las características en este conjunto de datos incluyen la edad de los trabajadores, su situación laboral (autónomos, empleados de la industria privada, empleados del gobierno, etc.), su nivel educativo, su género, sus horas de trabajo por semana, su ocupación y más.\n",
    "\n",
    "\n",
    "##### **Tabla 1: Información de empleados**\n",
    "| age | workclass         | education    | gender | hours-per-week | occupation        | income |\n",
    "|-----|------------------|-------------|--------|---------------|------------------|--------|\n",
    "| 39  | State-gov        | Bachelors   | Male   | 40            | Adm-clerical     | <=50K  |\n",
    "| 50  | Self-emp-not-inc | Bachelors   | Male   | 13            | Exec-managerial  | <=50K  |\n",
    "| 38  | Private          | HS-grad     | Male   | 40            | Handlers-cleaners| <=50K  |\n",
    "| 53  | Private          | 11th        | Male   | 40            | Handlers-cleaners| <=50K  |\n",
    "| 28  | Private          | Bachelors   | Female | 40            | Prof-specialty   | <=50K  |\n",
    "| 37  | Private          | Masters     | Female | 40            | Exec-managerial  | <=50K  |\n",
    "| 49  | Private          | 9th         | Female | 16            | Other-service    | <=50K  |\n",
    "| 52  | Self-emp-not-inc | HS-grad     | Male   | 45            | Exec-managerial  | >50K   |\n",
    "| 31  | Private          | Masters     | Female | 50            | Prof-specialty   | >50K   |\n",
    "| 42  | Private          | Bachelors   | Male   | 40            | Exec-managerial  | >50K   |\n",
    "| 37  | Private          | Some-college| Male   | 80            | Exec-managerial  | >50K   |\n",
    "\n",
    "\n",
    "La tarea se plantea como un **problema de clasificación** con dos clases:  \n",
    "- **`<=50K`** → Personas con ingresos inferiores o iguales a $50,000$.  \n",
    "- **`>50K`** → Personas con ingresos superiores a $50,000$.  \n",
    "\n",
    "También sería posible predecir el ingreso exacto y tratar el problema como una **tarea de regresión**. Sin embargo, esto sería mucho más complejo, y la división en **$50,000** es una frontera útil para analizar.\n",
    "\n",
    "En este conjunto de datos, las características `age` y `hours-per-week` son **variables continuas**, que sabemos cómo tratar. Sin embargo, las características **`workclass`**, **`education`**, **`gender`** y **`occupation`** son **variables categóricas**.  \n",
    "\n",
    "Las **variables categóricas** provienen de una lista fija de valores posibles, a diferencia de un rango numérico. Además, denotan una propiedad **cualitativa**, en lugar de representar una cantidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuJnx_oP0zVK"
   },
   "source": [
    "#### **One-Hot encoding (variables dummy)**\n",
    "\n",
    "La forma más común de representar **variables categóricas** es utilizando **one-hot encoding** (también conocida como **one-out-of-N encoding** o **variables dummy**).  \n",
    "\n",
    "La idea detrás de esta técnica es **reemplazar una variable categórica con una o más características nuevas** que pueden tomar valores de **0 o 1**. Estos valores son especialmente útiles en modelos de clasificación binaria y en la mayoría de los algoritmos de **machine learning**, como los implementados en `scikit-learn`.  \n",
    "\n",
    "Podemos representar cualquier cantidad de categorías introduciendo una **nueva característica por cada categoría posible**, como se muestra a continuación.\n",
    "\n",
    "Supongamos que la variable **`workclass`** tiene los siguientes posibles valores:\n",
    "\n",
    "- `Government Employee`\n",
    "- `Private Employee`\n",
    "- `Self Employed`\n",
    "- `Self Employed Incorporated`\n",
    "\n",
    "Para codificar estos cuatro valores, creamos **cuatro nuevas características** llamadas:  \n",
    "**`Government Employee`**, **`Private Employee`**, **`Self Employed`** y **`Self Employed Incorporated`**.  \n",
    "\n",
    "Cada una de estas nuevas características toma el valor `1` si el valor original de **`workclass`** coincide con la categoría correspondiente y `0` en caso contrario. Es decir, **exactamente una de estas cuatro características será 1 para cada punto de datos**.  \n",
    "\n",
    "Esta es la razón por la cual esta técnica se denomina **one-hot encoding** o **one-out-of-N encoding**.\n",
    "\n",
    "Después de aplicar esta codificación, la variable **original** `workclass` es eliminada y solo se mantienen las nuevas variables **0-1** cuando se entrenan modelos de **machine learning**.\n",
    "\n",
    "##### **Tabla 2: Codificación One-Hot de `workclass`**\n",
    "| workclass                  | Government Employee | Private Employee | Self Employed | Self Employed Incorporated |\n",
    "|----------------------------|--------------------|------------------|--------------|----------------------------|\n",
    "| Government Employee        | 1                  | 0                | 0            | 0                          |\n",
    "| Private Employee           | 0                  | 1                | 0            | 0                          |\n",
    "| Self Employed              | 0                  | 0                | 1            | 0                          |\n",
    "| Self Employed Incorporated | 0                  | 0                | 0            | 1                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXMgN8k40zVR"
   },
   "source": [
    "\n",
    "Hay dos formas de convertir los datos a una codificación **one-hot** para variables categóricas: utilizando **`pandas`** o **`scikit-learn`**.  \n",
    "\n",
    "Usar **`pandas`** es un poco más sencillo, por lo que seguiremos este enfoque.  \n",
    "\n",
    "Primero, cargamos los datos desde un archivo de valores separados por comas (**CSV**) utilizando **`pandas`**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2lLZGIs0zVU",
    "outputId": "81713193-6524-4fd2-9ae0-5fc94952b334"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datos = pd.read_csv(\"adult.data\", header=None, index_col=False,\n",
    "                   names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                          'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "                          'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "                          'income'])\n",
    "# Seleccionamos algunas columnas\n",
    "datos = datos[['age', 'workclass', 'education', 'gender', 'hours-per-week','occupation', 'income']]\n",
    "display(datos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTPuhEgr0zVp"
   },
   "source": [
    "#### **Comprobación de datos categóricos codificados como cadenas**\n",
    "\n",
    "Después de leer un conjunto de datos como este, es recomendable verificar si una columna contiene realmente datos **categóricos significativos**.  \n",
    "\n",
    "Al trabajar con datos ingresados manualmente (por ejemplo, por usuarios en un sitio web), puede que no exista un conjunto fijo de categorías. Además, **diferencias en la ortografía y el uso de mayúsculas** pueden requerir preprocesamiento.  \n",
    "\n",
    "Por ejemplo, es posible que algunas personas especifiquen el género como `\"male\"`, mientras que otras lo indiquen como `\"man\"`. En este caso, podría ser conveniente representar ambas entradas con la misma categoría.  \n",
    "\n",
    "Una forma efectiva de verificar el contenido de una columna es utilizando la función **`value_counts()`** de una **Serie de `pandas`**, que nos muestra los valores únicos y la frecuencia con la que aparecen:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VslxkQF0zVq",
    "outputId": "c6b3c50c-9c86-4194-a484-a328eecfe20c"
   },
   "outputs": [],
   "source": [
    "print(datos.gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDDuLcJZ0zV2"
   },
   "source": [
    "Hay una manera muy sencilla de **codificar datos categóricos** en **`pandas`**, utilizando la función **`get_dummies()`**.  \n",
    "\n",
    "Esta función **transforma automáticamente** todas las columnas que tienen tipo **`object`** (cadenas de texto) o son de tipo **categórico** (un tipo de dato especial en `pandas`, que aún no hemos explorado).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOKVZ7Nv0zV4",
    "outputId": "f80c13c7-c9a2-4e90-b479-6429f0e3ddea"
   },
   "outputs": [],
   "source": [
    "print(\"Caracteristicas originales:\\n\", list(datos.columns), \"\\n\")\n",
    "datos_dummies = pd.get_dummies(datos)\n",
    "print(\"Caracteristicas despues de  get_dummies:\\n\", list(datos_dummies.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ubFTsyo0zWF",
    "outputId": "33ac6d0f-f5c0-4a6e-9f41-ca325eef89b4"
   },
   "outputs": [],
   "source": [
    "datos_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErvmL4q80zWS"
   },
   "source": [
    "Ahora podemos utilizar el atributo **`values`** para convertir el **DataFrame** `datos_dummies` en una matriz de **NumPy** y luego entrenarlo en un modelo de **machine learning**.  \n",
    "\n",
    "Es importante asegurarse de **separar la variable objetivo** (que ahora está codificada en dos columnas **`income`**) de los datos antes de entrenar el modelo. **Incluir la variable de salida o alguna propiedad derivada de ella en las características del modelo es un error común en la creación de modelos supervisados de machine learning**.\n",
    "\n",
    "**Importante**: La indexación de columnas en **`pandas`** **incluye** el límite superior del rango, por lo que `age:occupation_Transport-moving` **incluye** `occupation_Transport-moving`.  \n",
    "\n",
    "Esto **es diferente** a un **slicing** en una matriz de **NumPy**, donde el límite superior del rango **no está incluido**. Por ejemplo, en NumPy:\n",
    "\n",
    "```python\n",
    "np.arange(11)[0:10]\n",
    "```\n",
    "No incluye la entrada con el índice `10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcUXKl2A0zWU",
    "outputId": "a25afaf1-f16e-4bec-f805-5376bc1e6739"
   },
   "outputs": [],
   "source": [
    "# Seleccionar características (corrigiendo el error de ix)\n",
    "caracteristica = datos_dummies.loc[:, 'age':'occupation_ Transport-moving']\n",
    "\n",
    "# Extraemos matrices NumPy\n",
    "X = caracteristica.values\n",
    "y = datos_dummies['income_ >50K'].values\n",
    "\n",
    "print(\"X.shape: {} y.shape: {}\".format(X.shape, y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yulzh5oe0zWh"
   },
   "source": [
    "Ahora los datos están representados de una manera que scikit-learn puede funcionar,  podemos proceder como de costumbre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fj4v_JN_0zWj",
    "outputId": "e8e5a9d5-0831-43bb-bb66-23e1295b7d38"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Dividimos los datos en conjunto de entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "# Escalamos los datos para mejorar la estabilidad numérica\n",
    "scaler = StandardScaler()\n",
    "X_entrenamiento = scaler.fit_transform(X_entrenamiento)\n",
    "X_prueba = scaler.transform(X_prueba)\n",
    "\n",
    "# Definimos el modelo de regresión logística con más iteraciones y un solver adecuado\n",
    "logreg = LogisticRegression(solver='saga', max_iter=1000, random_state=0)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "logreg.fit(X_entrenamiento, y_entrenamiento)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "puntuacion = logreg.score(X_prueba, y_prueba)\n",
    "print(\"Puntuación del modelo en el conjunto de prueba: {:.2f}\".format(puntuacion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MAl3-iZ0zWr"
   },
   "source": [
    "Imaginemos que tenemos los conjuntos de **entrenamiento** y **prueba** almacenados en **dos DataFrames diferentes**.  \n",
    "\n",
    "Si el valor **`Private Employee`**, correspondiente a la característica **`workclass`**, no aparece en el conjunto de prueba, **`pandas`** asumirá que solo existen **tres valores posibles** para esta característica y creará únicamente **tres nuevas variables dummy** en lugar de cuatro.\n",
    "Como resultado, nuestros conjuntos de **entrenamiento** y **prueba** tendrán un **número diferente de características**, lo que impedirá aplicar el modelo entrenado correctamente al conjunto de prueba.\n",
    "\n",
    "**Peor aún**, si la característica **`workclass`** contiene los valores **`Government Employee`** y **`Private Employee`** en el conjunto de **entrenamiento**, pero **`Self Employed`** y **`Self Employed Incorporated`** en el conjunto de **prueba**, **pandas** creará dos variables **dummy en ambos conjuntos**.  \n",
    "\n",
    "Sin embargo, aunque el número de características sea el mismo, **su significado será completamente diferente**. La columna que representa **`Government Employee`** en el conjunto de **entrenamiento** podría representar **`Self Employed`** en el conjunto de **prueba**.\n",
    "\n",
    "**Problema grave en machine learning**  \n",
    "Si construimos un **modelo de machine learning** con estos datos, su desempeño será **muy deficiente**. El modelo asumirá que las columnas representan las mismas categorías (porque están en la misma posición), cuando en realidad contienen información completamente diferente.\n",
    "\n",
    "#### **Solución**\n",
    "Para evitar este problema, podemos aplicar `get_dummies` de dos maneras:  \n",
    "\n",
    "- **Llamar a `get_dummies` en un único `DataFrame`** que contenga **tanto los datos de entrenamiento como los de prueba**.\n",
    "- **Asegurarnos de que los nombres de las columnas sean los mismos** en ambos conjuntos después de aplicar `get_dummies`, garantizando que mantengan la misma semántica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYZmFM3w0zWt"
   },
   "source": [
    "#### **Los números también pueden representar variables categóricas**\n",
    "\n",
    "En el conjunto de datos de adultos, las **variables categóricas** se codificaron como **cadenas**. Esto tiene ventajas y desventajas:  \n",
    "\n",
    "- Por un lado, **marcar una variable como categórica** usando cadenas evita confusiones.\n",
    "- Por otro lado, **abre la posibilidad de errores ortográficos** y de inconsistencias en la escritura.  \n",
    "\n",
    "Sin embargo, en muchas situaciones, ya sea para **optimizar el almacenamiento** o debido a la forma en que se recopilan los datos, las **variables categóricas se codifican como números enteros**.\n",
    "\n",
    "##### **Ejemplo: Datos del censo**\n",
    "\n",
    "Imaginemos que los datos del censo en el conjunto de datos **`adult`** se recopilaron mediante un **cuestionario**, y las respuestas para **`workclass`** fueron registradas de la siguiente manera:\n",
    "\n",
    "- `0` → Primera casilla marcada  \n",
    "- `1` → Segunda casilla marcada  \n",
    "- `2` → Tercera casilla marcada  \n",
    "- ...y así sucesivamente.\n",
    "\n",
    "Como resultado, en lugar de contener valores de tipo **cadena** como `\"Private\"`, la columna ahora contendrá **números enteros** del `0` al `8`.  \n",
    "\n",
    "**Problema**: A simple vista, no es evidente si esta variable debe tratarse como **continua** o **categórica**. Sin embargo, al saber que estos números representan el estado de empleo, es claro que son **categorías discretas** y no deben ser modeladas como una única variable continua.\n",
    "\n",
    "##### **¿Cuándo una variable entera debe tratarse como categórica?**\n",
    "Las **características categóricas** a menudo se codifican con **números enteros**. Sin embargo, el hecho de que sean **números** no significa que deban tratarse necesariamente como **variables continuas**.  \n",
    "\n",
    "**Reglas para decidir cómo tratarlas**:  \n",
    "\n",
    "- **Si no hay un orden lógico entre los valores** → Se deben tratar como **categóricas** y codificarlas con **one-hot encoding**.  \n",
    "   - Ejemplo: `\"workclass\"` (`0` = `Private Employee`, `1` = `Government Employee`, etc.).  \n",
    "   - No hay una relación de orden entre `\"Private Employee\"` y `\"Government Employee\"`, por lo que **no deben tratarse como continuas**.  \n",
    "\n",
    "- **Si los valores representan una escala ordinal** → Dependerá de la tarea y del algoritmo de **machine learning**.  \n",
    "   - Ejemplo: **Clasificaciones de cinco estrellas** (`1` a `5`).  \n",
    "   - Dependiendo del contexto, **puede ser mejor tratarlas como continuas o como categóricas**.\n",
    "\n",
    "##### **`pandas.get_dummies()` vs `OneHotEncoder`**\n",
    "- La función **`get_dummies()` de pandas** trata **todos los números como continuos**, por lo que **no creará variables dummy** para columnas numéricas.\n",
    "- Para evitar este problema, puedes utilizar el **`OneHotEncoder` de `scikit-learn`**, que permite especificar **qué variables son continuas y cuáles son categóricas**.\n",
    "- Otra opción es **convertir las columnas numéricas en cadenas** antes de aplicar `get_dummies()`.\n",
    "\n",
    "\n",
    "##### **Ejemplo en Python**\n",
    "Para ilustrar esto, creamos un **DataFrame** con dos columnas:  \n",
    "\n",
    "- **Una columna con valores categóricos como cadenas**.\n",
    "- **Otra con valores categóricos codificados como enteros**.  \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creamos un DataFrame con una columna categórica como string y otra como entero\n",
    "df = pd.DataFrame({\n",
    "    'workclass_str': ['Private', 'Self Employed', 'Government'],\n",
    "    'workclass_int': [0, 1, 2]  # Codificación numérica\n",
    "})\n",
    "\n",
    "# Aplicamos get_dummies en la columna string\n",
    "df_dummies = pd.get_dummies(df, columns=['workclass_str'])\n",
    "\n",
    "# Usamos OneHotEncoder de sklearn para la columna numérica\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_values = encoder.fit_transform(df[['workclass_int']])\n",
    "encoded_df = pd.DataFrame(encoded_values, columns=encoder.get_feature_names_out(['workclass_int']))\n",
    "\n",
    "# Concatenamos los resultados\n",
    "final_df = pd.concat([df_dummies, encoded_df], axis=1)\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(final_df)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrLLgeWx0zWv",
    "outputId": "59329230-8a63-4ae1-c45c-54570a08ffd8"
   },
   "outputs": [],
   "source": [
    "# creamos un DataFrame con una característica entera y una característica de cadena categórica\n",
    "\n",
    "demo_df = pd.DataFrame({'Caracteristica entera': [0, 1, 2, 1],\n",
    "                        'Caracteristica categorica': ['socks', 'fox', 'socks', 'box']})\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MISHFLIi0zW6"
   },
   "source": [
    "El uso de `get_dummies` solo codificará las característica de cadena y no cambiará la característica de entero, como puedes ver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h5IcdDf0zW8",
    "outputId": "fa23a2f2-61db-435d-fc0a-4a5cb153bcd9"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(demo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qmhkacI0zXE"
   },
   "source": [
    "Si queremos codificar **también la columna numérica** (`Caracteristica entera`), primero debemos convertirla en tipo `str` para que `get_dummies()` la reconozca como categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpBbYgJV0zXG",
    "outputId": "7fe6073c-be6a-4f77-a3f8-079dbe245b25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_df['Caracteristica entera'] = demo_df['Caracteristica entera'].astype(str)\n",
    "pd.get_dummies(demo_df, columns=['Caracteristica entera', 'Caracteristica categorica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ambas columnas (`Caracteristica entera` y `Caracteristica categorica`) han sido codificadas en variables dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uso de `OneHotEncoder` de `sklearn`**\n",
    "\n",
    "Otra forma de realizar la codificación one-hot es mediante OneHotEncoder de scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "demo_df['Caracteristica entera'] = demo_df['Caracteristica entera'].astype(str)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_values = encoder.fit_transform(demo_df[['Caracteristica entera', 'Caracteristica categorica']])\n",
    "encoded_df = pd.DataFrame(encoded_values, columns=encoder.get_feature_names_out(['Caracteristica entera', 'Caracteristica categorica']))\n",
    "\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0MqICFq0zXP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**Explicación paso a paso:**\n",
    "1. Convertimos `Caracteristica entera` a cadena para que `OneHotEncoder` la reconozca como categórica.\n",
    "2. Creamos un objeto `OneHotEncoder` y establecemos `sparse_output=False` para obtener una matriz densa (por defecto, devuelve una matriz dispersa).\n",
    "3. Aplicamos `fit_transform()` a ambas columnas categóricas, obteniendo una matriz con los valores codificados.\n",
    "4. Convertimos la matriz en un `DataFrame` y asignamos nombres de columna con `get_feature_names_out()`. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de binning y transformación en datos continuos\n",
    "\n",
    "Una técnica adicional importante es la transformación de variables continuas en variables categóricas mediante binning. Esto consiste en dividir el rango de valores continuos en intervalos (o bins), y luego asignar a cada dato el índice del intervalo al que pertenece.\n",
    "\n",
    "**Ejemplo con el conjunto de datos `wave`**\n",
    "\n",
    "Se crea un conjunto de datos sintético mediante la función `hacer_wave`, que genera una característica continua y una salida que es una combinación de una función seno y una componente lineal.\n",
    "\n",
    "A continuación, se muestra una **comparación** entre un **modelo de regresión lineal** y un **regresor basado en árboles de decisión** en este conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BuZmXO20zXR"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def hacer_wave(n_muestras=100):\n",
    "    rnd = np.random.RandomState(42)\n",
    "    x = rnd.uniform(-3, 3, size=n_muestras)\n",
    "    y_no_ruido = (np.sin(4 * x) + x)\n",
    "    y = (y_no_ruido + rnd.normal(size=len(x))) / 2\n",
    "    return x.reshape(-1, 1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OAaqiv50zXc",
    "outputId": "d4067015-ab4f-4519-9274-2f4dedf808e1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = hacer_wave(n_muestras=100)\n",
    "linea = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n",
    "reg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\n",
    "\n",
    "plt.plot(linea, reg.predict(linea), label=\"Arboles de decision\")\n",
    "reg = LinearRegression().fit(X, y)\n",
    "plt.plot(linea, reg.predict(linea), label=\"Regresion lineal\")\n",
    "plt.plot(X[:, 0], y, 'o', c='k')\n",
    "plt.ylabel(\"Salida de regresion\")\n",
    "plt.xlabel(\"Caracteristica de entradas\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figura muestra un conjunto de puntos (en negro) representando datos de un problema de regresión. La línea naranja corresponde a un modelo de regresión lineal, que intenta ajustar una única recta a los datos. Por otro lado, la línea azul representa un árbol de decisión, que divide el espacio de la característica en regiones y asigna un valor constante en cada una. Se observa que la regresión lineal capta una tendencia general con una sola pendiente, mientras que el árbol de decisión produce una aproximación escalonada más flexible, ajustándose localmente a los datos. Sin embargo, puede generar saltos abruptos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_secJlA0zXj"
   },
   "source": [
    "#### Binning\n",
    "\n",
    "Imaginamos una partición del rango de entrada para la característica (en este caso, los números de -3 a 3) en un número fijo de *bins* (contenedores), por ejemplo, 10. Cada punto de datos se representará según el contenedor al que pertenece. Para determinar esto, primero tenemos que definir los *bins*. En este caso, definiremos 10 intervalos igualmente espaciados entre -3 y 3. Utilizamos la función `np.linspace` para esto, creando 11 puntos que definen 10 *bins* (son los espacios entre dos límites consecutivos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beVMwBAT0zXk",
    "outputId": "dc9379f9-047d-431f-99f9-d0c024a0b0b1"
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(-3, 3, 11)\n",
    "print(\"bins: {}\".format(bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3WawQsm0zXy"
   },
   "source": [
    "A continuación, determinamos a qué *bin* pertenece cada punto de datos. Esto se puede calcular fácilmente usando la función `np.digitize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW56ZKiw0zX0",
    "outputId": "265c570e-09c7-4eb1-aa4d-9386ba648a2b"
   },
   "outputs": [],
   "source": [
    "bin_pertenece = np.digitize(X, bins=bins)\n",
    "print(\"\\nPuntos de datos:\\n\", X[:5])\n",
    "print(\"\\n*Bin* al que pertenecen los puntos de datos:\\n\", bin_pertenece[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt73Acyy0zX8"
   },
   "source": [
    "Lo que hicimos aquí fue transformar la única característica de entrada continua del conjunto de datos `wave` en una característica categórica que codifica en qué *bin* se encuentra cada punto de datos. \n",
    "\n",
    "Para usar un modelo de scikit‑learn en estos datos, transformamos esta característica discreta a una codificación *one‑hot* usando el `OneHotEncoder` del módulo `preprocessing`. Este codificador realiza una transformación similar a `pandas.get_dummies`, aunque actualmente solo funciona en variables categóricas que son enteras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzLZn2YX0zX-",
    "outputId": "7b85cce1-0a04-4414-a9d7-d3e4aa30c9af"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Transformamos usando OneHotEncoder.\n",
    "# NOTA: En versiones recientes de scikit‑learn, el argumento 'sparse' ha sido reemplazado por 'sparse_output'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# encoder.fit encuentra los valores únicos que aparecen en bin_pertenece.\n",
    "encoder.fit(bin_pertenece)\n",
    "# transform crea la codificación one‑hot.\n",
    "X_binned = encoder.transform(bin_pertenece)\n",
    "print(X_binned[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxMquH590zYG"
   },
   "source": [
    "Debido a que especificamos 10 *bins*, el conjunto de datos transformado `X_binned` ahora se compone de 10 características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9n35zlb0zYH",
    "outputId": "0084d5f8-88f2-4c9b-9625-db69a6d9356b"
   },
   "outputs": [],
   "source": [
    "print(\"Dimensión de X_binned: {}\".format(X_binned.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoFuSjQb0zYS"
   },
   "source": [
    "Ahora construimos un nuevo modelo de regresión lineal y un modelo de árbol de decisión usando los datos codificados con *one‑hot*. El resultado se visualiza en la siguiente figura, junto con los límites de los contenedores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7VHK4vs0zYT",
    "outputId": "b15b688d-8aa6-4d05-f5ae-37731872bce1"
   },
   "outputs": [],
   "source": [
    "linea_binned = encoder.transform(np.digitize(linea, bins=bins))\n",
    "reg = LinearRegression().fit(X_binned, y)\n",
    "plt.plot(linea, reg.predict(linea_binned), label='regresión lineal agrupada')\n",
    "\n",
    "reg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\n",
    "plt.plot(linea, reg.predict(linea_binned), label='árboles de decisión agrupados')\n",
    "\n",
    "plt.plot(X[:, 0], y, 'o', c='k')\n",
    "plt.vlines(bins, -3, 3, linewidth=1, alpha=.2)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"Salida de regresión\")\n",
    "plt.xlabel(\"Característica de entrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a45ZcvZy0zYZ"
   },
   "source": [
    "La línea punteada y la línea continua están exactamente una encima de la otra, lo que significa que el modelo de regresión lineal y el árbol de decisión hacen exactamente las mismas predicciones. Para cada contenedor, ambos modelos predicen un valor constante. Como las características son constantes dentro de cada contenedor, cualquier modelo debe predecir el mismo valor para todos los puntos dentro de un contenedor. Al comparar lo que los modelos aprendieron antes y después de agrupar las características, el modelo lineal se volvió mucho más flexible, ya que ahora tiene un valor diferente para cada contenedor, mientras que el modelo de árbol de decisión se volvió menos flexible.\n",
    "\n",
    "Las transformaciones basadas en *bins* generalmente no aportan beneficios a los modelos basados en árboles, ya que estos pueden aprender a dividir los datos en cualquier punto. En cierto sentido, esto significa que los árboles de decisión pueden aprender agrupamientos que sean más útiles para la predicción. En cambio, el modelo lineal se benefició enormemente de la transformación de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkRE8vQV0zYb"
   },
   "source": [
    "### Interacciones y polinomios\n",
    "\n",
    "Otra forma de enriquecer la representación de características, particularmente para modelos lineales, es agregar características de interacción y características polinómicas a partir de los datos originales. Este tipo de ingeniería de características se utiliza a menudo en el modelado estadístico, pero también es común en muchas aplicaciones prácticas de *machine learning*.\n",
    "\n",
    "Como primer ejemplo, recordemos la figura anterior: el modelo lineal aprendió un valor constante para cada contenedor en el conjunto de datos `wave`. Sin embargo, sabemos que los modelos lineales pueden aprender no solo desplazamientos, sino también pendientes. Una forma de agregar una pendiente al modelo lineal sobre los datos agrupados es volver a incluir la característica original (el eje *x* en el gráfico). Esto conduce a un conjunto de datos de 11 dimensiones, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTMI2HqN0zYi",
    "outputId": "078fd4ab-328b-472a-e436-bd201819073d"
   },
   "outputs": [],
   "source": [
    "X_combinado = np.hstack([X, X_binned])\n",
    "print(X_combinado.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el modelo de regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLnY7ESU0zYp",
    "outputId": "b410be10-06f4-490b-ce73-7dc2b2ed2ab7"
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_combinado, y)\n",
    "linea_combinada = np.hstack([linea, linea_binned])\n",
    "plt.plot(linea, reg.predict(linea_combinada), label='regresión lineal combinada')\n",
    "\n",
    "# Dibujamos las líneas verticales para cada límite de los bins\n",
    "for b in bins:\n",
    "    plt.plot([b, b], [-3, 3], ':', c='k')\n",
    "plt.plot(X[:, 0], y, 'o', c='k')\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"Salida de regresión\")\n",
    "plt.xlabel(\"Característica de entrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJcJUsEk0zYx"
   },
   "source": [
    "En este ejemplo, el modelo aprendió un desplazamiento para cada contenedor, junto con una pendiente compartida entre todos ellos (ya que se utiliza una única característica *x*).\n",
    "\n",
    "Sin embargo, dado que la pendiente se comparte entre todos los contenedores, no resulta muy útil. ¡Preferimos tener una pendiente separada para cada contenedor! Podemos lograr esto agregando una característica de interacción que combine el indicador del contenedor con la función original. Esta característica se obtiene como el producto entre el indicador del *bin* y la característica original. \n",
    "\n",
    "Creamos el siguiente conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjAne_D50zYy",
    "outputId": "cc6776cb-3d19-4a41-d6e0-fb60eef9537e"
   },
   "outputs": [],
   "source": [
    "X_producto = np.hstack([X_binned, X * X_binned])\n",
    "print(X_producto.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQGFcyVL0zY8"
   },
   "source": [
    "El conjunto de datos ahora tiene 20 características: los indicadores del *bin* y, para cada uno, el producto entre la característica original y dicho indicador. Se puede interpretar el producto como una copia separada de la función del eje *x* para cada contenedor.\n",
    "\n",
    "Ajustamos un modelo de regresión lineal sobre estos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwbh_k6-0zY-",
    "outputId": "c48bf17f-c1d6-45b7-ab7f-1ef2a8c7ed33"
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_producto, y)\n",
    "linea_producto = np.hstack([linea_binned, linea * linea_binned])\n",
    "plt.plot(linea, reg.predict(linea_producto), label='producto de regresión lineal')\n",
    "\n",
    "# Dibujamos las líneas verticales correspondientes a los bins\n",
    "for b in bins:\n",
    "    plt.plot([b, b], [-3, 3], ':', c='k')\n",
    "plt.plot(X[:, 0], y, 'o', c='k')\n",
    "\n",
    "plt.ylabel(\"Salida de regresión\")\n",
    "plt.xlabel(\"Características de entrada\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubBMEGEb0zZI"
   },
   "source": [
    "Como puede verse, ahora cada contenedor tiene su propio desplazamiento y pendiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkIvUFF90zZK"
   },
   "source": [
    "El uso de *binning* es una forma de expandir una característica continua. Otra estrategia es utilizar características polinómicas de la variable original. Para una característica `x`, podríamos incluir `x ** 2`, `x ** 3`, `x ** 4`, etc. Esto se implementa con `PolynomialFeatures` del módulo `preprocessing`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twelUHdj0zZM"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Incluimos polinomios hasta x ** 10.\n",
    "# \"include_bias=True\" agrega una característica constante de valor 1.\n",
    "polinomio = PolynomialFeatures(degree=10, include_bias=False)\n",
    "polinomio.fit(X)\n",
    "X_polinomio = polinomio.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCfYpsni0zZV"
   },
   "source": [
    "El uso de un grado de 10 produce 10 características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-7X1Q8O0zZW",
    "outputId": "9daaef5a-3789-45cb-eaba-505e4c313ee0"
   },
   "outputs": [],
   "source": [
    "print(\"Dimension de X_polinomio: {}\".format(X_polinomio.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CWK2VOp0zZc"
   },
   "source": [
    "Comparemos algunas entradas de `X` y `X_polinomio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mNgKvs-0zZd",
    "outputId": "b8fbc71e-835f-4040-c2b3-48c1e6118c5a"
   },
   "outputs": [],
   "source": [
    "print(\"Entradas de X:\\n{}\".format(X[:5]))\n",
    "print(\"Entradas de X_polinomio:\\n{}\".format(X_polinomio[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Xw_UmXO0zZo"
   },
   "source": [
    "Podemos obtener la semántica de las nuevas características llamando al método `get_feature_names_out()`, que indica el exponente asociado a cada característica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFNrg2Gl0zZq",
    "outputId": "4435408b-d85f-4847-be10-a4c8a8cee969"
   },
   "outputs": [],
   "source": [
    "print(\"Nombres de características polinomiales:\\n{}\".format(polinomio.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_mU_6jL0zZx"
   },
   "source": [
    "El uso de características polinómicas junto con un modelo de regresión lineal produce el modelo clásico de regresión polinómica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuMJ5Hav0zZz",
    "outputId": "42d15d1e-9a50-49fd-9a84-ea873e40ea30"
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_polinomio, y)\n",
    "linea_polinomio = polinomio.transform(linea)\n",
    "\n",
    "plt.plot(linea, reg.predict(linea_polinomio), label='regresión lineal polinómica')\n",
    "plt.plot(X[:, 0], y, 'o', c='k')\n",
    "plt.ylabel(\"Salida de regresión\")\n",
    "plt.xlabel(\"Característica de entrada\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxqN3e8p0zZ6"
   },
   "source": [
    "Como se observa, las características polinómicas producen un ajuste muy suave en esta información unidimensional, aunque polinomios de alto grado pueden comportarse de forma extrema en los límites o en zonas con pocos datos.\n",
    "\n",
    "A modo de comparación, a continuación se entrena un modelo SVM con un kernel RBF sobre los datos originales, sin ninguna transformación explícita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXo7lTQ-0zZ6",
    "outputId": "aae7b155-af30-4a56-ebf9-539f24337b82"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "for gamma in [1, 10]:\n",
    "    svr = SVR(gamma=gamma).fit(X, y)\n",
    "    plt.plot(linea, svr.predict(linea), label='SVR gamma={}'.format(gamma))\n",
    "plt.plot(X[:, 0], y, 'o', c='k')\n",
    "plt.ylabel(\"Salida de regresión\")\n",
    "plt.xlabel(\"Característica de entrada\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0fYp0CW0zaB"
   },
   "source": [
    "Usando un modelo más complejo como el SVM, podemos aprender una predicción tan compleja como la de la regresión polinómica sin realizar una transformación explícita de las características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uLQQ2S00zaB"
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "Como una aplicación más realista de interacciones y polinomios, veamos el conjunto de datos de California Housing. Se construyen nuevas características (incluyendo interacciones y términos polinomiales) y se evalúa cuánto ayudan a mejorar el rendimiento del modelo. Primero se cargan los datos y se reescala cada característica para que esté entre 0 y 1 utilizando `MinMaxScaler`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKEWOsTm0zaD"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Cargar el conjunto de datos de California Housing\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Reescalar los datos\n",
    "escalador = MinMaxScaler()\n",
    "X_entrenamiento_escalado = escalador.fit_transform(X_entrenamiento)\n",
    "X_prueba_escalado = escalador.transform(X_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvHwq6H80zaH"
   },
   "source": [
    "Ahora se extraen características polinomiales e interacciones hasta un grado de 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6be5Kuz0zaI",
    "outputId": "d660ec65-c9fa-4d0e-f4d4-c2e335ac18a2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Se generan características polinomiales (incluyendo interacciones) hasta el grado 2\n",
    "polinomio = PolynomialFeatures(degree=2).fit(X_entrenamiento_escalado)\n",
    "X_entrenamiento_polinomio = polinomio.transform(X_entrenamiento_escalado)\n",
    "X_prueba_polinomio = polinomio.transform(X_prueba_escalado)\n",
    "\n",
    "print(\"Dimensión X_entrenamiento: {}\".format(X_entrenamiento.shape))\n",
    "print(\"Dimensión X_entrenamiento_polinomio: {}\".format(X_entrenamiento_polinomio.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txTInv9A0zaS"
   },
   "source": [
    "La correspondencia exacta entre las características de entrada y salida se puede obtener utilizando el método `get_feature_names_out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDnLq9tY0zaT",
    "outputId": "a40e60fb-cf88-490e-a2f1-8e5a3b49131a"
   },
   "outputs": [],
   "source": [
    "print(\"Nombres de caracteristicas polinomiales:\\n{}\".format(polinomio.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccokbmAe0zaZ"
   },
   "source": [
    "A continuación, se compara el rendimiento de un modelo Ridge en los datos reescalados (sin interacciones) y en los datos transformados con interacciones polinomiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ethjzoVt0zab",
    "outputId": "ea118455-bc75-4d28-fe04-6212b433c034"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Modelo Ridge sin interacciones\n",
    "ridge = Ridge().fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "print(\"Puntuación sin interacciones: {:.3f}\".format(ridge.score(X_prueba_escalado, y_prueba)))\n",
    "\n",
    "# Modelo Ridge con interacciones\n",
    "ridge = Ridge().fit(X_entrenamiento_polinomio, y_entrenamiento)\n",
    "print(\"Puntuación con interacciones: {:.3f}\".format(ridge.score(X_prueba_polinomio, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DX9clNK0zah"
   },
   "source": [
    "Claramente, las interacciones y las características polinomiales proporcionan un buen impulso al rendimiento al usar Ridge. Sin embargo, al utilizar un modelo más complejo como un bosque aleatorio, la situación es algo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDhR8fzQ0zai",
    "outputId": "8d360836-6615-49e5-8922-455cf287f97a"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100).fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "print(\"Puntuación sin interacciones: {:.3f}\".format(rf.score(X_prueba_escalado, y_prueba)))\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100).fit(X_entrenamiento_polinomio, y_entrenamiento)\n",
    "print(\"Puntuación con interacciones: {:.3f}\".format(rf.score(X_prueba_polinomio, y_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7J5Vuuq0za2"
   },
   "source": [
    "Se observa que, incluso sin características adicionales, el bosque aleatorio supera el rendimiento de Ridge; de hecho, agregar interacciones y polinomios puede disminuir ligeramente el rendimiento en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itclAlnO0za3"
   },
   "source": [
    "### Transformaciones no lineales univariadas\n",
    "\n",
    "Hemos visto que agregar características cuadráticas o cúbicas puede ayudar a los modelos lineales en tareas de regresión. Existen, además, otras transformaciones útiles para determinadas características, en particular, aplicar funciones matemáticas como `log`, `exp` o `sin`.\n",
    "\n",
    "Mientras que los modelos basados en árboles solo consideran el orden de las características, los modelos lineales y las redes neuronales son sensibles a la escala y distribución de cada variable. Si existe una relación no lineal entre la característica y el objetivo, resulta complicado modelarla sin una transformación adecuada.  \n",
    "Las funciones `log` y `exp` pueden ayudar a ajustar la escala de los datos, mientras que `sin` y `cos` pueden ser útiles para patrones periódicos. La mayoría de los modelos funcionan mejor cuando las variables tienen una distribución similar a la gaussiana.\n",
    "\n",
    "Un caso común en el que se aplica una transformación logarítmica es en datos de recuentos, por ejemplo, el número de accesos de un usuario. Las cuentas suelen ser enteras, nunca negativas y, a menudo, muy asimétricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAvWwY1G0za4"
   },
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Utilizaremos un conjunto de datos artificiales de recuentos que presenta propiedades similares a las observadas en la práctica. En este ejemplo, las características son enteras y la respuesta es continua:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_tw3UJG0za4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rnd = np.random.RandomState(0)\n",
    "X_org = rnd.normal(size=(1000, 3))\n",
    "w = rnd.normal(size=3)\n",
    "\n",
    "# Se generan recuentos (valores enteros positivos) a partir de una transformación exponencial\n",
    "X = rnd.poisson(10 * np.exp(X_org))\n",
    "y = np.dot(X_org, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CLJcnZm0za9"
   },
   "source": [
    "Observamos las primeras 10 entradas de la primera característica. Todos son valores enteros y positivos; sin embargo, para apreciar la distribución resulta útil contar la frecuencia de cada valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQO7C-Ix0za-",
    "outputId": "23510abf-0c3b-423a-f221-28986e0a4f08"
   },
   "outputs": [],
   "source": [
    "print(\"Número de apariciones de la primera característica:\\n{}\".format(np.bincount(X[:, 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLlrg0880zbC"
   },
   "source": [
    "Visualizamos los recuentos en la siguiente figura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpnkoS1d0zbE",
    "outputId": "798d6307-bab6-4492-d4a1-f925f5531a96"
   },
   "outputs": [],
   "source": [
    "bins_val = np.bincount(X[:, 0])\n",
    "plt.bar(range(len(bins_val)), bins_val, color='grey')\n",
    "plt.ylabel(\"Número de apariciones\")\n",
    "plt.xlabel(\"Valores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8aRrzVk0zbK"
   },
   "source": [
    "Tratemos de ajustar una regresión ridge a este modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDQRpoaj0zbL",
    "outputId": "f908e6b9-eaa5-485e-9a20-7c9500b1a94d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=0)\n",
    "puntuacion = Ridge().fit(X_entrenamiento, y_entrenamiento).score(X_prueba, y_prueba)\n",
    "print(\"Puntuación del conjunto de prueba (sin transformación): {:.3f}\".format(puntuacion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOBmGw6p0zbQ"
   },
   "source": [
    "El puntaje $R^2$ relativamente bajo indica que Ridge no capta adecuadamente la relación entre `X` e `y`. Aplicar una transformación logarítmica puede ayudar, pero dado que `0` aparece en los datos y el logaritmo no está definido en cero, se calcula `log(X + 1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPctb0Kq0zbQ"
   },
   "outputs": [],
   "source": [
    "X_entrenamiento_log = np.log(X_entrenamiento + 1)\n",
    "X_prueba_log = np.log(X_prueba + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I38RSMcZ0zbW"
   },
   "source": [
    "Después de la transformación, la distribución de los datos es menos asimétrica y no presenta valores atípicos tan extremos. Por ejemplo, visualicemos el histograma de la primera característica transformada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq6AIYPr0zbX",
    "outputId": "3e877c36-0c15-40d2-9abd-d7826b96b5cc"
   },
   "outputs": [],
   "source": [
    "plt.hist(X_entrenamiento_log[:, 0], bins=25, color='gray')\n",
    "plt.ylabel(\"Número de apariciones\")\n",
    "plt.xlabel(\"Valores transformados\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnFVYo410zbc"
   },
   "source": [
    "Finalmente, se ajusta nuevamente un modelo Ridge sobre los datos transformados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnOerpt00zbd",
    "outputId": "b4cdc2e1-88e1-4552-8a9d-fd2a703d690c"
   },
   "outputs": [],
   "source": [
    "puntuacion = Ridge().fit(X_entrenamiento_log, y_entrenamiento).score(X_prueba_log, y_prueba)\n",
    "print(\"Puntuación del conjunto de prueba (con transformación logarítmica): {:.3f}\".format(puntuacion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cQpAfiz0zbj"
   },
   "source": [
    "Encontrar la transformación que mejor funcione para cada conjunto de datos y modelo es casi un arte. En este ejemplo, todas las características tienen propiedades similares, lo cual rara vez ocurre en la práctica; a menudo solo se transforma un subconjunto o cada característica se transforma de manera diferente.  \n",
    "Cabe destacar que, mientras que estas transformaciones son esenciales para modelos lineales o redes neuronales, son irrelevantes para modelos basados en árboles. Además, en regresión puede ser útil transformar también la variable objetivo `y` (por ejemplo, mediante `log(y + 1)`), lo que se acerca a la idea de una regresión de Poisson desde un punto de vista probabilístico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsxY28lx0zbk"
   },
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### Ejercicio 1: Binning y codificación *one‑hot* en una variable continua\n",
    "\n",
    "**Objetivo:**  \n",
    "- Aplicar una transformación de *binning* sobre una variable continua.\n",
    "- Codificar la variable agrupada mediante *one‑hot encoding*.\n",
    "- Comparar el rendimiento de un modelo lineal y un árbol de decisión en la predicción de un objetivo.\n",
    "\n",
    "**Tareas sugeridas:**\n",
    "\n",
    "1. Selecciona o genera un conjunto de datos unidimensional (por ejemplo, una distribución normal o el dataset `wave` mencionado en el ejemplo).\n",
    "2. Divide el rango de la variable en un número variable de *bins* (prueba con 5, 10 y 20 contenedores) utilizando `np.linspace` y `np.digitize`.\n",
    "3. Transforma los *bins* a una codificación *one‑hot* usando `OneHotEncoder` (recuerda usar el parámetro `sparse_output=False` en versiones recientes).\n",
    "4. Ajusta un modelo de regresión lineal y otro de árbol de decisión para predecir un objetivo (puede ser una función conocida o datos generados).\n",
    "5. Visualiza y analiza cómo varía el ajuste y el rendimiento según el número de *bins* y el tipo de modelo.\n",
    "\n",
    "\n",
    "\n",
    "#### Ejercicio 2: Transformaciones no lineales en datos de recuento\n",
    "\n",
    "**Objetivo:**  \n",
    "- Trabajar con un dataset simulado de recuentos que presenta alta asimetría.\n",
    "- Evaluar el impacto de transformaciones no lineales (por ejemplo, `log`, raíz cuadrada, `exp`) sobre el rendimiento de modelos lineales.\n",
    "\n",
    "**Tareas sugeridas:**\n",
    "\n",
    "1. Genera un dataset sintético similar al ejemplo, donde las características se obtengan a partir de una distribución (por ejemplo, usando `np.random.poisson`).\n",
    "2. Visualiza la distribución de las características (por ejemplo, con histogramas) para identificar la asimetría y la presencia de valores extremos.\n",
    "3. Aplica distintas transformaciones no lineales sobre las características. Ten en cuenta que si hay ceros, deberás aplicar la transformación `log(X + 1)` o similar.\n",
    "4. Ajusta un modelo de regresión (por ejemplo, Ridge) sobre los datos originales y sobre los datos transformados.\n",
    "5. Compara y discute las diferencias en el rendimiento y la estabilidad del modelo.\n",
    "\n",
    "\n",
    "#### Ejercicio 3: Pipeline para selección de transformación óptima\n",
    "\n",
    "**Objetivo:**  \n",
    "- Construir un *pipeline* en scikit‑learn que aplique diferentes transformaciones a las características.\n",
    "- Usar GridSearchCV para determinar cuál transformación (o combinación de ellas) mejora la capacidad predictiva del modelo.\n",
    "\n",
    "**Tareas sugeridas:**\n",
    "\n",
    "1. Selecciona un dataset (puede ser un dataset real o sintético) que contenga características con distribuciones muy dispares.\n",
    "2. Crea un pipeline que incluya:\n",
    "   - La posibilidad de aplicar transformaciones como `log`, raíz cuadrada o ninguna.\n",
    "   - Expansión polinómica opcional.\n",
    "   - Un estimador (por ejemplo, Ridge o RandomForestRegressor).\n",
    "3. Define una cuadrícula de parámetros que explore:\n",
    "   - El tipo de transformación para cada característica (o grupo de características).\n",
    "   - El grado de la expansión polinómica.\n",
    "   - Parámetros del modelo final (por ejemplo, el parámetro de regularización en Ridge).\n",
    "4. Usa GridSearchCV para encontrar la mejor combinación de transformaciones y parámetros.\n",
    "5. Discute los resultados obtenidos y qué transformación resultó ser la más efectiva para cada tipo de característica.\n",
    "\n",
    "\n",
    "#### Ejercicio 5: Interpretabilidad en modelos con interacciones y transformaciones\n",
    "\n",
    "**Objetivo:**  \n",
    "- Comparar la interpretabilidad de modelos lineales y modelos basados en árboles cuando se utilizan interacciones y transformaciones no lineales.\n",
    "- Emplear técnicas de interpretación (por ejemplo, análisis de coeficientes o SHAP) para identificar las características más relevantes.\n",
    "\n",
    "**Tareas sugeridas:**\n",
    "\n",
    "1. Usa un dataset con múltiples variables y aplica interacciones (con `PolynomialFeatures`) y transformaciones (por ejemplo, logarítmica o raíz cuadrada).\n",
    "2. Ajusta dos modelos:\n",
    "   - Un modelo lineal (por ejemplo, Ridge) sobre las características transformadas.\n",
    "   - Un modelo basado en árboles (por ejemplo, RandomForestRegressor) sobre las mismas características.\n",
    "3. Para el modelo lineal, analiza los coeficientes para identificar qué interacciones y transformaciones tienen mayor peso.\n",
    "4. Para el modelo basado en árboles, utiliza herramientas de interpretabilidad como SHAP o la importancia de características incorporada.\n",
    "5. Realiza un informe comparativo sobre qué características transformadas resultan ser más relevantes en cada modelo y discute posibles razones.\n",
    "\n",
    "\n",
    "#### Ejercicio 6: Características polinomiales en redes neuronales\n",
    "\n",
    "**Objetivo:**  \n",
    "Explorar el efecto de ampliar el espacio de características con términos polinomiales en una red neuronal para un problema de regresión.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "1. **Carga y preprocesamiento:**  \n",
    "   Utiliza un dataset clásico (por ejemplo, Boston Housing o California Housing). Escala los datos usando un escalador como `MinMaxScaler` o `StandardScaler`.\n",
    "\n",
    "2. **Generación de características polinomiales:**  \n",
    "   Usa `PolynomialFeatures` (por ejemplo, con grado 2 o 3) para crear interacciones y términos no lineales.  \n",
    "   ```python\n",
    "   from sklearn.preprocessing import PolynomialFeatures\n",
    "   polinomio = PolynomialFeatures(degree=2, include_bias=False)\n",
    "   X_entrenamiento_poly = polinomio.fit_transform(X_entrenamiento_escalado)\n",
    "   X_prueba_poly = polinomio.transform(X_prueba_escalado)\n",
    "   ```\n",
    "\n",
    "3. **Diseño de la red neuronal:**  \n",
    "   Con frameworks como Keras o PyTorch, diseña una red neuronal (por ejemplo, con dos o tres capas densas) y entrena dos modelos:  \n",
    "   - Uno utilizando las características originales.  \n",
    "   - Otro utilizando las características expandidas con términos polinomiales.\n",
    "\n",
    "4. **Comparación y análisis:**  \n",
    "   - Evalúa el desempeño (por ejemplo, con el coeficiente $R^2$ o MSE) en el conjunto de prueba.  \n",
    "   - Analiza si el aumento de dimensiones mejora la capacidad predictiva o induce sobreajuste.  \n",
    "   - Experimenta con técnicas de regularización (dropout, L2) para mitigar posibles efectos adversos.\n",
    "\n",
    "\n",
    "#### Ejercicio 7: Transformaciones no lineales en datos de recuento para redes neuronales\n",
    "\n",
    "**Objetivo:**  \n",
    "Investigar cómo afectan las transformaciones no lineales (como logarítmica, exponencial o trigonométrica) al entrenamiento de una red neuronal cuando se trabaja con datos de recuento.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "1. **Generación del dataset:**  \n",
    "   Crea un conjunto de datos artificiales en el que las características provengan de distribuciones de recuentos (por ejemplo, usando `np.random.poisson`).\n",
    "\n",
    "2. **Visualización de la distribución:**  \n",
    "   Utiliza histogramas para visualizar la distribución original y detectar asimetrías o presencia de valores extremos.\n",
    "\n",
    "3. **Aplicación de transformaciones:**  \n",
    "   - Aplica la transformación logarítmica ajustada (por ejemplo, `np.log(X + 1)`) para evitar problemas con ceros.  \n",
    "   - Alternativamente, prueba otras transformaciones (por ejemplo, raíz cuadrada o incluso funciones seno en caso de datos periódicos).\n",
    "\n",
    "4. **Entrenamiento de la red neuronal:**  \n",
    "   Diseña y entrena una red neuronal simple para un problema de regresión o clasificación, comparando el desempeño con datos originales versus datos transformados.\n",
    "\n",
    "5. **Análisis de resultados:**  \n",
    "   Discute cómo la transformación afecta la convergencia, la estabilidad numérica y la capacidad de la red para modelar relaciones complejas.\n",
    "\n",
    "\n",
    "#### Ejercicio 8: Uso de binning y codificación one-hot como entrada para redes neuronales\n",
    "\n",
    "**Objetivo:**  \n",
    "Evaluar el impacto de transformar una variable continua en una representación categórica (usando binning y codificación one-hot) antes de alimentar una red neuronal.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "1. **Binning:**  \n",
    "   Selecciona una característica continua de un dataset y define intervalos (por ejemplo, utilizando `np.linspace` y `np.digitize`).\n",
    "\n",
    "2. **Codificación One-Hot:**  \n",
    "   Transforma los bins en variables dummy utilizando `OneHotEncoder` (recordando utilizar el argumento adecuado, por ejemplo, `sparse_output=False` en versiones recientes de scikit‑learn).\n",
    "\n",
    "3. **Diseño de la red neuronal:**  \n",
    "   - Entrena una red neuronal con la característica original.  \n",
    "   - Entrena otra red neuronal en la que la entrada se compone de la codificación one-hot del bin (o incluso, combina ambas representaciones).\n",
    "\n",
    "4. **Comparación:**  \n",
    "   - Evalúa métricas de desempeño y discute las ventajas y desventajas de representar la variable como continua versus categórica.  \n",
    "   - Reflexiona sobre la pérdida de orden o la potencial ganancia en flexibilidad del modelo.\n",
    "\n",
    "\n",
    "#### Ejercicio 9: Creación de una capa personalizada para transformaciones de características\n",
    "\n",
    "**Objetivo:**  \n",
    "Implementar una capa personalizada en PyTorch que aplique una transformación no lineal o un mapeo polinómico a las entradas antes de pasarlas a las capas densas, y comparar el desempeño con una arquitectura tradicional.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "1. **Definición de la capa personalizada:**  \n",
    "   Crea una clase de capa que reciba la entrada y realice, por ejemplo, una transformación polinómica o una combinación de funciones (log, exp, sin).\n",
    "\n",
    "2. **Integración en la red:**  \n",
    "   Inserta esta capa al inicio de la red neuronal. El resto de la arquitectura puede ser estándar (varias capas densas, activaciones ReLU, etc.).\n",
    "\n",
    "3. **Entrenamiento y evaluación:**  \n",
    "   - Compara el rendimiento del modelo con la capa personalizada frente a un modelo similar sin dicha capa.  \n",
    "   - Analiza la interpretabilidad de las transformaciones aprendidas y si la red aprovecha mejor las relaciones no lineales.\n",
    "\n",
    "4. **Discusión:**  \n",
    "   Reflexiona sobre cuándo y por qué podría ser útil incorporar transformaciones explícitas en la arquitectura de una red, en contraposición a dejar que la red aprenda dichas transformaciones de manera implícita.\n",
    "\n",
    "\n",
    "#### Ejercicio 10: Interpretabilidad y análisis de importancia de características transformadas\n",
    "\n",
    "**Objetivo:**  \n",
    "Utilizar métodos de interpretabilidad (por ejemplo, LIME o SHAP) para entender cómo influyen las características transformadas (polinomiales, interacciones o transformaciones logarítmicas) en las predicciones de una red neuronal.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "1. **Entrenamiento del modelo:**  \n",
    "   Entrena una red neuronal utilizando características originales y, por separado, características transformadas (por ejemplo, características polinomiales o combinaciones con transformaciones logarítmicas).\n",
    "\n",
    "2. **Aplicación de técnicas interpretables:**  \n",
    "   Emplea herramientas como SHAP para analizar la contribución de cada característica en la predicción de la red.  \n",
    "   - Compara la importancia relativa de las variables originales y las transformadas.  \n",
    "   - Observa si algunas de las transformaciones aportan información redundante o, por el contrario, resaltan patrones no evidentes en los datos originales.\n",
    "\n",
    "3. **Reporte de hallazgos:**  \n",
    "   Elabora un informe en el que discutas cómo varía la importancia de las características según la transformación aplicada, y cómo esto se relaciona con el desempeño del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2uLQQ2S00zaB",
    "MAvWwY1G0za4"
   ],
   "name": "Representacion-datos-Ingenieria-Caracteristicas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
