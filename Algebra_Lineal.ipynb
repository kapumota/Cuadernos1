{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "import scipy.linalg as la\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Notas de álgebra lineal\n",
    "\n",
    "Referencias:\n",
    "\n",
    "1 .  [Coding the Matrix: Linear Algebra through Applications to Computer Science](http://codingthematrix.com/), Philip N. Klein, 2013.\n",
    "\n",
    "2 . [Linear Algebra Done Right](http://linear.axler.net/), Sheldon Axler, 2015.\n",
    "\n",
    "3 . [Numerical Algorithms](http://people.csail.mit.edu/jsolomon/share/book/numerical_book.pdf), Justin Solomon 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Sistemas lineales\n",
    "\n",
    "Consideremos un conjunto de $m$ ecuaciones lineales y $n$ incógnitas. Reescribamos el conjunto como el sistema:\n",
    "\n",
    "$$Ax=b$$\n",
    "\n",
    "es decir un  problema de una ecuación matricial.  Resolver el sistema cuenta con calcular $A^{-1}$.\n",
    "\n",
    "La información de las propiedades de $A$ produce importante información acerca del sistema lineal.\n",
    "\n",
    "\n",
    "* Cuando $m < n$ el sistema que el sistema es subdeterminado. El sistema no tiene soluciones (si el sistema es inconsistente) o tiene infinitas soluciones. Una única solución no es posible.\n",
    "\n",
    "* Cuando $m > n$ se dice que el sistema es sobredeterminado. El sistema puede ser inconsistente o algunas soluciones pueden ser redundantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independencia lineal\n",
    "\n",
    "Una colección de vectores $v_1 \\dots v_n$ se dice linealmente independiente si\n",
    "\n",
    "$$c_1v_1 + \\dots c_nv_n  \\iff c_1 = \\cdots c_n = 0$$\n",
    "\n",
    "En otras palabras, una combinación de los vectores que resulta en un vector cero es trivial. Una interpretación de esta definición es que ningún vector puede ser expresado  por la combinación lineal de los otros vectores, en este sentido, la independencia lineal es una expresión de no redundancia en un conjunto de vectores.\n",
    "\n",
    "Un conjunto de $n$ vectores linealmente independientes genera un espacio n-dimensional, es decir el conjunto de todas las posibles combinaciones es $R^n$. Tal conjunto de vectores se dice que es una base  de  $R^n$.\n",
    "\n",
    "Algunos resultados importantes:\n",
    "\n",
    "* Si $A$ es una matriz de orden $m \\times n$ y $m >n $, si toda fila es linealmente independiente, entonces el sistema es sobredeterminado y inconsistente. El sistema no puede resolverse exactamente. Este es un usual caso en ciencia de datos y por que los mínimos cuadrados son muy importantes.\n",
    "* Si $A$ es una matriz de orden $m \\times n$ y $m < n $, si toda fila es linealmente independiente, entonces el sistema es subdeterminado y tiene infinitas soluciones.\n",
    "* Si $A$ es una matriz de orden $m \\times n$  y algunas de sus filas son linealmente dependientes, entonces el sistema es reducible.\n",
    "* Si $A$ es una matriz cuadrada y las filas son linealmente independientes, el sistema tiene solución única.  $A$ es invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norma y distancia de vectores\n",
    "\n",
    "La norma de un vector $v$ es denotada por $\\Vert v \\Vert$ es simplemente su longitud. Para vectores con componentes $v = (v_1 \\dots v_n)$ la norma de $v$ es dada por:\n",
    "\n",
    "$$v = \\sqrt{v_1^2 + \\dots v_n^2}$$\n",
    "\n",
    "\n",
    "La distancia entre dos vectores $v$ y $w$ es la longitud de sus diferencias.\n",
    "\n",
    "$$d(v, w) =\\Vert v -w \\Vert$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# norma de un vector\n",
    "\n",
    "v = np.array([3,4])\n",
    "linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# distancia entre dos   vectores\n",
    "\n",
    "w = np.array([1,1])\n",
    "linalg.norm(v-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producto interno\n",
    "\n",
    "El producto interno estándar de dos vectores n-dimensional $v$ y $w$ es dado por \n",
    "\n",
    "$$\\langle v, w \\rangle =  v_1w_1 + \\cdots v_nw_n$$\n",
    "\n",
    "El producto es sólo la suma del producto de los componentes de los vectores. Ciertas matrices también definen productos internos.\n",
    "\n",
    "Un producto interno determina una norma: $\\Vert v \\Vert = \\sqrt{\\langle v, v\\rangle}$.\n",
    "\n",
    "En un caso general, el producto interno sobre un espacio $V$ es una forma bilineal, simétrica y definida positiva.\n",
    "\n",
    "De forma análoga una norma es una función cuyo espacio vectorial es el de los números reales, que es definida positiva, escalable y que satisface la desigualdad triangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "v.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producto externo\n",
    "\n",
    "El producto interno es sólo la multiplicación matricial de un vector $1 \\times n$ con un vector $n \\times 1$. En efecto, podemos escribir: $\\langle v, w \\rangle = v^{t}w$, considerando $v$ y $w$ vectores columnas.\n",
    "\n",
    "El producto externo de dos vectores es lo opuesto. Es dado por: $$v\\otimes w=vw^t$$.\n",
    "\n",
    "Si el resultado del producto interno es un  escalar, el resultado del producto exterior es una matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Producto externo\n",
    "\n",
    "np.outer(v, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La [matriz de covarianza](https://en.wikipedia.org/wiki/Covariance_matrix) es un producto externo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Tenemos n observaciones de p variables\n",
    "\n",
    "n, p = 10,4\n",
    "v = np.random.random((p, n))\n",
    "\n",
    "# La matriz covarianza es de orden p xp\n",
    "np.cov(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traza y determinantes de matrices\n",
    "\n",
    "La traza de una matriz $A$ es la suma de los elementos en sus diagonales.\n",
    "\n",
    "* Es invariante de una matriz bajo cambios de bases.\n",
    "* Define una norma matricial.\n",
    "\n",
    "El determinante es una matriz es definida como la suma alternada de permutaciones de los elementos de una matriz. Podemos calcular los determinantes con herramientas  como\n",
    "\n",
    "```\n",
    "np.linalg.det(A)\n",
    "```\n",
    "\n",
    "* Como la traza, es invariante bajo cambios de bases.\n",
    "* Una matriz $A$ de orden $n \\times n$ es invertible si y sólo si $\\det(A) \\neq 0$.\n",
    "* Las filas(columnas) de una  matriz $A$ de orden $n \\times n$  es  linealmente independiente si y sólo si $\\det(A) \\neq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo de determinante\n",
    "\n",
    "n = 6\n",
    "M = np.random.randint(100,size=(n,n))\n",
    "print(M)\n",
    "np.linalg.det(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Espacio columna, espacio fila, rango y núcleo\n",
    "\n",
    "Sea $A$ una matriz de orden $m\\times n$. Podemos ver las columnas de $A$ como vectores $a_1, \\dots ,a_n$. El espacio de todas las combinaciones  de los $a_i$ son el espacio columna de la matriz $A$. Ahora si $a_1, \\dots, a_n$ son linealmente independientes, entonces el espacio columna tiene dimensión $n$. Por otra parte, la dimensión del espacio columna es el tamaño del conjunto maximal linealmente independiente de $a_i$. El espacio fila es de forma análoga, pero los vetores son filas de la matriz $A$.\n",
    "\n",
    "El rango de una matriz $A$ es la dimensión del espacio columna-espacio fila. El rango se puede considerar como una medida de la no degeneración de un sistema de ecuaciones lineales, ya que es la dimensión de la imagen de la transformación lineal determinada por $A$.\n",
    "\n",
    "El núcleo de una matriz $A$  es el conjunto de todos los elementos cuya imagen es el vector cero bajo la transformación de $A$. La dimensión del núcleo  de la transformación lineal es llamado nulidad .\n",
    "\n",
    "\n",
    "**Teorema del Index**\n",
    "\n",
    "Para una matriz $A$ de orden $m \\times n$:\n",
    "\n",
    "\n",
    "$$rango(A) + nulidad(A)  = n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices como transformaciones lineales\n",
    "\n",
    "\n",
    "* Una matriz define una transformación lineal.\n",
    "* La forma matricial de una matriz no es única.\n",
    "* Podemos definir una transformación  a partir de una base.\n",
    "\n",
    "Supongamos que tenemos una matriz $A$ que define  alguna transformación. Podemos tomar una matriz invertible $B$ y $BAB^{-1}$ define la misma transformación. Esta operación se llama **cambio de base**, ya que expresamos la tranformación con respecto a diferentes bases.\n",
    "\n",
    "Esto es lo que se hace en PCA, expresamos la matriz en una base de autovectores.\n",
    "\n",
    "Sea $f(x)$ una transformación  que lleva  $e_1 = (1,0)$ a $(2,3)$ y $e_2 = (0,1)$ a $(1,1)$. Una representación matricial de $f$ es dada por:\n",
    "\n",
    "$$\\begin{split}A = \\left(\\begin{matrix}2 & 1\\\\3&1\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "Esta es la matriz si es que consideramos los vectores de $R^2$ es una combinación de la forma\n",
    "\n",
    "$$c_1e_1 + c_2e_2$$\n",
    "\n",
    "Ahora consideremos un segundo par de vectores linealmente independiente en $R^2$, $v_1 = (1,3)$ y $v_2 = (4,1)$. Encontremos la transformación que toma $e_1$ a $v_1$ y $e_2$ a $v_2$. Una representación matricial para esto es:\n",
    "\n",
    "$$\\begin{split}B = \\left(\\begin{matrix}1 & 4\\\\3&1\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "La transformación original $f$ puede ser expresado con respecto a la base $v_1, v_2$ vía\n",
    "\n",
    "$$BAB^{-1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "A = np.array([[2,1],[3,1]])  # transformacion f en la base estandar\n",
    "e1 = np.array([1,0])         # vectores de la base estandar e1,e2\n",
    "e2 = np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(A.dot(e1))             # probamos que  Ae1 es (2,3)\n",
    "print(A.dot(e2))             # probamos que  Ae2 es (1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuevos vectores\n",
    "\n",
    "v1 = np.array([1,3])\n",
    "v2 = np.array([4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Como v1 y v2 son tranasformados por A\n",
    "\n",
    "print(\"Av1: \")\n",
    "print(A.dot(v1))\n",
    "print(\"Av2: \")\n",
    "print(A.dot(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Cambio de base  desde e1, e2 a v1,v2\n",
    "\n",
    "B = np.array([[1,4],[3,1]])\n",
    "print(B)\n",
    "B_inv = linalg.inv(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# verificamos la inversa\n",
    "\n",
    "print(\"B B_inv \")\n",
    "print(B.dot(B_inv))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# transformamos  e1 bajo cambio de coordenadas\n",
    "\n",
    "T = B.dot(A.dot(B_inv))        # B A B^{-1}\n",
    "coeffs = T.dot(e1)\n",
    "print(coeffs[0]*v1 + coeffs[1]*v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribamos un grafico de este ejemplo\n",
    "\n",
    "def g_vector(vs):\n",
    "    \"\"\"dibujamos los vectores asumiendo el origen en (0,0)\"\"\"\n",
    "    n = len(vs)\n",
    "    X, Y = np.zeros((n, 2))\n",
    "    U, V = np.vstack(vs).T\n",
    "    plt.quiver(X, Y, U, V, range(n), angles='xy', scale_units='xy', scale=1)\n",
    "    xmin, xmax = np.min([U, X]), np.max([U, X])\n",
    "    ymin, ymax = np.min([V, Y]), np.max([V, Y])\n",
    "    xrng = xmax - xmin\n",
    "    yrng = ymax - ymin\n",
    "    xmin -= 0.05*xrng\n",
    "    xmax += 0.05*xrng\n",
    "    ymin -= 0.05*yrng\n",
    "    ymax += 0.05*yrng\n",
    "    plt.axis([xmin, xmax, ymin, ymax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocamos los datos\n",
    "\n",
    "e1 = np.array([1,0])\n",
    "e2 = np.array([0,1])\n",
    "A = np.array([[2,1],[3,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mostramos  un grafico de Ae_1  y Ae_2\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "g_vector([e1, e2])\n",
    "plt.subplot(1,2,2)\n",
    "g_vector([A.dot(e1), A.dot(e2)])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normas matriciales\n",
    "\n",
    " Podemos extender la norma de un vector a una norma matricial. Las normas de matrices son usadas para determinar la condición de una matriz. Hay muchas normas matriciales, pero las más comunes son llamadas `p-normas`. Así por ejemplo para un vector $n$ dimensional y para $1 \\leq p < \\infty$, tenemos \n",
    " \n",
    " $$\\Vert v \\Vert_p = \\left(\\sum\\limits_{i=1}^n \\vert v_i \\vert ^p\\right)^{\\frac{1}{p}}$$\n",
    " \n",
    " y para el caso en que $p = \\infty$\n",
    " \n",
    " $$\\Vert v \\Vert_\\infty = \\max{\\vert v_i\\vert}$$\n",
    " \n",
    " De forma similar para el caso de las matrices\n",
    " \n",
    " $$ \\Vert A\\Vert_p = \\sup_x \\frac{\\Vert Ax\\Vert_p}{\\Vert x \\Vert_p}$$\n",
    " \n",
    " \n",
    " $$\\Vert A \\Vert_{1} = \\max_j\\left(\\sum\\limits_{i=1}^n\\vert a_{ij} \\vert \\right)$$ \n",
    " \n",
    " que es una suma de columnas.\n",
    " \n",
    " $$ \\Vert A \\Vert _{\\infty} = \\max_i\\left(\\sum\\limits_{j=1}^n\\vert a_{ij}\\vert \\right)$$\n",
    " \n",
    " que es una suma de filas.\n",
    " \n",
    " \n",
    " La norma $\\Vert A \\Vert_2$ es dado por el mayor autovalor de $(A^TA)^{\\frac{1}{2}}$, en otras palabras es el mayor `valor singular` de $A$.\n",
    " \n",
    " Otra norma usada es la conocida como la norma Frobenius, que se puede calcular de la siguiente forma\n",
    " \n",
    " $$\\Vert A \\Vert_F = \\left(\\sum\\sum \\left(a_{ij}\\right)^2\\right)^{\\frac{1}{2}} $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras matrices\n",
    "\n",
    "Las matrices cuadradas tienen el mismo número de columnas. Si una matriz cuadrada $n \\times n$ es invertible (el rango de $A$ es n)  esta es única. Esta situación conduce a una solución única en un sistema lineal.\n",
    "\n",
    "Una matriz diagonal es una matriz cuyas entradas fuera de la diagonal son ceros. En este caso también se pueden considerar matrices rectangulares de orden $m \\times n$ ser diagonales si todas las entradas $a_{ij}$ son cero para $i \\neq j$.\n",
    "\n",
    "Una matriz $A$ es (skew)simétrica si: $a_{ij} = (-)a_{ji}$ o de forma equivalente $A = (-)A^{T}$.\n",
    "\n",
    "Una matriz $A$ es triangular (superior)(inferior) si $a_{ij} = 0$ para todo $(i >j)(i < j)$.\n",
    "\n",
    "Esas matrices tienen muchos ceros en sus entradas. Matrices bandas tienen `bandas` de elementos distintos de ceros y se puden utilizar para simplicar cálculos. \n",
    "\n",
    "Las matrices sparse, son aquellas que tienen pocos elementos distintos de cero.\n",
    "\n",
    "Una matriz $A$ es ortogonal si $AA^{T} = I$, en otras palabras $A^{T} = A^{-1}$.\n",
    "\n",
    "* Las filas y columnas de una matriz ortogonal son un conjunto ortonormal de vectores.\n",
    "* Las transformaciones ortogonales preservan longitud y ángulos entre vectores.\n",
    "\n",
    "Una matriz cuadrada es definida positiva si $u^{T}Au > 0$ para un vector $n$-dimensional distinto de cero.\n",
    "\n",
    "Una matriz definida positiva, simétrica $A$ es una matriz definida positiva tal que $A = A^{T}$.\n",
    "\n",
    "* Una matriz simétrica, definida positiva es diagonalizable.\n",
    "* La matriz covarianza es simétrica y definida positiva.\n",
    "* Las matrices simétricas, definidas positivas  tienen raíz cuadrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Descomposiciones matriciales\n",
    "\n",
    "La descomposiciones de matrices es un importante paso para resolver sistema de ecuaciones de manera eficiente computacionalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomposición LU y eliminación Gaussiana\n",
    "\n",
    "\n",
    "\n",
    "La eliminación Gaussiana, es esencialmente la descomposición $LU$, veamos como trabaja:\n",
    "\n",
    "\n",
    "Consideremos un sistema de ecuaciones $3\\times 3$, en general este procedimiento funciona para sistemas $n \\times n$:\n",
    "\n",
    "\n",
    "$$\\begin{split}\\left(\\begin{matrix}a_{11}&a_{12} & a_{13}\\\\a_{21}&a_{22}&a_{23}\\\\a_{31}&a_{32}&a_{33}\\end{matrix}\\right)\\left(\\begin{matrix}x_1\\\\x_2\\\\x_3\\end{matrix}\\right) = \\left(\\begin{matrix}b_1\\\\b_2\\\\b_3\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "\n",
    "Si la matriz $A$ es no singular, resolvamos el sistema usando la  `eliminación Gaussiana`, empezando por escribir la matriz aumentada del sistema\n",
    "\n",
    "\n",
    "$$\\begin{split}\\left(\\begin{array}{ccc|c}a_{11}&a_{12} & a_{13}& b_1 \\\\a_{21}&a_{22}&a_{23}&b_2\\\\a_{31}&a_{32}&a_{33}&b_3\\end{array}\\right)\\end{split}$$\n",
    "\n",
    "\n",
    "Si la primera entrada $a_{11}$ es diferente de cero, entonces dividimos la primera fila por $a_{11}$ y substraemos el múltiplo apropiado de la primera fila desde las otras filas, de manera que `0` sea el primer valor de todas las filas. Si $a_{11}$ es cero, debemos permutar las filas). El resultado es como sigue\n",
    "\n",
    "$$\\begin{split}\\left(\\begin{array}{ccc|c}\n",
    "1 & \\frac{a_{12}}{a_{11}} & \\frac{a_{13}}{a_{11}} & \\frac{b_1}{a_{11}} \\\\\n",
    "0 & a_{22} - a_{21}\\frac{a_{12}}{a_{11}} & a_{23} - a_{21}\\frac{a_{13}}{a_{11}}  & b_2 - a_{21}\\frac{b_1}{a_{11}}\\\\\n",
    "0&a_{32}-a_{31}\\frac{a_{12}}{a_{11}} & a_{33} - a_{31}\\frac{a_{13}}{a_{11}}  &b_3- a_{31}\\frac{b_1}{a_{11}}\\end{array}\\right)\\end{split}$$\n",
    "\n",
    "\n",
    "Repetimos el procedimiento para la segunda fila, primero dividiendo  por la entrada distinta de cero, a continuación, restando el múltiplo apropiado de la fila con cada una de las filas tercera y primera, de modo que la segunda entrada en la fila $1$ y $3$ sean $0$. Se debe  continuar hasta que la matriz de la izquierda sea la identidad. El vector $x$ solución del sistema  es el vector columna resultante de la derecha. \n",
    "\n",
    "\n",
    "Por lo general, es más eficiente  parar en forma de un `nivel de fila reducida` (triangular superior, con unos en la diagonal), y luego usar otra forma de sustitución (`back sustitution`) para obtener la respuesta final. Ten en cuenta que en algunos casos, es necesario permutar filas para obtener un nivel de fila reducida (` pivoteo parcial`). Si también manipulamos columnas, tenemos un  `pivoteo completo`.\n",
    "\n",
    "Con la eliminación Gaussiana se puede obtener  la inversa de una matriz, mediante la reducción de la matriz $A$ a la identidad, con la matriz de identidad en  la porción aumentada.\n",
    "\n",
    "Ahora, todo esto está bien cuando estamos resolviendo un sistema  una  vez, para un resultado $b$. Muchas aplicaciones implican soluciones a múltiples problemas, donde el  lado izquierdo de la ecuación  no cambia, pero hay muchos vectores  resultados $b$.\n",
    "\n",
    "En este caso, es más eficiente para descomponer $A$.\n",
    "\n",
    "\n",
    "La descomposición $LU$ de una matriz $A$  es una descomposición tal que \n",
    "\n",
    "$$A = LU$$\n",
    "\n",
    "donde $L$ es una matriz triangular inferior y $U$ es una matriz triangular superior.\n",
    "\n",
    "\n",
    "**Ejemplo** \n",
    "\n",
    "Consideramos una matriz \n",
    "\n",
    "$$\\begin{split}A = \\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
    "  2& 1& 3\\\\\n",
    "  4&1&2\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "\n",
    "Necesitamos multiplicar la fila $1$ por 2 y restar a la fila $2$ para eliminar la primera entrada en la fila $2$, y luego multiplicar la fila $1$ por 4 y restar de la fila $3$. En lugar de escribir ceros  en las primeras entradas de las filas $2$ y $3$, se registrar los múltiplos necesarios para su eliminación:\n",
    "\n",
    "\n",
    "$$\\begin{split}\\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
    "  (2)& -5 & -5\\\\\n",
    "   (4)&-11&-14\n",
    "  \\end{matrix}\\right)\\end{split}$$\n",
    "  \n",
    "  \n",
    "y entonces eliminamos la segunda entrada en la tercera fila:\n",
    "\n",
    "$$\\begin{split}\\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
    " (2)& -5 & -5\\\\\n",
    "  (4)&(\\frac{-11}{5})&-3\n",
    "   \\end{matrix}\\right)\\end{split}$$\n",
    "   \n",
    "Ahora tenemos la descomposición:\n",
    "\n",
    "\n",
    "$$\\begin{split}L= \\left(\\begin{matrix} 1 & 0 & 0 \\\\\n",
    "   2& 1 & 0\\\\\n",
    "   4&\\frac{-11}5&1\n",
    "   \\end{matrix}\\right)\n",
    "   U = \\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
    "   0& -5 & -5\\\\\n",
    "   0&0&-3\n",
    "   \\end{matrix}\\right)\\end{split}$$\n",
    "   \n",
    "   \n",
    " Podemos resolver el sistema, con dos substituciones:\n",
    " \n",
    " $$Ly = b$$\n",
    " \n",
    " \n",
    " $$Ux = y$$\n",
    " \n",
    "Con `numpy` se puede realizar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,3,4],[2,1,3],[4,1,2]])\n",
    "print(A)\n",
    "\n",
    "P, L, U = la.lu(A)\n",
    "print(np.dot(P.T, A))\n",
    "print\n",
    "print(np.dot(L, U))\n",
    "print(P)\n",
    "print(L)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numpy` utiliza pivoteo parcial en las descomposiciones (las filas de la matriz son permutadas para  utilizar el  mayor pivote). Esto se debe a que  pequeños pivotes pueden conducir a  inestabilidad numérica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomposición de Cholesky\n",
    "\n",
    "Sea $A$ una matriz simétrica, definida positiva. Hay una única descomposición tal que \n",
    "\n",
    "$$A = LL^{T}$$\n",
    "\n",
    "donde $L$ es una matriz triangular con elementos en la diagonal positivos y $L^{T}$ es su transpuesta. \n",
    "\n",
    "\n",
    "* Sea $A$ una matriz $n \\times n$. Encontramos la matriz $L$ usando el siguiente procedimiento iterativo\n",
    "\n",
    "$$\\begin{split}A = \\left(\\begin{matrix}a_{11}&A_{12}\\\\A_{12}&A_{22}\\end{matrix}\\right) =\n",
    "\\left(\\begin{matrix}\\ell_{11}&0\\\\\n",
    "L_{12}&L_{22}\\end{matrix}\\right)\n",
    "\\left(\\begin{matrix}\\ell_{11}&L_{12}\\\\0&L_{22}\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "* Sea $\\ell_{11} = \\sqrt{a_{11}}$.\n",
    "* $L_{12} = \\frac{1}{\\ell_{11}}A_{12}$.\n",
    "* Resolvemos $A_{22} - L_{12}L_{12}^T = L_{22}L_{22}^T$ para $L_{22}$.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "$$\\begin{split}A = \\left(\\begin{matrix}1&3&5\\\\3&13&23\\\\5&23&42\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "$$\\ell_{11} = \\sqrt{a_{11}} = 1$$\n",
    "\n",
    "$$L_{12} = \\frac{1}{\\ell_{11}} A_{12} = A_{12}$$\n",
    "\n",
    "Y así se concluye con $\\ell_{33}=1$\n",
    "\n",
    "\n",
    "Esto produce la descomposición:\n",
    "\n",
    "$$\\begin{split}\\left(\\begin{matrix}1&3&5\\\\3&13&23\\\\5&23&42\\end{matrix}\\right) =\n",
    "\\left(\\begin{matrix}1&0&0\\\\3&2&0\\\\5&4&1\\end{matrix}\\right)\\left(\\begin{matrix}1&3&5\\\\0&2&4\\\\0&0&1\\end{matrix}\\right)\\end{split}$$\n",
    "\n",
    "\n",
    "Usando `numpy` tenemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,3,5],[3,13,23],[5,23,42]])\n",
    "L = la.cholesky(A)\n",
    "print(np.dot(L.T, L))\n",
    "\n",
    "print(L)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomposición matricial para PCA y mínimos cuadrados\n",
    "\n",
    " Un autovector de una matriz $A$ es un vector distinto cero $v$ tal que\n",
    " \n",
    " $$Av = \\lambda v$$\n",
    " \n",
    " para un escalar $\\lambda$. Un valor $\\lambda$ es llamado un autovalor de $A$.\n",
    " \n",
    " Si una matriz $A$ de orden $n\\times n$ tienen $n$ autovectores independientes, entonces $A$ puede ser descompuesto de la siguiente manera:\n",
    " \n",
    " $$A = B \\Lambda B^{-1}$$\n",
    " \n",
    " \n",
    " donde $\\Lambda$ es una matriz diagonal cuyas elementos en las diagonales son los autovalores  de $A$ y las columnas de $B$ son los autovectores de $A$.\n",
    " \n",
    " * Una matriz $A$ de orden $n \\times n \\iff $ tiene $n$ autovectores linealmente.\n",
    " * Una matriz definida positiva, simétrica tiene sólo autovalores positivos y su descomposición:\n",
    " \n",
    " $$A=B\\Lambda B^{-1}$$\n",
    " \n",
    " es vía una transformación ortogonal $B$, es decir sus autovectores con conjuntos ortonormales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Autovalores\n",
    "\n",
    "Si $v$ es un autovector de una matriz $A$ es de orden $n \\times n$ con un autovalor $\\lambda$, entonces\n",
    "\n",
    "$$Av - \\lambda I = 0$$\n",
    "\n",
    "donde $I$ es la matriz identidad de dimensión $n$ y $0$ es el vector cero n-dimensional. Por tanto los autovalores de $A$ satisfacen:\n",
    "\n",
    "$$\\det(A - \\lambda I) = 0$$\n",
    "\n",
    "El lado izquierdo de la ecuación es un polinomio en $\\lambda$ y es llamado polinomio característico de $A$. Así para encontrar un autovalor de $A$, debemos encontrar las raices del polinomio característico.\n",
    "\n",
    "\n",
    "Computacionalmente, el cálculo de polinomio caractístico y resolver las raices se hace a través de cálculo numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "A = np.array([[0,1,1],[2,1,0],[1,1,1]])\n",
    "\n",
    "u, V = la.eig(A)\n",
    "print(np.dot(V,np.dot(np.diag(u), la.inv(V))))\n",
    "print(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Si $A$ es definida positva (matriz convarianza)\n",
    "# usamos real_if_close \n",
    "\n",
    "A = np.array([[0,1,1],[2,1,0],[3,4,5]])\n",
    "u, V = la.eig(A)\n",
    "print(u)\n",
    "print (np.real_if_close(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores singulares\n",
    "\n",
    "Para una matriz $A$ de orden $m \\times n$, definimos sus `valores singulares` como  las raices cuadradas de los autovalores de $A^TA$. Esos valores son siempre bien definidos ya que $A^TA$ es siempre simétrica, definida positiva, de manera que sus autovalores son reales y positivos.\n",
    "\n",
    "Los valores singulares tienen importantes propiedades de la matriz. Geométricamente una matriz $A$ lleva la esfera unitaria $R^n$ a un elipse. Los valores singulares son la longitud de los semiejes.  \n",
    "\n",
    "\n",
    "\n",
    "Los valores singulares proporcionan una medida de la `estabilidad de una matriz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomposición QR\n",
    "\n",
    "QR es un método de descomposición para escribir una matriz $A$ como el producto de dos matrices:\n",
    "\n",
    "$$A = QR$$\n",
    "\n",
    "donde $Q$ es una matriz ortogonal de orden $m \\times n$ y $R$ es una matriz triangular de orden $n \\times n$. Las primeras $k$ columnas de $Q$ son una base ortonormal para el espacio columna de las primeras $k$ columnas de $A$.\n",
    "\n",
    "La descomposición QR iterativa es a menudo usada en el cálculo de los autovalores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pprint  # módulo de mejor presentación de las estructuras de Python\n",
    "\n",
    "A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])  # Usar numpy para crear la matriz\n",
    "Q, R = la.qr(A)  # Descomposición QR con SciPy\n",
    "\n",
    "print(\"A:\")\n",
    "pprint.pprint(A)\n",
    "\n",
    "print(\"Q:\")\n",
    "pprint.pprint(Q)\n",
    "\n",
    "print(\"R:\")\n",
    "pprint.pprint(R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomposición en valores singulares\n",
    "\n",
    "Una importante descomposición de una matriz, es la `Descomposición en valores singulares ` o `SVD`. Para alguna matriz $A$ de orden $m \\times n$, podemos escribir\n",
    "\n",
    "$$A = UDV^T$$\n",
    "\n",
    "donde $U$ es una matriz unitaria (ortogonal en el caso real) de orden $m \\times m$, D es una matriz diagonal de orden $m \\times n$, con elementos en la diagonal $d_1, \\dots d_m$ no negativos y $V$ una matriz unitaria (ortogonal en el caso real) de orden $n \\times n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo (Jeremy kun)\n",
    "\n",
    "from numpy.linalg import svd\n",
    "import pprint\n",
    " \n",
    "MR = [\n",
    "    [2, 5, 3],\n",
    "    [1, 2, 1],\n",
    "    [4, 1, 1],\n",
    "    [3, 5, 2],\n",
    "    [5, 3, 1],\n",
    "    [4, 5, 5],\n",
    "    [2, 4, 2],\n",
    "    [2, 2, 5],\n",
    "]\n",
    "\n",
    "U, valores_singulares, V = svd(MR)\n",
    "\n",
    "\n",
    "print (\"U:\")\n",
    "pprint.pprint(U)\n",
    "\n",
    "print (\"Valores singulares:\")\n",
    "pprint.pprint(valores_singulares)\n",
    "\n",
    "print (\"V:\")\n",
    "pprint.pprint(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos resultados, son un poco extraños., ya que las matrices $U$ y $V$ son de forma incorrecta, debido a que la matriz de entrada tiene rango $3$. Mejoremos un poco, esta situación, reconstuyendo la forma de la matriz inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "S = np.vstack([\n",
    "    np.diag(valores_singulares),\n",
    "    np.zeros((5, 3)),\n",
    "])\n",
    " \n",
    "print(np.round(MR - np.dot(U, np.dot(S, V)), decimals=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Repetimos  el procedimiento anterior\n",
    "\n",
    "U, valores_singulares, V = svd(MR, full_matrices=False)\n",
    "\n",
    "\n",
    "print (\"U:\")\n",
    "pprint.pprint(U)\n",
    "\n",
    "print (\"Valores singulares:\")\n",
    "pprint.pprint(valores_singulares)\n",
    "\n",
    "print (\"V:\")\n",
    "pprint.pprint(V)\n",
    "\n",
    "S = np.diag(valores_singulares)\n",
    "\n",
    "print(\"Diferencia: \")\n",
    "pprint.pprint(np.round(MR - np.dot(U, np.dot(S, V)), decimals=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto hace que la reconstrucción mucho mejor , ya que sólo podemos multiplicar todo sin tener que añadir más filas de ceros a $D$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Número de condición\n",
    "\n",
    "Una de las aplicaciones del SVD es lo que se conoce como **número de condición**:\n",
    "\n",
    "Para una matriz cuadrada no singular, el número de condición que se define como\n",
    "\n",
    "$$cond (A) = \\Vert A\\Vert\\Vert A^{-1}\\Vert$$ y se puede expresar usando valores singulares.\n",
    "\n",
    "Para eso si $A =UDV^T$, entonces $A^{-1}= VD^{-1}U^T$. Por lo tanto los valores singulares de $A^{-1}$ s son $\\frac{1}{\\lambda_1}, \\dots, \\frac{1}{\\lambda_n}$ y \n",
    "\n",
    "\n",
    "$$\\frac{1}{\\lambda_n} \\geq \\dots \\geq \\frac{1}{\\lambda_1}$$.\n",
    "\n",
    "De aquí se obtiene que $\\Vert A^{-1} \\Vert = \\frac{1}{\\lambda_n}$, de manera que \n",
    "\n",
    "$$cond (A) = \\Vert A\\Vert\\Vert A^{-1}\\Vert = \\lambda_1/\\lambda_n$$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pprint \n",
    "\n",
    "U, s, V = np.linalg.svd(A)\n",
    "\n",
    "print (\"Valores singulares:\")\n",
    "pprint.pprint(s)\n",
    "\n",
    "print (\"Numero de condicion:\")\n",
    "pprint.pprint(max(s)/min(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las principales aplicaciones a machine learning es el siguiente teorema:\n",
    "\n",
    "**Teorema SVD** \n",
    "\n",
    "`Cálcular el mejor subespacio k-dimensional se reduce a k aplicaciones del problema unidimensional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ejemplo completo de Teorema SVD: \n",
    "# \"Cálcular el mejor subespacio k-dimensional se reduce a k aplicaciones del problema unidimensional.\"\n",
    "#\n",
    "# En este ejemplo, generamos una matriz de datos, calculamos su SVD, y reconstruimos \n",
    "# la mejor aproximación de rango k (subespacio k-dimensional) usando los k componentes \n",
    "# singulares principales. Se incluye visualización de los valores singulares y la comparación\n",
    "# entre datos originales y reconstruidos.\n",
    "#\n",
    "\n",
    "# =============================================================================\n",
    "# Sección 1: Importar librerías necesarias\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Para gráficos 3D en caso de ser necesarios\n",
    "import random\n",
    "\n",
    "# =============================================================================\n",
    "# Sección 2: Definición de funciones auxiliares\n",
    "# =============================================================================\n",
    "\n",
    "def generar_datos(n_muestras=200, n_dim=3, ruido=0.1, semilla=42):\n",
    "    \"\"\"\n",
    "    Genera datos sintéticos en un espacio de n_dim dimensiones.\n",
    "    Se genera un conjunto de puntos que se aproximan a un plano (o línea, etc.)\n",
    "    dependiendo de las combinaciones lineales.\n",
    "    \n",
    "    Parámetros:\n",
    "    - n_muestras: número de puntos a generar.\n",
    "    - n_dim: dimensiones del espacio.\n",
    "    - ruido: amplitud del ruido gaussiano agregado.\n",
    "    - semilla: para reproducibilidad.\n",
    "    \n",
    "    Retorna:\n",
    "    - datos: matriz de datos de tamaño (n_muestras, n_dim).\n",
    "    \"\"\"\n",
    "    np.random.seed(semilla)\n",
    "    # Generar datos base de forma aleatoria\n",
    "    datos = np.random.rand(n_muestras, n_dim)\n",
    "    \n",
    "    # Crear una combinación lineal para simular dependencia entre variables\n",
    "    if n_dim >= 2:\n",
    "        # Se asume que la segunda dimensión es casi linealmente dependiente de la primera\n",
    "        datos[:, 1] = 0.5 * datos[:, 0] + np.random.normal(0, ruido, n_muestras)\n",
    "    if n_dim >= 3:\n",
    "        # La tercera dimensión es combinación lineal de la primera y segunda\n",
    "        datos[:, 2] = 0.3 * datos[:, 0] + 0.7 * datos[:, 1] + np.random.normal(0, ruido, n_muestras)\n",
    "    \n",
    "    return datos\n",
    "\n",
    "def calcular_svd(datos):\n",
    "    \"\"\"\n",
    "    Calcula la descomposición en valores singulares de la matriz de datos.\n",
    "    \n",
    "    Parámetros:\n",
    "    - datos: matriz de datos (n_muestras x n_dim)\n",
    "    \n",
    "    Retorna:\n",
    "    - U, S, Vt: componentes de la descomposición SVD, donde:\n",
    "      datos = U * diag(S) * Vt\n",
    "    \"\"\"\n",
    "    U, S, Vt = np.linalg.svd(datos, full_matrices=False)\n",
    "    return U, S, Vt\n",
    "\n",
    "def reconstruir_datos(U, S, Vt, k):\n",
    "    \"\"\"\n",
    "    Reconstruye la matriz de datos utilizando los k componentes singulares principales.\n",
    "    \n",
    "    Parámetros:\n",
    "    - U, S, Vt: resultados de la SVD.\n",
    "    - k: número de componentes a conservar.\n",
    "    \n",
    "    Retorna:\n",
    "    - datos_reconstruidos: aproximación de la matriz de datos usando rango k.\n",
    "    \"\"\"\n",
    "    # Seleccionar los primeros k componentes\n",
    "    U_k = U[:, :k]\n",
    "    S_k = np.diag(S[:k])\n",
    "    Vt_k = Vt[:k, :]\n",
    "    datos_reconstruidos = np.dot(np.dot(U_k, S_k), Vt_k)\n",
    "    return datos_reconstruidos\n",
    "\n",
    "def error_reconstruccion(original, reconstruido):\n",
    "    \"\"\"\n",
    "    Calcula el error de reconstrucción usando la norma Frobenius.\n",
    "    \n",
    "    Parámetros:\n",
    "    - original: matriz de datos original.\n",
    "    - reconstruido: matriz reconstruida.\n",
    "    \n",
    "    Retorna:\n",
    "    - error: valor de la norma Frobenius entre original y reconstruido.\n",
    "    \"\"\"\n",
    "    error = np.linalg.norm(original - reconstruido, ord='fro')\n",
    "    return error\n",
    "\n",
    "def plot_valores_singulares(S):\n",
    "    \"\"\"\n",
    "    Grafica los valores singulares para observar su decaimiento.\n",
    "    \n",
    "    Parámetros:\n",
    "    - S: vector de valores singulares.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(S) + 1), S, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.title(\"Valores Singulares\")\n",
    "    plt.xlabel(\"Índice\")\n",
    "    plt.ylabel(\"Valor Singular\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_datos_vs_reconstruidos(datos, datos_reconstruidos):\n",
    "    \"\"\"\n",
    "    Grafica los datos originales y los datos reconstruidos.\n",
    "    Se asume que los datos son de 2 o 3 dimensiones.\n",
    "    \n",
    "    Parámetros:\n",
    "    - datos: datos originales.\n",
    "    - datos_reconstruidos: datos aproximados usando SVD.\n",
    "    \"\"\"\n",
    "    n_dim = datos.shape[1]\n",
    "    if n_dim == 2:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(datos[:, 0], datos[:, 1], c='blue', label='Original', alpha=0.6)\n",
    "        plt.scatter(datos_reconstruidos[:, 0], datos_reconstruidos[:, 1], \n",
    "                    c='red', label='Reconstruido', alpha=0.6)\n",
    "        plt.title(\"Datos Originales vs Reconstruidos (2D)\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif n_dim >= 3:\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(datos[:, 0], datos[:, 1], datos[:, 2], c='blue', label='Original', alpha=0.6)\n",
    "        ax.scatter(datos_reconstruidos[:, 0], datos_reconstruidos[:, 1], datos_reconstruidos[:, 2],\n",
    "                   c='red', label='Reconstruido', alpha=0.6)\n",
    "        ax.set_title(\"Datos Originales vs Reconstruidos (3D)\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_zlabel(\"Z\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"La visualización solo se implementa para 2 o 3 dimensiones.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Sección 3: Ejecución principal del ejemplo\n",
    "# =============================================================================\n",
    "\n",
    "# Paso 1: Generar datos sintéticos\n",
    "datos = generar_datos(n_muestras=300, n_dim=3, ruido=0.05, semilla=123)\n",
    "print(\"Datos originales generados con forma:\", datos.shape)\n",
    "\n",
    "# Paso 2: Calcular la SVD de la matriz de datos\n",
    "U, S, Vt = calcular_svd(datos)\n",
    "print(\"Dimensiones de U:\", U.shape)\n",
    "print(\"Dimensiones de S:\", S.shape)\n",
    "print(\"Dimensiones de Vt:\", Vt.shape)\n",
    "\n",
    "# Paso 3: Visualizar los valores singulares\n",
    "plot_valores_singulares(S)\n",
    "\n",
    "# =============================================================================\n",
    "# Sección 4: Reconstrucción de datos para diferentes valores de k\n",
    "# =============================================================================\n",
    "\n",
    "# k puede variar desde 1 hasta el número de dimensiones originales (en este caso 3)\n",
    "valores_k = [1, 2, 3]\n",
    "errores = []\n",
    "\n",
    "for k in valores_k:\n",
    "    # Reconstruir los datos usando los primeros k componentes singulares\n",
    "    datos_reconstruidos = reconstruir_datos(U, S, Vt, k)\n",
    "    \n",
    "    # Calcular el error de reconstrucción\n",
    "    err = error_reconstruccion(datos, datos_reconstruidos)\n",
    "    errores.append(err)\n",
    "    \n",
    "    # Imprimir el error obtenido\n",
    "    print(f\"Error de reconstrucción usando k = {k}: {err:.5f}\")\n",
    "    \n",
    "    # Graficar la reconstrucción en comparación con los datos originales\n",
    "    plot_datos_vs_reconstruidos(datos, datos_reconstruidos)\n",
    "\n",
    "# =============================================================================\n",
    "# Sección 5: Análisis y visualización del error de reconstrucción\n",
    "# =============================================================================\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(valores_k, errores, 'ro-', linewidth=2, markersize=8)\n",
    "plt.title(\"Error de reconstrucción vs. número de componentes k\")\n",
    "plt.xlabel(\"Número de componentes (k)\")\n",
    "plt.ylabel(\"Error (norma Frobenius)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### Ejercicio 1: Descomposición LU y solución de sistemas de ecuaciones\n",
    "\n",
    "**Problema:**  \n",
    "Dada la matriz cuadrada  \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "4 & 3 & 0 \\\\\n",
    "3 & 4 & -1 \\\\\n",
    "0 & -1 & 4\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "y el vector de términos independientes  \n",
    "$$\n",
    "b = \\begin{pmatrix} 24 \\\\ 30 \\\\ -24 \\end{pmatrix},\n",
    "$$\n",
    "se pide:  \n",
    "1. Realizar la descomposición $A = LU$, donde $L$ es una matriz triangular inferior con 1 en la diagonal y $U$ es una matriz triangular superior.  \n",
    "2. Resolver el sistema $Ax = b$ usando la descomposición obtenida (procedimiento de sustitución hacia adelante y hacia atrás).\n",
    "\n",
    "**Sugerencias de solución:**  \n",
    "- Aplica el método de eliminación gaussiana para factorizar la matriz en $L$ y $U$.  \n",
    "- Primero, resuelve $Ly = b$ por sustitución hacia adelante.  \n",
    "- Luego, resuelve $Ux = y$ por sustitución hacia atrás.  \n",
    "- Puedes verificar la solución comparándola con la obtenida usando alguna librería de álgebra lineal, como `numpy.linalg.solve`.\n",
    "\n",
    "#### Ejercicio 2: Descomposición de Cholesky en matrices simétricas y definidas positivas\n",
    "\n",
    "**Problema:**  \n",
    "Sea la matriz simétrica definida positiva  \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "6 & 15 & 55 \\\\\n",
    "15 & 55 & 225 \\\\\n",
    "55 & 225 & 979\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "1. Realiza la descomposición de Cholesky, es decir, encuentra la matriz triangular inferior $L$ tal que $A = LL^T$.  \n",
    "2. Utiliza la descomposición para resolver el sistema $Ax = b$ para un vector $b$ dado (por ejemplo, $b = \\begin{pmatrix} 76 \\\\ 295 \\\\ 1259 \\end{pmatrix}$).\n",
    "\n",
    "**Sugerencias de solución:**  \n",
    "- La descomposición de Cholesky es posible solo para matrices simétricas y definidas positivas.  \n",
    "- Calcula cada entrada de $L$ usando las fórmulas:\n",
    "  $$\n",
    "  L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}, \\quad\n",
    "  L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik}L_{jk} \\right), \\quad i > j.\n",
    "  $$\n",
    "- Resuelve $Ly = b$ y luego $L^Tx = y$.\n",
    "\n",
    "#### Ejercicio 3: Descomposición en valores singulares (SVD) y reducción de dimensionalidad\n",
    "\n",
    "**Problema:**  \n",
    "Considera una matriz de datos $X$ de tamaño $m \\times n$ (por ejemplo, $m = 100$ muestras y $n = 50$ características).  \n",
    "1. Calcula la descomposición SVD de $X$:  \n",
    "   $$\n",
    "   X = U \\Sigma V^T.\n",
    "   $$\n",
    "2. Utilizando solo los $k$ valores singulares y vectores asociados más grandes (por ejemplo, $k = 5$), reconstruye una aproximación $X_k$ de la matriz original.  \n",
    "3. Analiza el error de reconstrucción y comenta sobre la varianza capturada por estos $k$ componentes.\n",
    "\n",
    "**Sugerencias de solución:**  \n",
    "- Elije $k$ según la proporción de varianza explicada (puedes calcular el porcentaje de varianza explicado con cada valor singular).  \n",
    "- La reconstrucción se realiza mediante:\n",
    "  $$\n",
    "  X_k = U_k \\Sigma_k V_k^T,\n",
    "  $$\n",
    "  donde $U_k$ son las primeras $k$ columnas de $U$, $\\Sigma_k$ es la matriz diagonal con los $k$ valores singulares más grandes, y $V_k^T$ contiene las primeras $k$ filas de $V^T$.  \n",
    "- Compara $X$ y $X_k$ evaluando la norma de la diferencia (por ejemplo, norma Frobenius).\n",
    "\n",
    "\n",
    "#### Ejercicio 4: Análisis de componentes principales (PCA)\n",
    "\n",
    "**Problema:**  \n",
    "En el contexto de aprendizaje automático, el análisis de componentes principales (PCA) se utiliza para reducir la dimensionalidad de los datos y mejorar la eficiencia de los modelos.  \n",
    "1. Dado un conjunto de datos $X$ (por ejemplo, imágenes en escala de grises de dimensión $28 \\times 28$ convertidas en vectores), aplica la SVD para obtener sus componentes principales.  \n",
    "2. Reduce la dimensionalidad a $k$ componentes principales y visualiza la varianza explicada en función de $k$.  \n",
    "3. Comenta cómo la reducción de dimensionalidad puede ayudar a mejorar el rendimiento de algoritmos de clasificación o clustering en IA.\n",
    "\n",
    "**Sugerencias de solución:**  \n",
    "- Centra los datos (resta la media) antes de aplicar la SVD.  \n",
    "- Utiliza la SVD para extraer los componentes principales, ya que estos corresponden a las direcciones de mayor varianza.  \n",
    "- Grafica la varianza acumulada para determinar el número óptimo de componentes $k$.  \n",
    "- Discute cómo la reducción de dimensionalidad reduce el ruido, disminuye el tiempo de cómputo y puede prevenir el sobreajuste, facilitando la visualización y análisis en tareas de IA.\n",
    "\n",
    "#### Ejercicio 5: Recomendadores basados en SVD\n",
    "\n",
    "**Problema:**  \n",
    "Los sistemas de recomendación pueden basarse en la descomposición SVD para identificar patrones en los datos de usuarios y productos.  \n",
    "1. Se tiene una matriz de calificaciones $R$ de tamaño $m \\times n$, donde $m$ es el número de usuarios y $n$ el número de productos.  \n",
    "2. Realiza la SVD de la matriz $R$ para predecir las calificaciones faltantes.  \n",
    "3. Evalúa la precisión de la predicción utilizando alguna métrica (por ejemplo, RMSE-raíz del error cuadrático medio).\n",
    "\n",
    "**Sugerencias de solución:**  \n",
    "- Antes de aplicar la SVD, se pueden realizar técnicas de imputación para las entradas faltantes o utilizar variantes de la SVD que soporten datos esparsos.  \n",
    "- La predicción de una calificación se puede obtener aproximando la matriz $R$ por su reconstrucción de rango $k$ y utilizando la entrada correspondiente en $R_k$.  \n",
    "- Evalúa el modelo dividiendo los datos en conjunto de entrenamiento y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
