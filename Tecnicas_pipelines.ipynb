{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4e819f-ecc5-49bc-ab0a-3c86e3f9af22",
   "metadata": {},
   "source": [
    "## Notas de pipelines\n",
    "\n",
    "Estas notas acompañan al cuaderno [Pipeline.ipynb](https://github.com/kapumota/Cuadernos1/blob/main/Pipelines.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1d131-f94c-40de-88eb-8eb28f0c01b6",
   "metadata": {},
   "source": [
    "### Introducción a scikit-learn y sus componentes básicos\n",
    "\n",
    "Scikit-learn es una biblioteca de Python ampliamente utilizada en el mundo del aprendizaje automático y la ciencia de datos, la cual facilita la construcción, entrenamiento y evaluación de modelos predictivos. En scikit-learn, el flujo de trabajo se basa en tres conceptos esenciales:\n",
    "\n",
    "- **Estimador (Estimator):** Es cualquier objeto que implemente los métodos `fit` (para entrenar o ajustar el modelo) y, en su caso, `predict` (para generar predicciones). Los estimadores pueden ser tanto modelos predictivos (por ejemplo, regresión lineal, SVM, árboles de decisión) como transformadores de datos (por ejemplo, escaladores, codificadores, transformaciones no lineales).\n",
    "\n",
    "- **Transformador (Transformer):** Es un tipo especial de estimador que implementa además del método `fit` el método `transform`. Su función es modificar o transformar los datos de entrada. Por ejemplo, normalizar, escalar, aplicar transformaciones no lineales o generar nuevas características.\n",
    "\n",
    "- **Pipeline:** Es una herramienta que permite encadenar secuencialmente varios transformadores y un estimador final. La gran ventaja es que se garantiza que cada paso del proceso se aplica de forma consistente tanto en entrenamiento como en predicción. Esto simplifica el flujo de trabajo y reduce la posibilidad de errores (como aplicar transformaciones diferentes a los datos de entrenamiento y a los de prueba).\n",
    "\n",
    "Estos tres conceptos trabajan conjuntamente para facilitar la construcción de flujos de trabajo complejos y reproducibles. Un pipeline es, en sí mismo, un estimador, lo que permite encapsular todo el proceso (preprocesamiento y modelado) en un solo objeto.\n",
    "\n",
    "\n",
    "### 2. Pipeline en scikit-learn: Diseño y composición\n",
    "\n",
    "El **Pipeline** es una de las herramientas más poderosas de scikit-learn. Su función es encadenar múltiples pasos, donde cada paso puede ser un transformador o un modelo. Por ejemplo, en el código se construyen pipelines separados para el preprocesamiento de variables numéricas y categóricas, los cuales se combinan mediante un `ColumnTransformer`. Algunas ventajas del uso de pipelines son:\n",
    "\n",
    "- **Reproducibilidad:** Se asegura que cada transformación se aplica de la misma forma en cada fase del desarrollo.\n",
    "- **Evitar fugas de información:** Al encapsular el preprocesamiento dentro del pipeline, se reduce el riesgo de aplicar información del conjunto de prueba en la fase de entrenamiento.\n",
    "- **Facilidad para la búsqueda de hiperparámetros:** Herramientas como `GridSearchCV` permiten ajustar parámetros de cada uno de los pasos del pipeline, incluyendo parámetros de transformadores y del modelo final.\n",
    "- **Mantenimiento y escalabilidad:** Se simplifica el código y se facilita la incorporación de nuevos pasos o transformaciones.\n",
    "\n",
    "El diseño del pipeline en el ejemplo se compone de dos grandes bloques: el preprocesamiento (dividido en pipelines para variables numéricas y categóricas) y el modelo de regresión Ridge.\n",
    "\n",
    "\n",
    "### 3. Estimadores y transformadores: Definición y diferencias\n",
    "\n",
    "#### Estimador\n",
    "\n",
    "Un **estimador** en scikit-learn es cualquier objeto que puede aprender a partir de datos. Se caracteriza por el método `fit()`, y en el caso de modelos predictivos, también por `predict()`. Los estimadores pueden incluir algoritmos de clasificación, regresión, clustering, entre otros.\n",
    "\n",
    "#### Transformador\n",
    "\n",
    "Un **transformador** es un estimador que implementa el método `transform()`, permitiendo modificar o transformar los datos. Muchos transformadores tienen el propósito de preparar los datos para el modelado, por ejemplo:\n",
    "\n",
    "- **Estandarizadores (StandardScaler, RobustScaler):** Normalizan o escalan las características.\n",
    "- **Codificadores (OneHotEncoder):** Transforman variables categóricas en variables binarias.\n",
    "- **Generadores de características (PolynomialFeatures):** Crean nuevas variables a partir de interacciones y potencias de las existentes.\n",
    "\n",
    "En el código, se utiliza un transformador personalizado llamado `LogTransformer`, diseñado para aplicar una transformación logarítmica a variables numéricas que presentan distribución asimétrica.\n",
    "\n",
    "#### Pipeline\n",
    "\n",
    "Un **pipeline** es, esencialmente, una secuencia encadenada de transformadores y un estimador final. Este objeto es a la vez un estimador, lo que permite llamarlo directamente para ajustar y predecir, encapsulando todas las operaciones necesarias de preprocesamiento y modelado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754e358-93f6-4ad8-b73d-84e44404ea74",
   "metadata": {},
   "source": [
    "## 1. Pipeline con transformadores personalizados y regresión lineal (Ridge)\n",
    "\n",
    "El código presentado se divide en diversas secciones, que se explican a continuación:\n",
    "\n",
    "#### 1.1 Generación del dataset sintético\n",
    "\n",
    "Se generan datos de forma aleatoria para simular un problema de regresión:\n",
    "\n",
    "- **Variables numéricas:**  \n",
    "  - `feature1` se genera mediante una distribución exponencial, lo que típicamente produce valores asimétricos y con cola larga.\n",
    "  - `feature2` se genera mediante una distribución normal con media 50 y desviación estándar 10.\n",
    "  \n",
    "- **Variable categórica:**  \n",
    "  - `category` se genera mediante una selección aleatoria entre tres categorías: A, B y C.\n",
    "  \n",
    "- **Variable objetivo (target):**  \n",
    "  - Se define una relación lineal entre las variables numéricas con la adición de ruido gaussiano, simulando un escenario real donde los datos presentan variabilidad adicional.\n",
    "\n",
    "La unión de estos DataFrames en `data` permite tener un conjunto completo que incluye tanto variables predictoras como la variable objetivo.\n",
    "\n",
    "#### 1.2 Creación de un transformador personalizado: LogTransformer\n",
    "\n",
    "El **LogTransformer** es una clase que hereda de `BaseEstimator` y `TransformerMixin`, lo que garantiza que se comportará como un estimador en scikit-learn. Su propósito es aplicar la función logarítmica a las variables que se consideran asimétricas. Algunos puntos importantes sobre este transformador:\n",
    "\n",
    "- **Constructor (`__init__`):**  \n",
    "  Se reciben parámetros como `variables` (una lista con los nombres de columnas a transformar) y `column_names` en caso de trabajar con arrays en lugar de DataFrames.\n",
    "  \n",
    "- **Método `fit`:**  \n",
    "  Durante el ajuste, si los datos son un DataFrame se almacenan los nombres de las columnas. Esto es útil para asegurar que, en el caso de transformaciones posteriores, se pueda reconstruir el DataFrame original.\n",
    "  \n",
    "- **Método `transform`:**  \n",
    "  Se transforma cada variable listada en `variables` aplicando `np.log1p`, que calcula el logaritmo de (1 + x). El uso de `log1p` es una técnica común para evitar problemas con valores cero, ya que el logaritmo de 0 es indefinido. De esta forma, se reduce la asimetría de la distribución de las variables, ayudando a que los métodos que asumen una distribución más cercana a la normalidad tengan un mejor desempeño.\n",
    "\n",
    "#### 1.3 Pipelines para variables numéricas y categóricas\n",
    "\n",
    "Se definen dos pipelines distintos que permiten aplicar transformaciones específicas a cada tipo de variable:\n",
    "\n",
    "#### Pipeline para variables numéricas\n",
    "\n",
    "Contiene los siguientes pasos:\n",
    "1. **SimpleImputer:**  \n",
    "   Se utiliza para imputar valores faltantes en las variables numéricas utilizando la mediana, lo que es robusto ante outliers.\n",
    "2. **LogTransformer:**  \n",
    "   Se aplica la transformación logarítmica a la variable `feature1`, que probablemente presente asimetría.\n",
    "3. **RobustScaler:**  \n",
    "   Este transformador escala los datos utilizando la mediana y el rango intercuartílico (IQR), lo que lo hace menos sensible a valores atípicos. A diferencia de escaladores basados en la media y desviación estándar, el escalado robusto es más adecuado cuando existen outliers.\n",
    "4. **PolynomialFeatures:**  \n",
    "   Permite generar nuevas características a partir de las originales, incluyendo interacciones y términos al cuadrado. Esto es útil para capturar relaciones no lineales, aunque a su vez incrementa la dimensionalidad del problema. El parámetro `include_bias=False` indica que no se añade una constante adicional (intercepto) en el diseño de las características.\n",
    "\n",
    "#### Pipeline para variables categóricas\n",
    "\n",
    "Este pipeline se encarga de procesar la variable `category`:\n",
    "1. **SimpleImputer:**  \n",
    "   Se aplica la estrategia de imputación de la moda, es decir, se reemplazan los valores faltantes por la categoría más frecuente.\n",
    "2. **OneHot Encoding a través de FunctionTransformer:**  \n",
    "   Se utiliza una función lambda para aplicar `pd.get_dummies` a la serie resultante, convirtiendo la variable categórica en múltiples columnas binarias. La función `squeeze()` se utiliza para transformar el DataFrame de una sola columna en una Serie, lo cual es compatible con `get_dummies`.\n",
    "\n",
    "#### 1.4 Composición final con ColumnTransformer y pipeline global\n",
    "\n",
    "El objeto **ColumnTransformer** permite aplicar diferentes pipelines a diferentes subconjuntos de columnas de forma simultánea:\n",
    "- Se asigna el pipeline numérico a las variables en `numeric_features`.\n",
    "- Se asigna el pipeline categórico a las variables en `categorical_features`.\n",
    "\n",
    "El pipeline final encapsula dos pasos:\n",
    "1. **Preprocesamiento:**  \n",
    "   Se aplica el `preprocessor` que integra los pipelines para las variables numéricas y categóricas.\n",
    "2. **Regresor:**  \n",
    "   Se utiliza un modelo de regresión Ridge. La regresión Ridge es una forma de regresión lineal que añade una penalización L2 a los coeficientes. Esto significa que el ajuste del modelo no solo minimiza el error cuadrático, sino que también penaliza la magnitud de los coeficientes, lo que ayuda a evitar el sobreajuste (overfitting).\n",
    "\n",
    "\n",
    "### 2. Regresión Ridge y la regularización L2\n",
    "\n",
    "La regresión Ridge es una técnica de regularización que modifica la función de costo de la regresión lineal para penalizar coeficientes grandes. Matemáticamente, se define la función de costo de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij})^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\beta$ representa los coeficientes del modelo.\n",
    "- $\\alpha$ es el parámetro de regularización, que controla la magnitud de la penalización. Valores mayores de \\( \\alpha \\) imponen una mayor restricción, lo que suele reducir la varianza del modelo a costa de aumentar el sesgo.\n",
    "\n",
    "La regularización Ridge es particularmente útil cuando existen múltiples variables correlacionadas o cuando se han generado muchas características (por ejemplo, tras la expansión polinómica) que pueden llevar a problemas de sobreajuste.\n",
    "\n",
    "\n",
    "### 3. Búsqueda de hiperparámetros y validación cruzada\n",
    "\n",
    "Una parte fundamental del modelado en aprendizaje automático es la selección de los mejores hiperparámetros. En el ejemplo se utiliza **GridSearchCV**, que implementa la estrategia de búsqueda exhaustiva sobre un conjunto predefinido de valores:\n",
    "\n",
    "- **Grid Search:**  \n",
    "  Se define un diccionario `param_grid` que contiene combinaciones de valores para hiperparámetros. En el ejemplo se ajustan:\n",
    "  - `regressor__alpha`: Valores de \\( \\alpha \\) para el modelo Ridge (0.1, 1.0, 10.0).\n",
    "  - `preprocessor__num__poly__degree`: El grado de la expansión polinómica (1 o 2).  \n",
    "  Es importante notar que el uso de pipelines permite referenciar los hiperparámetros de pasos anidados utilizando la notación de doble guion bajo (`__`).\n",
    "\n",
    "- **Validación Cruzada (CV):**  \n",
    "  GridSearchCV realiza una validación cruzada (en este caso, con 5 particiones) para evaluar cada combinación de hiperparámetros. La idea es dividir el conjunto de entrenamiento en múltiples subconjuntos, entrenar el modelo en algunos y validarlo en otros, lo que ayuda a estimar de manera robusta el rendimiento y a evitar que el modelo se adapte únicamente a una partición de los datos.\n",
    "\n",
    "Esta metodología permite encontrar el conjunto de hiperparámetros que optimiza la métrica de evaluación (en el ejemplo, la métrica es el error cuadrático negativo, es decir, `neg_mean_squared_error`).\n",
    "\n",
    "\n",
    "### 4. Análisis de residuos y evaluación de métricas\n",
    "\n",
    "Una vez que se ha seleccionado y entrenado el modelo, es crucial evaluar su desempeño. En el ejemplo se utilizan tres métricas principales:\n",
    "\n",
    "- **MSE (Mean Squared Error):**  \n",
    "  Es el promedio de los errores al cuadrado. Penaliza fuertemente los errores grandes, lo que puede ser útil para identificar cuándo existen predicciones muy alejadas de los valores reales.\n",
    "  \n",
    "- **MAE (Mean Absolute Error):**  \n",
    "  Calcula el promedio de los errores absolutos. Es menos sensible a valores extremos en comparación con el MSE, lo que lo hace útil cuando se desea una medida más robusta frente a outliers.\n",
    "  \n",
    "- **$R^2$ (Coeficiente de determinación):**  \n",
    "  Mide la proporción de la varianza de la variable dependiente que es explicada por las variables independientes. Un valor cercano a 1 indica un buen ajuste, mientras que valores bajos sugieren que el modelo no está capturando la variabilidad de los datos.\n",
    "\n",
    "#### Análisis de residuos\n",
    "\n",
    "El análisis de residuos es una técnica diagnóstica que consiste en estudiar la diferencia entre los valores reales y las predicciones del modelo. En el ejemplo, se calcula el residuo para cada observación (la diferencia entre el valor real y el predicho) y se realiza un gráfico de dispersión entre las predicciones y los residuos:\n",
    "\n",
    "- **Interpretación del gráfico:**  \n",
    "  Se traza una línea horizontal en cero para ayudar a visualizar la dispersión de los residuos. Un patrón aleatorio sin estructura aparente sugiere que el modelo está capturando correctamente la relación subyacente. En cambio, patrones sistemáticos (por ejemplo, residuos que aumentan o disminuyen con las predicciones) pueden indicar que existen aspectos del problema que el modelo no está considerando, como relaciones no lineales o la presencia de outliers.\n",
    "\n",
    "\n",
    "### 5. Aspectos avanzados en el uso de scikit-learn\n",
    "\n",
    "Además de lo explicado anteriormente, existen varios aspectos avanzados que vale la pena resaltar:\n",
    "\n",
    "#### 5.1 Composición de transformadores y uso de ColumnTransformer\n",
    "\n",
    "El uso de **ColumnTransformer** permite aplicar diferentes pipelines a subconjuntos de columnas en el mismo conjunto de datos. Esto es especialmente útil cuando se trabaja con datos heterogéneos (por ejemplo, datos numéricos y categóricos). Gracias a esta herramienta, se puede:\n",
    "- Imputar valores faltantes de forma diferenciada.\n",
    "- Aplicar transformaciones específicas (por ejemplo, escalado o codificación) a cada tipo de variable.\n",
    "- Encapsular todo el preprocesamiento en un solo objeto que se integra al pipeline global.\n",
    "\n",
    "#### 5.2 Transformadores personalizados y el ciclo de vida de un transformador\n",
    "\n",
    "El desarrollo de transformadores personalizados, como el `LogTransformer` del ejemplo, es una práctica avanzada que permite adaptar el preprocesamiento a las necesidades específicas del problema. Al heredar de `BaseEstimator` y `TransformerMixin`, se asegura que el transformador se integre perfectamente en el ecosistema de scikit-learn. Aspectos importantes en el diseño de estos transformadores son:\n",
    "- **Gestión de DataFrames vs. arrays:**  \n",
    "  El transformador debe ser capaz de trabajar tanto con estructuras de datos de pandas como con arrays de NumPy. En el ejemplo se contempla esta situación, permitiendo que, en caso de que los datos no sean DataFrame, se utilice un parámetro `column_names` para reconstruir la estructura.\n",
    "- **Evitación de problemas numéricos:**  \n",
    "  En transformaciones logarítmicas, es común sumar un pequeño valor (en este caso se usa `log1p`) para evitar problemas cuando se tienen ceros o valores negativos.\n",
    "\n",
    "#### 5.3 Técnicas de escalado: RobustScaler vs. StandardScaler\n",
    "\n",
    "El escalado de variables es fundamental para muchos algoritmos de machine learning, especialmente aquellos basados en distancias o penalizaciones. Dos técnicas populares son:\n",
    "- **StandardScaler:**  \n",
    "  Estandariza los datos removiendo la media y escalando a la desviación estándar. Es ideal cuando los datos siguen una distribución aproximadamente normal.\n",
    "- **RobustScaler:**  \n",
    "  Utiliza la mediana y el rango intercuartílico, lo que lo hace más robusto frente a outliers. En escenarios donde existen valores extremos que pueden distorsionar la media, RobustScaler es preferible.\n",
    "\n",
    "#### 5.4 Expansión polinómica con PolynomialFeatures\n",
    "\n",
    "El uso de **PolynomialFeatures** permite transformar variables originales en un espacio de mayor dimensionalidad al incluir interacciones y términos de mayor orden (por ejemplo, cuadrados, productos entre variables). Esto es particularmente útil cuando la relación entre las variables predictoras y la variable objetivo no es estrictamente lineal. Sin embargo, la expansión polinómica incrementa el número de variables y puede conducir a problemas de sobreajuste, por lo que resulta crucial combinarla con técnicas de regularización (como Ridge) y una adecuada selección de hiperparámetros.\n",
    "\n",
    "#### 5.5 Regularización y su impacto en el sobreajuste\n",
    "\n",
    "La **regularización** es una técnica que introduce un término de penalización en la función de costo del modelo. En el caso de Ridge (regularización L2), se penaliza la suma de los cuadrados de los coeficientes. Esto tiene varios efectos:\n",
    "- **Reducción de la varianza:**  \n",
    "  Al limitar la magnitud de los coeficientes, el modelo se vuelve menos sensible a pequeñas variaciones en los datos.\n",
    "- **Manejo de colinealidad:**  \n",
    "  En presencia de variables altamente correlacionadas, la regularización ayuda a estabilizar la estimación de los coeficientes.\n",
    "- **Compensación entre sesgo y varianza:**  \n",
    "  Un mayor grado de regularización puede introducir sesgo, pero la reducción en la varianza suele resultar en una mejor capacidad de generalización.\n",
    "\n",
    "#### 5.6 Búsqueda de hiperparámetros y validación cruzada\n",
    "\n",
    "El proceso de **GridSearchCV** es esencial para encontrar la combinación óptima de hiperparámetros que maximice el desempeño del modelo en datos no vistos. Aspectos importantes a tener en cuenta en este proceso son:\n",
    "- **División de datos en folds:**  \n",
    "  La validación cruzada divide el conjunto de entrenamiento en múltiples subconjuntos. Cada combinación de hiperparámetros se evalúa de manera repetida, lo que reduce la probabilidad de seleccionar un modelo que se adapte solo a una partición específica.\n",
    "- **Selección de la métrica de evaluación:**  \n",
    "  En el ejemplo se utiliza el MSE negativo (`neg_mean_squared_error`) para la optimización, lo que implica que el grid search selecciona los parámetros que minimizan el error cuadrático promedio.\n",
    "- **Exploración conjunta de parámetros:**  \n",
    "  La capacidad de ajustar parámetros de múltiples etapas del pipeline (por ejemplo, tanto el grado polinómico como el parámetro $\\alpha$ del modelo Ridge) permite una exploración más exhaustiva y una mayor integración entre el preprocesamiento y el modelado.\n",
    "\n",
    "#### 5.7 Análisis de residuos en profundidad\n",
    "\n",
    "El análisis de residuos es una herramienta diagnóstica fundamental para evaluar la calidad del ajuste del modelo. Al estudiar los residuos, se pueden identificar patrones que sugieran:\n",
    "- **No linealidad:**  \n",
    "  Si los residuos muestran una tendencia sistemática en función de las predicciones, es posible que existan relaciones no lineales que el modelo lineal no está capturando.\n",
    "- **Heterocedasticidad:**  \n",
    "  La variabilidad de los residuos puede cambiar a lo largo del rango de predicciones. En un modelo bien ajustado se espera que los residuos tengan una dispersión constante.\n",
    "- **Outliers:**  \n",
    "  La presencia de puntos con residuos muy altos o muy bajos puede indicar datos atípicos que afectan el rendimiento del modelo.\n",
    "\n",
    "La visualización gráfica de los residuos, tal como se realiza en el código (gráfico de dispersión de residuos vs. predicciones), es una forma efectiva de detectar estas irregularidades.\n",
    "\n",
    "#### 5.8 Métricas de evaluación y su interpretación\n",
    "\n",
    "Las métricas seleccionadas permiten evaluar el desempeño del modelo desde diferentes perspectivas:\n",
    "\n",
    "- **MSE (Error Cuadrático Medio):**  \n",
    "  Penaliza fuertemente los errores grandes. Es útil para medir la magnitud global del error, aunque puede estar influenciado por outliers.\n",
    "\n",
    "- **MAE (Error Absoluto Medio):**  \n",
    "  Ofrece una medida del error promedio en las predicciones sin amplificar excesivamente los errores grandes, lo que puede dar una perspectiva más robusta frente a valores atípicos.\n",
    "\n",
    "- **$R^2$ (Coeficiente de determinación):**  \n",
    "  Indica qué proporción de la varianza total se explica por el modelo. Un valor cercano a 1 significa que el modelo es capaz de predecir con alta precisión, mientras que valores bajos indican que existen aspectos del fenómeno que no se están capturando.\n",
    "\n",
    "Estas métricas, en conjunto con el análisis de residuos, proporcionan una visión integral sobre la capacidad del modelo para generalizar a datos no vistos.\n",
    "\n",
    "### 6. Técnicas adicionales y consideraciones avanzadas en scikit-learn\n",
    "\n",
    "Además de los elementos mencionados, existen otras técnicas y consideraciones avanzadas que pueden ser útiles en un contexto de modelado y análisis:\n",
    "\n",
    "#### 6.1 Selección de características\n",
    "\n",
    "- **Eliminación recursiva de características (RFE):**  \n",
    "  Es una técnica que permite seleccionar las variables más relevantes para el modelo. Se realiza de forma iterativa eliminando las variables menos importantes, lo que ayuda a reducir la dimensionalidad y a mejorar la interpretabilidad del modelo.\n",
    "\n",
    "- **Importancia de variables:**  \n",
    "  Muchos modelos de regresión y árboles de decisión permiten evaluar la importancia de cada variable, lo que puede guiar la selección o creación de nuevas características.\n",
    "\n",
    "#### 6.2 Validación y particionado de datos\n",
    "\n",
    "- **Train-test split:**  \n",
    "  La división inicial de los datos en conjunto de entrenamiento y prueba (como se hace en el ejemplo) es fundamental para evaluar el rendimiento real del modelo en datos no vistos. Además, se pueden utilizar métodos más sofisticados como la validación anidada para ajustar hiperparámetros y estimar el error de generalización.\n",
    "\n",
    "- **Cross-validation estratificada:**  \n",
    "  En problemas de clasificación, se suele utilizar la validación cruzada estratificada para asegurar que cada partición mantenga la proporción de clases. Aunque en este ejemplo se trata de un problema de regresión, es importante considerar la técnica en otros contextos.\n",
    "\n",
    "#### 6.3 Pipelines anidados y personalización\n",
    "\n",
    "Los pipelines pueden anidarse y personalizarse para adaptarse a flujos de trabajo complejos:\n",
    "- **Pipelines anidados:**  \n",
    "  Se pueden construir pipelines que incluyan otros pipelines como pasos individuales, lo que permite estructurar el preprocesamiento de manera modular y reutilizable.\n",
    "- **Integración con modelos externos:**  \n",
    "  Es posible integrar transformaciones que provengan de librerías externas a scikit-learn, siempre y cuando se implementen los métodos `fit` y `transform` necesarios.\n",
    "\n",
    "#### 6.4 Optimización computacional y paralelización\n",
    "\n",
    "En problemas con conjuntos de datos grandes o cuando se realiza una búsqueda extensa de hiperparámetros, es fundamental considerar:\n",
    "- **Uso de `n_jobs` en GridSearchCV:**  \n",
    "  La opción `n_jobs=-1` permite utilizar todos los núcleos disponibles en la máquina, acelerando la búsqueda.\n",
    "- **Pipelines parciales:**  \n",
    "  En escenarios donde se requieren modificaciones dinámicas, se pueden utilizar pipelines parciales para reajustar únicamente ciertos pasos sin necesidad de recomputar el preprocesamiento completo.\n",
    "\n",
    "#### 6.5 Interpretabilidad del modelo\n",
    "\n",
    "Aunque los modelos lineales son intrínsecamente interpretables, la inclusión de transformaciones y la expansión polinómica pueden dificultar la interpretación directa de los coeficientes. Algunas técnicas para mejorar la interpretabilidad incluyen:\n",
    "- **Visualización de coeficientes:**  \n",
    "  Graficar la magnitud y dirección de los coeficientes para identificar cuáles son las variables o interacciones más relevantes.\n",
    "- **Análisis de sensibilidad:**  \n",
    "  Evaluar cómo cambios en las variables predictoras afectan las predicciones del modelo, lo que puede ser particularmente útil cuando se trabaja con características transformadas o generadas.\n",
    "\n",
    "#### 6.6 Estrategias de manejo de outliers\n",
    "\n",
    "Además de utilizar transformadores como el `RobustScaler`, existen otras estrategias para el manejo de outliers:\n",
    "- **Winsorización:**  \n",
    "  Limitar los valores extremos a percentiles específicos.\n",
    "- **Detección y eliminación:**  \n",
    "  Utilizar técnicas estadísticas o de machine learning para detectar y, en su caso, eliminar o ajustar los outliers antes del modelado.\n",
    "\n",
    "#### 6.7 Escalabilidad y producción\n",
    "\n",
    "Cuando se trabaja con pipelines en un entorno de producción, es importante:\n",
    "- **Persistencia de modelos:**  \n",
    "  Utilizar herramientas como `joblib` o `pickle` para guardar y cargar pipelines completos, asegurando que el preprocesamiento y el modelo se apliquen de forma consistente.\n",
    "- **Integración con API o servicios web:**  \n",
    "  Los pipelines permiten encapsular el flujo de trabajo en un solo objeto que se puede desplegar fácilmente, facilitando la integración con aplicaciones web o sistemas de recomendación.\n",
    "\n",
    "\n",
    "El ejemplo presentado es una buena representación de cómo se pueden integrar diversas técnicas de preprocesamiento, transformación y modelado en un único flujo de trabajo mediante pipelines en scikit-learn. No obstante, en proyectos reales se pueden considerar otros aspectos o técnicas adicionales, tales como:\n",
    "\n",
    "- **Incorporación de variables temporales:**  \n",
    "  En problemas de series de tiempo, se pueden incluir transformaciones específicas para capturar la estacionalidad o tendencias a largo plazo.\n",
    "\n",
    "- **Uso de técnicas de reducción de dimensionalidad:**  \n",
    "  Herramientas como PCA (Análisis de Componentes Principales) pueden incluirse en el pipeline para reducir la complejidad del modelo, especialmente cuando se incrementa la dimensionalidad mediante la expansión polinómica.\n",
    "\n",
    "- **Ensamblado de modelos (Ensemble):**  \n",
    "  La combinación de varios modelos (por ejemplo, a través de métodos de bagging o boosting) puede integrarse en pipelines para mejorar la robustez y la capacidad predictiva.\n",
    "\n",
    "- **Automatización de la búsqueda de modelos:**  \n",
    "  Herramientas como `RandomizedSearchCV` o técnicas de optimización bayesiana permiten explorar de forma más eficiente el espacio de hiperparámetros en casos donde la búsqueda exhaustiva resulte computacionalmente costosa.\n",
    "\n",
    "- **Validación de supuestos:**  \n",
    "  Técnicas adicionales como la verificación de la normalidad de los residuos, la homocedasticidad o la ausencia de multicolinealidad son aspectos avanzados que se pueden incluir en un análisis completo para asegurar la validez del modelo.\n",
    "\n",
    "Cada uno de estos elementos puede integrarse en el flujo de trabajo mediante pipelines anidados o mediante la construcción de nuevas clases transformadoras que extiendan la funcionalidad de scikit-learn, lo que demuestra la flexibilidad y potencia que ofrece esta librería.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a45dfd-6af0-4252-897d-61cdc44df896",
   "metadata": {},
   "source": [
    "## 2. Pipeline para clasificación con KNN y SVM en datos desbalanceados\n",
    "\n",
    "####  Contexto de la Clasificación en Datos Desbalanceados\n",
    "\n",
    "Cuando se trabaja en problemas de clasificación, a menudo se presenta el problema del **desbalance de clases**. Esto significa que una de las clases (la minoritaria) se encuentra muy subrepresentada en comparación con la otra (la mayoritaria). Este desbalance puede llevar a que modelos aparentemente con alta exactitud (accuracy) resulten engañosos, ya que el modelo puede predecir siempre la clase mayoritaria y aún así obtener una alta tasa de aciertos.\n",
    "\n",
    "#### Métricas acordes al desbalance\n",
    "\n",
    "- **F1-score:** Es la media armónica entre la precisión (precision) y la exhaustividad (recall). Es una métrica robusta en situaciones de desbalance porque toma en cuenta tanto los falsos positivos como los falsos negativos.\n",
    "- **AUC-ROC (Área Bajo la Curva ROC):** Evalúa la capacidad del modelo para distinguir entre las clases a lo largo de distintos umbrales. La curva ROC (Receiver Operating Characteristic) representa la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR).\n",
    "\n",
    "Estas métricas permiten obtener una visión más realista del desempeño del modelo en comparación con la simple exactitud.\n",
    "\n",
    "\n",
    "### 3. Técnicas para el manejo del desbalance de clases\n",
    "\n",
    "#### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "**SMOTE** es una técnica de sobremuestreo que genera nuevas instancias sintéticas para la clase minoritaria. En lugar de replicar aleatoriamente las muestras existentes, SMOTE crea ejemplos intermedios mediante la interpolación entre muestras cercanas. Esto ayuda a:\n",
    "- Reducir el riesgo de sobreajuste, ya que las nuevas muestras no son simples duplicados.\n",
    "- Mejorar la representatividad de la clase minoritaria, permitiendo que el modelo tenga más ejemplos sobre los cuales aprender.\n",
    "\n",
    "En el código se define una función `balance_data` que utiliza SMOTE para transformar el conjunto de datos de forma que las clases queden balanceadas.\n",
    "\n",
    "\n",
    "### 4. Modelos de clasificación: KNN y SVM\n",
    "\n",
    "#### K-Nearest Neighbors (KNN)\n",
    "\n",
    "El **KNN** es un algoritmo de clasificación basado en instancias. No tiene una fase de entrenamiento explícita; en su lugar, para clasificar una nueva instancia, el algoritmo busca los $k$ vecinos más cercanos (según alguna métrica de distancia, generalmente la Euclidiana) y decide la clase mayoritaria entre estos vecinos. Aspectos importantes en KNN:\n",
    "- **Parámetro n_neighbors:** Controla cuántos vecinos se consideran. Valores pequeños pueden llevar a alta varianza y sobreajuste, mientras que valores grandes pueden suavizar demasiado la frontera de decisión.\n",
    "- **Pesos:** El parámetro `weights` puede tomar valores como `uniform` (todos los vecinos tienen el mismo peso) o `distance` (los vecinos cercanos tienen mayor peso), lo cual afecta la toma de decisiones.\n",
    "\n",
    "#### Máquinas de Vectores de Soporte (SVM)\n",
    "\n",
    "Las **SVM** son algoritmos de clasificación que buscan encontrar el hiperplano que maximice el margen entre las clases. En escenarios no lineales, se utilizan versiones kernelizadas, como la función RBF (Radial Basis Function), que permiten proyectar los datos a espacios de mayor dimensión para encontrar una separación óptima. Aspectos clave en SVM:\n",
    "- **Hiperparámetro C:** Controla la penalización por errores de clasificación. Un valor alto de $C$ intenta clasificar todos los puntos correctamente, lo que puede llevar a sobreajuste, mientras que un valor bajo permite errores en pos de un margen mayor y, por tanto, puede generar un modelo más generalizable.\n",
    "- **Kernel:** Permite la transformación de los datos a espacios de características de mayor dimensión. En el ejemplo se prueban kernels 'rbf' y 'linear'.\n",
    "\n",
    "\n",
    "### 5. Análisis visual: curva ROC y matriz de confusión\n",
    "\n",
    "#### Curva ROC\n",
    "\n",
    "La **Curva ROC** es una herramienta gráfica que muestra la capacidad de un modelo para distinguir entre clases, trazando la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) en diferentes umbrales de decisión. El área bajo la curva (AUC) es una medida cuantitativa de la capacidad del modelo: cuanto más se acerque a 1, mejor será la discriminación.\n",
    "\n",
    "#### Matriz de Confusión\n",
    "\n",
    "La **matriz de confusión** es una representación tabular que permite visualizar el desempeño del modelo, mostrando la cantidad de verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos. Esto ayuda a identificar de manera precisa qué tipo de errores comete el modelo y a evaluar métricas derivadas, como precisión, exhaustividad y F1-score.\n",
    "\n",
    "\n",
    "### 6. Explicación detallada del código fuente\n",
    "\n",
    "El código proporcionado se puede dividir en varias secciones que se explican a continuación:\n",
    "\n",
    "#### 6.1 Generación de datos sintéticos desbalanceados\n",
    "\n",
    "```python\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "X_class = pd.DataFrame({\n",
    "    'feat1': np.random.normal(0, 1, n_samples),\n",
    "    'feat2': np.random.normal(5, 2, n_samples)\n",
    "})\n",
    "# Clase minoritaria: 10%, mayoritaria: 90%\n",
    "y_class = np.where(np.random.rand(n_samples) < 0.1, 1, 0)\n",
    "```\n",
    "\n",
    "Aquí se generan 1000 muestras con dos características (`feat1` y `feat2`). La variable de clase `y_class` se define de manera que aproximadamente el 10% de las muestras pertenecen a la clase 1 (minoritaria) y el 90% a la clase 0 (mayoritaria). Esto simula una situación de desbalance que es común en muchos problemas reales, como la detección de fraudes o diagnósticos médicos.\n",
    "\n",
    "#### 6.2 Transformador personalizado: FeatureEngineer\n",
    "\n",
    "```python\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convertir a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=['feat1', 'feat2'])\n",
    "        \n",
    "        # Crear una nueva feature: relación entre feat1 y feat2\n",
    "        X['feat_ratio'] = X['feat1'] / (X['feat2'] + 1e-5)\n",
    "        # Crear feature de interacción\n",
    "        X['feat_interaction'] = X['feat1'] * X['feat2']\n",
    "        \n",
    "        return X\n",
    "```\n",
    "\n",
    "El transformador **FeatureEngineer** hereda de `BaseEstimator` y `TransformerMixin` para integrarse perfectamente en el ecosistema de scikit-learn. Este transformador realiza dos operaciones de ingeniería de características:\n",
    "- Calcula la razón entre `feat1` y `feat2` (añadiendo un pequeño valor para evitar división por cero).\n",
    "- Genera una nueva característica como la interacción (producto) entre `feat1` y `feat2`.\n",
    "\n",
    "Esto permite capturar relaciones adicionales entre las variables originales que podrían mejorar el desempeño del modelo.\n",
    "\n",
    "#### 6.3 Pipeline de preprocesamiento para clasificación\n",
    "\n",
    "```python\n",
    "preprocessor_cls = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('engineer', FeatureEngineer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "```\n",
    "\n",
    "Este pipeline para preprocesamiento realiza tres pasos:\n",
    "1. **Imputación:** Con `SimpleImputer` se reemplazan valores faltantes utilizando la media.\n",
    "2. **Ingeniería de características:** Se aplica el transformador `FeatureEngineer` para generar las nuevas variables.\n",
    "3. **Estandarización:** Se usa `StandardScaler` para escalar los datos y que tengan media 0 y desviación estándar 1, lo cual es fundamental para algoritmos sensibles a la escala, como SVM y KNN.\n",
    "\n",
    "#### 6.4 Balanceo de datos con SMOTE\n",
    "\n",
    "```python\n",
    "def balance_data(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "\n",
    "X_processed = preprocessor_cls.fit_transform(X_class)\n",
    "X_balanced, y_balanced = balance_data(pd.DataFrame(X_processed), y_class)\n",
    "```\n",
    "\n",
    "Una vez preprocesados los datos, se aplica la función `balance_data` que utiliza **SMOTE** para generar nuevas instancias de la clase minoritaria y equilibrar la distribución de clases. Esto es crucial en problemas desbalanceados, ya que mejora la capacidad del modelo para aprender características relevantes de la clase minoritaria.\n",
    "\n",
    "#### 6.5 División de datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "```python\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "El conjunto balanceado se divide en entrenamiento y prueba. Esto permite evaluar el desempeño del modelo en datos no vistos y asegurar que el proceso de balanceo y preprocesamiento se mantenga consistente.\n",
    "\n",
    "#### 6.6 Creación de pipelines para modelos KNN y SVM\n",
    "\n",
    "Para cada modelo se define un pipeline simple que, en este caso, únicamente encapsula al clasificador. Esto facilita la posterior búsqueda de hiperparámetros y la integración en flujos de trabajo más complejos.\n",
    "\n",
    "```python\n",
    "pipeline_knn = Pipeline([\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "pipeline_svm = Pipeline([\n",
    "    ('classifier', SVC(probability=True))\n",
    "])\n",
    "```\n",
    "\n",
    "- En el pipeline de **KNN**, se utiliza `KNeighborsClassifier`.\n",
    "- En el pipeline de **SVM**, se utiliza `SVC` con `probability=True` para poder obtener probabilidades de clasificación, lo cual es necesario para calcular la curva ROC.\n",
    "\n",
    "#### 6.7 Definición de grids de hiperparámetros y búsqueda\n",
    "\n",
    "Se definen dos diccionarios de parámetros para ajustar las configuraciones de cada modelo mediante **GridSearchCV**:\n",
    "\n",
    "```python\n",
    "param_grid_knn = {\n",
    "    'classifier__n_neighbors': [3, 5, 7],\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "param_grid_svm = {\n",
    "    'classifier__C': [0.5, 1.0, 5.0],\n",
    "    'classifier__kernel': ['rbf', 'linear']\n",
    "}\n",
    "```\n",
    "\n",
    "- Para **KNN**, se exploran distintos valores para `n_neighbors` y la forma de ponderar los vecinos (`uniform` vs. `distance`).\n",
    "- Para **SVM**, se ajusta el parámetro `C` y se prueba con kernels 'rbf' y 'linear'. Como se explicó, un valor mayor de $C$ tiende a penalizar más los errores, lo que puede llevar a sobreajuste, mientras que un $C$ menor permite un margen más amplio a costa de cometer más errores.\n",
    "\n",
    "La búsqueda de hiperparámetros se realiza de la siguiente forma:\n",
    "\n",
    "```python\n",
    "grid_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=5, scoring='f1')\n",
    "grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring='f1')\n",
    "\n",
    "grid_knn.fit(X_train_cls, y_train_cls)\n",
    "grid_svm.fit(X_train_cls, y_train_cls)\n",
    "```\n",
    "\n",
    "Se utiliza validación cruzada con 5 particiones (cv=5) y la métrica F1 como criterio de optimización, lo cual es adecuado en contextos de datos desbalanceados.\n",
    "\n",
    "#### 6.8 Evaluación de los modelos en el conjunto de prueba\n",
    "\n",
    "Una vez ajustados los modelos, se evalúan sobre el conjunto de prueba utilizando varias métricas y técnicas visuales.\n",
    "\n",
    "```python\n",
    "print(\"Mejores hiperparámetros KNN:\", grid_knn.best_params_)\n",
    "print(\"Mejores hiperparámetros SVM:\", grid_svm.best_params_)\n",
    "```\n",
    "\n",
    "Se muestran los mejores parámetros encontrados para cada modelo. Luego, para cada modelo se realizan las siguientes operaciones:\n",
    "\n",
    "1. **Predicción y cálculo de probabilidades:**\n",
    "\n",
    "   ```python\n",
    "   y_pred = model.predict(X_test_cls)\n",
    "   y_proba = model.predict_proba(X_test_cls)[:,1]\n",
    "   ```\n",
    "\n",
    "   Se generan las predicciones y se extraen las probabilidades de la clase positiva, necesarias para calcular el AUC-ROC.\n",
    "\n",
    "2. **Cálculo de F1-score y AUC:**\n",
    "\n",
    "   ```python\n",
    "   f1 = f1_score(y_test_cls, y_pred)\n",
    "   auc_val = roc_auc_score(y_test_cls, y_proba)\n",
    "   print(f\"{name} - F1-score: {f1:.3f}, AUC: {auc_val:.3f}\")\n",
    "   ```\n",
    "\n",
    "   Se calcula el F1-score y el AUC, métricas críticas en escenarios de desbalance.\n",
    "\n",
    "3. **Matriz de Confusión:**\n",
    "\n",
    "   ```python\n",
    "   cm = confusion_matrix(y_test_cls, y_pred)\n",
    "   sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "   plt.title(f\"Matriz de confusión - {name}\")\n",
    "   plt.xlabel(\"Predicción\")\n",
    "   plt.ylabel(\"Real\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   La matriz de confusión permite identificar cuántos verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos se producen, ofreciendo una visión detallada de la capacidad del modelo para discriminar entre clases.\n",
    "\n",
    "4. **Curva ROC:**\n",
    "\n",
    "   ```python\n",
    "   fpr, tpr, thresholds = roc_curve(y_test_cls, y_proba)\n",
    "   roc_auc = auc(fpr, tpr)\n",
    "   plt.figure()\n",
    "   plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "   plt.plot([0, 1], [0, 1], 'k--')\n",
    "   plt.xlabel('Tasa Falso Positivo')\n",
    "   plt.ylabel('Tasa Verdadero Positivo')\n",
    "   plt.title(f'Curva ROC - {name}')\n",
    "   plt.legend(loc=\"lower right\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   Se traza la curva ROC comparando la tasa de verdaderos positivos con la tasa de falsos positivos en distintos umbrales, y se calcula el AUC para cuantificar la capacidad discriminante del modelo.\n",
    "\n",
    "\n",
    "### 7. Técnicas y consideraciones adicionales\n",
    "\n",
    "A continuación se detallan técnicas adicionales y aspectos avanzados relacionados con el procesamiento de datos desbalanceados y la construcción de pipelines en scikit-learn:\n",
    "\n",
    "#### 7.1 Selección y extracción de características\n",
    "\n",
    "- **Ingeniería de características:**  \n",
    "  El transformador `FeatureEngineer` en el código es un ejemplo de cómo se pueden crear nuevas variables a partir de las originales para mejorar la capacidad predictiva. La relación y la interacción entre características pueden capturar patrones no evidentes.\n",
    "  \n",
    "- **Selección de características:**  \n",
    "  Técnicas como Recursive Feature Elimination (RFE) o métodos basados en importancia de variables pueden ser incorporados en el pipeline para eliminar variables redundantes o irrelevantes, reduciendo la dimensionalidad y mejorando la interpretabilidad del modelo.\n",
    "\n",
    "#### 7.2 Otras técnicas de balanceo\n",
    "\n",
    "Además de SMOTE, existen otros métodos para tratar el desbalance de clases:\n",
    "- **Over-sampling aleatorio:**  \n",
    "  Simplemente replica instancias de la clase minoritaria. Es sencillo, pero puede llevar a sobreajuste.\n",
    "- **Under-sampling:**  \n",
    "  Reduce la cantidad de muestras de la clase mayoritaria. Si bien equilibra la distribución, puede descartar información valiosa.\n",
    "- **Variantes de SMOTE:**  \n",
    "  Existen variantes como Borderline-SMOTE o ADASYN, que modifican el proceso de generación de instancias sintéticas para enfocarse en zonas críticas del espacio de características.\n",
    "\n",
    "#### 7.3 Optimización de hiperparámetros y validación cruzada\n",
    "\n",
    "- **GridSearchCV:**  \n",
    "  Permite explorar combinaciones de hiperparámetros de manera exhaustiva, como se mostró en el código. Es posible también utilizar **RandomizedSearchCV** para explorar de manera más eficiente cuando el espacio de parámetros es muy amplio.\n",
    "  \n",
    "- **Validación cruzada estratificada:**  \n",
    "  En problemas de clasificación, especialmente cuando hay desbalance, es recomendable utilizar validación cruzada estratificada para asegurar que cada fold mantenga la proporción de clases, lo que mejora la fiabilidad de la evaluación.\n",
    "\n",
    "#### 7.4 Modelos ensemble y técnicas de combinación\n",
    "\n",
    "- **Ensambles:**  \n",
    "  Técnicas como bagging, boosting o stacking pueden combinar varios modelos para mejorar la robustez y el rendimiento predictivo. Aunque en este ejemplo se utilizan KNN y SVM de manera individual, en escenarios reales se puede considerar la construcción de un modelo ensemble que integre las predicciones de distintos clasificadores.\n",
    "\n",
    "#### 7.5 Interpretabilidad y visualización de resultados\n",
    "\n",
    "- **Matriz de confusión:**  \n",
    "  Además de la visualización mediante heatmaps, es útil analizar en detalle los falsos positivos y falsos negativos para identificar posibles causas de error y ajustar el modelo o el preprocesamiento.\n",
    "  \n",
    "- **Curvas ROC y AUC:**  \n",
    "  La interpretación de la curva ROC junto con el valor del AUC ofrece una visión global de la capacidad discriminante del modelo. Se pueden superponer curvas de distintos modelos para comparar su desempeño de manera visual.\n",
    "\n",
    "- **Reporte de clasificación:**  \n",
    "  Herramientas como `classification_report` de scikit-learn permiten obtener un resumen detallado de precisión, exhaustividad y F1-score para cada clase, lo que es especialmente útil en escenarios de desbalance.\n",
    "\n",
    "#### 7.6 Consideraciones en la implementación de pipelines\n",
    "\n",
    "- **Persistencia de modelos:**  \n",
    "  Una vez entrenado el pipeline, se pueden guardar los modelos utilizando `joblib` o `pickle` para su uso posterior sin necesidad de repetir el preprocesamiento.\n",
    "  \n",
    "- **Modularidad y escalabilidad:**  \n",
    "  El uso de pipelines facilita la incorporación de nuevos pasos de transformación o el reemplazo de modelos sin alterar significativamente la estructura global del código.\n",
    "\n",
    "- **Integración en producción:**  \n",
    "  Al encapsular todo el flujo de preprocesamiento y clasificación en un pipeline, se simplifica la implementación de soluciones en entornos productivos, permitiendo la integración con API o sistemas de scoring en tiempo real.\n",
    "\n",
    "#### 7.7 Evaluación y ajuste de modelos en contextos reales\n",
    "\n",
    "En aplicaciones reales, además de ajustar hiperparámetros y evaluar métricas, es importante considerar:\n",
    "- **Análisis de sensibilidad:**  \n",
    "  Estudiar cómo la variación de ciertas características afecta la predicción, lo que puede ayudar a identificar variables críticas y mejorar la interpretación del modelo.\n",
    "- **Análisis de errores:**  \n",
    "  Revisar los casos en que el modelo falla (especialmente los falsos negativos en problemas críticos) para ajustar el umbral de decisión o considerar estrategias de re-muestreo.\n",
    "- **Ajuste de umbrales:**  \n",
    "  En problemas de desbalance, modificar el umbral de decisión (por defecto 0.5) puede mejorar la detección de la clase minoritaria, y se puede analizar mediante curvas de precisión-recall.\n",
    "\n",
    "### 8. Técnicas complementarias y avanzadas en el contexto de clasificación\n",
    "\n",
    "Para enriquecer aún más el proceso de modelado en escenarios de clasificación con datos desbalanceados, se pueden considerar otras estrategias y técnicas adicionales:\n",
    "\n",
    "#### 8.1 Técnicas de preprocesamiento avanzado\n",
    "\n",
    "- **Estandarización vs. Normalización:**  \n",
    "  Además de `StandardScaler`, se pueden utilizar técnicas de normalización (como MinMaxScaler) dependiendo de la distribución de los datos y el algoritmo a utilizar.\n",
    "- **Transformaciones no lineales:**  \n",
    "  En ocasiones, transformar los datos mediante funciones logarítmicas o de potencia puede ayudar a estabilizar la varianza y mejorar la capacidad del modelo para capturar patrones complejos.\n",
    "\n",
    "#### 8.2 Ajuste fino de modelos\n",
    "\n",
    "- **Ajuste del umbral de decisión:**  \n",
    "  Tras obtener probabilidades de clase, se puede optimizar el umbral de clasificación para maximizar métricas específicas como F1-score o recall, especialmente en aplicaciones donde los falsos negativos resultan críticos.\n",
    "- **Modelos híbridos:**  \n",
    "  La combinación de distintos algoritmos (por ejemplo, mediante un meta-clasificador que integre KNN y SVM) puede ofrecer mejoras en términos de robustez y capacidad de generalización.\n",
    "\n",
    "#### 8.3 Estrategias de validación y evaluación\n",
    "\n",
    "- **Validación estratificada:**  \n",
    "  Es crucial que las particiones de validación mantengan la proporción de clases para asegurar una evaluación representativa en cada fold.\n",
    "- **Reportes detallados:**  \n",
    "  Utilizar herramientas como `classification_report` puede proporcionar una visión más granular del desempeño de cada clase, permitiendo identificar áreas de mejora específicas.\n",
    "\n",
    "#### 8.4 Integración y despliegue en producción\n",
    "\n",
    "- **Persistencia del pipeline:**  \n",
    "  Guardar el pipeline completo (preprocesamiento + modelo) permite aplicar el mismo flujo de transformación en nuevos datos sin inconsistencias.\n",
    "- **Monitorización en producción:**  \n",
    "  Una vez desplegado, es importante monitorizar las métricas de desempeño y la distribución de clases en tiempo real, para detectar cambios que puedan requerir la reentrenamiento o ajuste del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506cfbe-3626-46ee-918c-9cc23cf8c4f8",
   "metadata": {},
   "source": [
    "## 3.  Pipeline con árboles de decisión y ensamble\n",
    "\n",
    "### 1. Contexto general\n",
    "\n",
    "El objetivo principal del código es construir un pipeline de preprocesamiento y modelado para un problema de clasificación utilizando modelos basados en árboles. Para ello, se ha generado un conjunto de datos sintético que simula características numéricas (por ejemplo, edad, ingreso y una puntuación) y se define la variable objetivo en función de la puntuación (por ejemplo, asignando la clase 1 si la puntuación supera cierto umbral).  \n",
    "El pipeline incorpora transformaciones para limpiar y mejorar la calidad de los datos, tales como la imputación, el escalado y dos transformaciones personalizadas: una para la selección de características mediante correlación y otra para el manejo de outliers mediante Winsorización. Posteriormente, se definen tres modelos de árboles:  \n",
    "\n",
    "- **Árbol de Decisión**  \n",
    "- **Random Forest**  \n",
    "- **Gradient Boosting**\n",
    "\n",
    "Para cada modelo se define un pipeline completo que incluye el preprocesamiento seguido por el clasificador. Además, se establecen grids de hiperparámetros para realizar una búsqueda optimizada a través de GridSearchCV, que se evalúa utilizando la métrica F1, una métrica adecuada en problemas de clasificación. En la parte final, se extrae y visualiza la importancia de las características cuando el modelo lo permite (por ejemplo, en Random Forest y Gradient Boosting).\n",
    "\n",
    "\n",
    "### 2. Generación del dataset sintético\n",
    "\n",
    "El código inicia con la generación de un dataset sintético para clasificación:\n",
    "\n",
    "```python\n",
    "np.random.seed(2021)\n",
    "n_samples = 800\n",
    "X_dt = pd.DataFrame({\n",
    "    'age': np.random.randint(20, 70, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'score': np.random.uniform(300, 850, n_samples)\n",
    "})\n",
    "y_dt = np.where(X_dt['score'] > 600, 1, 0)\n",
    "```\n",
    "\n",
    "Aquí se crean 800 muestras con tres características:\n",
    "- **age (edad):** Se genera mediante números enteros aleatorios entre 20 y 70.  \n",
    "- **income (ingreso):** Se crea a partir de una distribución normal con media 50000 y desviación estándar 15000.  \n",
    "- **score (puntuación):** Se asigna utilizando una distribución uniforme entre 300 y 850.  \n",
    "\n",
    "La variable objetivo `y_dt` se define asignando la clase 1 a aquellas muestras cuyo score supere el valor de 600 y la clase 0 en caso contrario. Esta definición ilustra un problema de clasificación binaria en el que el criterio de decisión es relativamente sencillo, pero en escenarios reales la separación puede depender de múltiples factores.\n",
    "\n",
    "\n",
    "### 3. Transformador personalizado para selección de características basada en correlación\n",
    "\n",
    "#### Conceptos\n",
    "\n",
    "La **selección de características basada en correlación** es una técnica utilizada para reducir la dimensionalidad de los datos, eliminando aquellas variables que no aportan información relevante. En este caso, se utiliza un transformador personalizado llamado `CorrelationSelector` que evalúa la correlación entre las variables y elimina aquellas que no superan un umbral medio.  \n",
    "La idea es que, si una variable presenta baja correlación (en términos absolutos) con las demás, puede ser considerada poco informativa o ruidosa. Al eliminar estas variables se reduce la complejidad del modelo y se mejora la capacidad de generalización, además de mitigar problemas de multicolinealidad.\n",
    "\n",
    "#### Implementación del transformador\n",
    "\n",
    "```python\n",
    "class CorrelationSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.1):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convertir X a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "        # Calcular la matriz de correlación absoluta entre las variables\n",
    "        corr = np.abs(np.corrcoef(X.T))\n",
    "        # Se calcula la media de las correlaciones para cada característica\n",
    "        self.features_ = X.columns[(corr.mean(axis=0) > self.threshold)].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convertir X a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.features_)\n",
    "        # Seleccionar únicamente las características que superaron el umbral\n",
    "        return X[self.features_]\n",
    "```\n",
    "\n",
    "#### Detalles clave:\n",
    "- **Constructor (`__init__`):**  \n",
    "  Se define un parámetro `threshold` que permite especificar el umbral mínimo de correlación (por ejemplo, 0.1 o 0.2) que una variable debe tener en promedio con las demás para ser considerada relevante.\n",
    "\n",
    "- **Método `fit`:**  \n",
    "  Aquí se calcula la matriz de correlación absoluta entre las variables. Se utiliza `np.corrcoef(X.T)` para obtener la correlación entre columnas (ya que se transpone el DataFrame). A continuación, se calcula la media de cada columna en la matriz de correlación y se retienen aquellas variables cuya media supere el umbral definido.\n",
    "\n",
    "- **Método `transform`:**  \n",
    "  Se utiliza para seleccionar y devolver únicamente las variables que se han considerado relevantes según la fase de ajuste.\n",
    "\n",
    "Esta técnica resulta en un filtro simple y efectivo para eliminar variables ruidosas o poco correlacionadas, reduciendo la dimensionalidad y el riesgo de sobreajuste.\n",
    "\n",
    "\n",
    "### 4. Transformador para Winsorización de outliers\n",
    "\n",
    "#### Conceptos\n",
    "\n",
    "El **manejo de outliers** es fundamental en el preprocesamiento de datos. La **Winsorización** es una técnica que, en lugar de eliminar las muestras con valores extremos, recorta dichos valores a límites predefinidos basados en percentiles (por ejemplo, al 5% y al 95%). Esto permite conservar todas las observaciones, pero reduce la influencia de valores atípicos en el entrenamiento del modelo.  \n",
    "Los modelos basados en árboles, aunque son robustos frente a outliers, pueden beneficiarse de la Winsorización para evitar que algunos nodos se vean influenciados por valores extremos y para estabilizar la distribución de las variables.\n",
    "\n",
    "#### Implementación del transformador\n",
    "\n",
    "```python\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.05, upper_quantile=0.95):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convertir X a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "        # Calcular los límites inferior y superior para cada columna\n",
    "        self.lower_bounds_ = X.quantile(self.lower_quantile)\n",
    "        self.upper_bounds_ = X.quantile(self.upper_quantile)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Convertir a DataFrame si es necesario\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.lower_bounds_.index)\n",
    "        # Aplicar la Winsorización recortando cada columna a sus límites\n",
    "        for col in X.columns:\n",
    "            X[col] = np.clip(X[col], self.lower_bounds_[col], self.upper_bounds_[col])\n",
    "        return X\n",
    "```\n",
    "\n",
    "#### Detalles clave:\n",
    "- **Parámetros:**  \n",
    "  Los parámetros `lower_quantile` y `upper_quantile` definen los percentiles a utilizar para el recorte. En este caso, se usan 0.05 y 0.95, lo que significa que los valores menores al 5% se elevarán al valor correspondiente al percentil 5, y los mayores al 95% se reducirán al valor del percentil 95.\n",
    "\n",
    "- **Método `fit`:**  \n",
    "  Calcula los límites inferior y superior para cada columna utilizando el método `quantile` de pandas.\n",
    "\n",
    "- **Método `transform`:**  \n",
    "  Utiliza la función `np.clip` para recortar los valores de cada columna dentro de los límites definidos en la fase de ajuste.\n",
    "\n",
    "La Winsorización es útil en situaciones en las que se desea mantener el conjunto completo de datos, pero se quiere atenuar el impacto de los valores extremos en el modelo.\n",
    "\n",
    "\n",
    "### 5. Pipeline de preprocesamiento para datos numéricos\n",
    "\n",
    "Una vez definidos los transformadores personalizados, se construye un pipeline de preprocesamiento específico para datos numéricos:\n",
    "\n",
    "```python\n",
    "numeric_pipeline_dt = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('winsorizer', Winsorizer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', CorrelationSelector(threshold=0.2))\n",
    "])\n",
    "```\n",
    "\n",
    "#### Explicación de cada paso:\n",
    "1. **Imputación:**  \n",
    "   Se utiliza `SimpleImputer` con la estrategia de la media para reemplazar valores faltantes. Esto es fundamental para asegurar que no existan valores nulos antes de aplicar transformaciones posteriores.\n",
    "\n",
    "2. **Winsorización:**  \n",
    "   Se aplica el transformador `Winsorizer` para recortar los outliers en cada variable, estabilizando la distribución.\n",
    "\n",
    "3. **Estandarización:**  \n",
    "   Con `StandardScaler`, se transforman las variables para que tengan media 0 y desviación estándar 1, lo que ayuda a normalizar la escala de los datos y es especialmente útil para métodos basados en distancias o que requieran datos centrados.\n",
    "\n",
    "4. **Selección de características basada en correlación:**  \n",
    "   Finalmente, se aplica `CorrelationSelector` con un umbral de 0.2 para filtrar aquellas variables que no cumplen con el criterio de correlación mínima en promedio. Esto reduce la dimensionalidad y elimina posibles variables ruidosas.\n",
    "\n",
    "Este pipeline prepara los datos numéricos para que sean introducidos en el modelo de clasificación, garantizando que se apliquen todas las transformaciones de forma secuencial y consistente.\n",
    "\n",
    "### 6. Modelos de árboles y ensamble\n",
    "\n",
    "El siguiente bloque de código define tres modelos basados en árboles que se utilizarán en el pipeline:\n",
    "\n",
    "```python\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.1 Árbol de decisión\n",
    "\n",
    "- **Concepto:**  \n",
    "  Un árbol de decisión es un modelo de clasificación (o regresión) que divide el espacio de características en regiones homogéneas utilizando reglas de decisión simples.  \n",
    "- **Ventajas y desventajas:**  \n",
    "  - **Ventaja:** Alta interpretabilidad, ya que se puede visualizar la estructura del árbol y entender las decisiones tomadas en cada nodo.  \n",
    "  - **Desventaja:** Tendencia al sobreajuste, especialmente si el árbol crece demasiado profundo.  \n",
    "- **Hiperparámetros importantes:**  \n",
    "  - `max_depth`: Profundidad máxima del árbol.  \n",
    "  - `min_samples_split`: Número mínimo de muestras requeridas para dividir un nodo.\n",
    "\n",
    "#### 6.2 Random Forest\n",
    "\n",
    "- **Concepto:**  \n",
    "  Random Forest es un método de ensamble que construye múltiples árboles de decisión utilizando muestras bootstrap del conjunto de datos y, en cada división, se selecciona un subconjunto aleatorio de características.  \n",
    "- **Ventajas y desventajas:**  \n",
    "  - **Ventaja:** Reducción significativa de la varianza gracias al ensamble, lo que mejora la generalización del modelo.  \n",
    "  - **Desventaja:** Menor interpretabilidad en comparación con un único árbol, ya que la decisión se toma a partir de la agregación de múltiples árboles.  \n",
    "- **Hiperparámetros importantes:**  \n",
    "  - `n_estimators`: Número de árboles en el ensamble.  \n",
    "  - `max_depth`: Profundidad máxima permitida para cada árbol (puede ser None para dejar que crezcan completamente).\n",
    "\n",
    "#### 6.3 Gradient Boosting\n",
    "\n",
    "- **Concepto:**  \n",
    "  Gradient Boosting es una técnica que entrena secuencialmente árboles de decisión “débiles” (poco profundos) donde cada nuevo árbol corrige los errores residuales del conjunto de árboles anteriores.  \n",
    "- **Ventajas y desventajas:**  \n",
    "  - **Ventaja:** Generalmente produce modelos muy precisos y es menos propenso a sobreajuste si se ajustan adecuadamente los hiperparámetros.  \n",
    "  - **Desventaja:** Requiere un ajuste cuidadoso del `learning_rate` y el número de estimadores para evitar el sobreajuste.\n",
    "- **Hiperparámetros importantes:**  \n",
    "  - `n_estimators`: Número de árboles a entrenar.  \n",
    "  - `learning_rate`: Tasa de aprendizaje que controla cuánto contribuye cada árbol al modelo final.\n",
    "\n",
    "\n",
    "### 7. Creación del pipeline completo para cada modelo\n",
    "\n",
    "Para cada uno de los modelos definidos se crea un pipeline que integra el preprocesamiento (numeric_pipeline_dt) y el clasificador:\n",
    "\n",
    "```python\n",
    "pipelines = {}\n",
    "for name, model in models.items():\n",
    "    pipelines[name] = Pipeline([\n",
    "        ('preprocessor', numeric_pipeline_dt),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "```\n",
    "\n",
    "Este enfoque modular permite definir y evaluar distintos modelos sobre el mismo conjunto de datos preprocesados, facilitando la comparación entre un árbol de decisión simple, un ensamble de árboles (Random Forest) y un modelo basado en boosting (Gradient Boosting).\n",
    "\n",
    "\n",
    "### 8. Búsqueda de hiperparámetros con GridSearchCV\n",
    "\n",
    "Cada modelo se ajusta mediante búsqueda en cuadrícula (GridSearchCV) utilizando un grid de hiperparámetros específico para cada clasificador. Se definen tres diccionarios de hiperparámetros:\n",
    "\n",
    "#### 8.1 Para árbol de decisión\n",
    "\n",
    "```python\n",
    "param_grid_dt = {\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "```\n",
    "\n",
    "Se exploran distintos valores para la profundidad máxima y el número mínimo de muestras para dividir un nodo. Estos parámetros controlan la complejidad del árbol y ayudan a prevenir el sobreajuste.\n",
    "\n",
    "#### 8.2 Para Random Forest\n",
    "\n",
    "```python\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 5, 10]\n",
    "}\n",
    "```\n",
    "\n",
    "Se evalúa el número de árboles y se permite probar sin restricción de profundidad (None) o con restricciones, para ver cuál configuración logra mejores resultados en términos de F1-score.\n",
    "\n",
    "#### 8.3 Para Gradient Boosting\n",
    "\n",
    "```python\n",
    "param_grid_gb = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "```\n",
    "\n",
    "Aquí se varían el número de estimadores y la tasa de aprendizaje, parámetros críticos en la convergencia y precisión del modelo de boosting.\n",
    "\n",
    "Posteriormente, se definen los objetos GridSearchCV para cada modelo:\n",
    "\n",
    "```python\n",
    "grids = {\n",
    "    'DecisionTree': GridSearchCV(pipelines['DecisionTree'], param_grid_dt, cv=5, scoring='f1'),\n",
    "    'RandomForest': GridSearchCV(pipelines['RandomForest'], param_grid_rf, cv=5, scoring='f1'),\n",
    "    'GradientBoosting': GridSearchCV(pipelines['GradientBoosting'], param_grid_gb, cv=5, scoring='f1')\n",
    "}\n",
    "```\n",
    "\n",
    "Cada grid se configura con validación cruzada de 5 particiones y se utiliza el F1-score como métrica de evaluación, lo cual es especialmente adecuado en problemas de clasificación donde se desea equilibrar la precisión y la exhaustividad.\n",
    "\n",
    "### 9. Entrenamiento, evaluación y visualización de resultados\n",
    "\n",
    "El ciclo final del código recorre cada modelo, entrena la búsqueda de hiperparámetros y, tras el ajuste, imprime el mejor score junto con los hiperparámetros óptimos encontrados:\n",
    "\n",
    "```python\n",
    "for name, grid in grids.items():\n",
    "    grid.fit(X_dt, y_dt)\n",
    "    print(f\"{name} - Mejor Score: {grid.best_score_:.3f} | Mejores Hiperparámetros: {grid.best_params_}\")\n",
    "```\n",
    "\n",
    "Para los modelos que poseen el atributo `feature_importances_` (típicamente Random Forest y Gradient Boosting), se extrae la importancia de las variables. La importancia de características en modelos basados en árboles se calcula a partir de la reducción en la impureza (o en la función de pérdida) que se produce al dividir en cada nodo. Es una métrica útil para identificar cuáles son los predictores más relevantes para la toma de decisiones del modelo, aunque se debe tener en cuenta que puede favorecer variables con muchos valores únicos o aquellas correlacionadas entre sí.\n",
    "\n",
    "El siguiente fragmento de código se encarga de visualizar la importancia de las variables:\n",
    "\n",
    "```python\n",
    "    # Obtener la importancia de las variables, si aplica\n",
    "    best_model = grid.best_estimator_.named_steps['classifier']\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_names = grid.best_estimator_.named_steps['preprocessor'].named_steps['selector'].features_\n",
    "        importances = best_model.feature_importances_\n",
    "        df_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        df_imp = df_imp.sort_values(by='importance', ascending=False)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(x='importance', y='feature', data=df_imp)\n",
    "        plt.title(f\"Importancia de Variables - {name}\")\n",
    "        plt.show()\n",
    "```\n",
    "\n",
    "#### Detalles importantes:\n",
    "- Se extrae el modelo óptimo (best_model) y se verifica si tiene el atributo `feature_importances_`.\n",
    "- Se recuperan los nombres de las características seleccionadas por el `CorrelationSelector` dentro del pipeline de preprocesamiento.\n",
    "- Se crea un DataFrame con las importancias y se ordena para visualizar de manera descendente.\n",
    "- Se utiliza `seaborn.barplot` para mostrar gráficamente la relevancia de cada variable.\n",
    "\n",
    "Esta visualización es fundamental para interpretar el modelo y tomar decisiones sobre posibles ajustes en el preprocesamiento o incluso en la selección de variables para futuras iteraciones.\n",
    "\n",
    "\n",
    "### 10. Concepto y uso de validación cruzada anidada (Nested CV)\n",
    "\n",
    "Aunque en el código se utiliza GridSearchCV de manera clásica, es importante explicar el concepto de **validación cruzada anidada (Nested CV)**, especialmente en contextos donde el espacio de hiperparámetros es extenso y se busca obtener una estimación más robusta del rendimiento del modelo.\n",
    "\n",
    "#### Concepto de nested CV\n",
    "\n",
    "La validación cruzada anidada consiste en dos bucles de validación:\n",
    "- **Loop interno:** Se utiliza para la selección y ajuste de hiperparámetros (por ejemplo, mediante GridSearchCV). En este loop, se ajustan diferentes combinaciones de parámetros y se elige la que maximiza la métrica de interés.\n",
    "- **Loop externo:** Se encarga de evaluar el desempeño del modelo ajustado en particiones de datos que no han sido utilizadas en la optimización de hiperparámetros. Esto permite estimar de manera más objetiva la capacidad de generalización del modelo, ya que cada partición de validación externa es completamente independiente de la búsqueda de parámetros.\n",
    "\n",
    "#### Ventajas del nested CV\n",
    "\n",
    "- **Prevención del sobreajuste en la selección de hiperparámetros:**  \n",
    "  Al separar la fase de optimización de hiperparámetros de la evaluación final, se evita que el proceso de búsqueda ajuste excesivamente a una partición concreta del conjunto de datos.\n",
    "- **Estimación más realista del rendimiento:**  \n",
    "  Se obtiene una estimación menos optimista y más realista de la capacidad predictiva del modelo en datos nuevos, ya que el conjunto de prueba externo no ha sido utilizado en el ajuste.\n",
    "\n",
    "#### Implementación en escenarios reales\n",
    "\n",
    "En escenarios reales, se puede implementar Nested CV utilizando, por ejemplo, la función `cross_val_score` o configurando manualmente los bucles internos y externos. Aunque el código presentado no implementa Nested CV, la idea es aplicable para la validación final cuando se requiere una evaluación robusta, sobre todo en competiciones o en aplicaciones donde el sobreajuste puede tener consecuencias importantes.\n",
    "\n",
    "### 11. Técnicas y estrategias adicionales en el contexto de ensamble y árboles\n",
    "\n",
    "Además de lo explicado anteriormente, existen otras técnicas y estrategias complementarias que pueden aplicarse para mejorar el rendimiento y la interpretabilidad del modelo:\n",
    "\n",
    "#### 11.1 Selección de características adicional\n",
    "\n",
    "- **Selección basada en información mutua:**  \n",
    "  Además de la selección por correlación, se pueden utilizar métodos basados en la información mutua para evaluar la dependencia entre cada predictor y la variable objetivo.\n",
    "- **Regularización:**  \n",
    "  En algunos modelos lineales se puede incorporar regularización (como Lasso) para hacer una selección de características, pero en modelos basados en árboles esta aproximación se basa en la reducción de impureza.\n",
    "\n",
    "#### 11.2 Manejo avanzado de outliers\n",
    "\n",
    "- **Transformaciones logarítmicas o de potencia:**  \n",
    "  A veces, transformar los datos antes de aplicar Winsorización puede estabilizar aún más la varianza.\n",
    "- **Modelos robustos:**  \n",
    "  Aunque los árboles son relativamente robustos a outliers, en algunos casos se pueden combinar técnicas de Winsorización con otros métodos de detección y tratamiento de outliers para lograr un preprocesamiento más fino.\n",
    "\n",
    "#### 11.3 Ensambles y métodos de boosting\n",
    "\n",
    "- **Stacking:**  \n",
    "  La combinación de diferentes modelos (por ejemplo, un árbol de decisión, un Random Forest y un Gradient Boosting) mediante un meta-clasificador puede mejorar la capacidad predictiva del sistema.\n",
    "- **Ajuste de parámetros de ensamble:**  \n",
    "  La optimización de parámetros como el número de árboles, la profundidad, el learning rate y otros hiperparámetros es crucial para obtener el mejor rendimiento sin sobreajustar.\n",
    "\n",
    "#### 11.4 Interpretabilidad de modelos basados en árboles\n",
    "\n",
    "- **Visualización de árboles:**  \n",
    "  Es posible exportar y visualizar un árbol de decisión para entender la lógica de clasificación.  \n",
    "- **Análisis de importancia de características:**  \n",
    "  La información obtenida de `feature_importances_` puede guiar no solo la interpretación del modelo, sino también ayudar a refinar la ingeniería de características y el preprocesamiento.\n",
    "\n",
    "#### 11.5 Implementación de nested CV en la práctica\n",
    "\n",
    "Si se quisiera implementar Nested CV en este flujo, se podría estructurar de la siguiente forma:\n",
    "- Dividir el conjunto de datos en un conjunto de validación externo (por ejemplo, mediante `KFold`).\n",
    "- Para cada partición del conjunto externo, ejecutar un GridSearchCV interno para ajustar los hiperparámetros.\n",
    "- Evaluar el modelo resultante en la partición de validación externa y promediar los resultados obtenidos en cada ciclo.\n",
    "Esta estrategia proporciona una evaluación más confiable del rendimiento del modelo, aunque a costa de un mayor coste computacional.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc083d5-b650-4a2f-9960-9df81277ba21",
   "metadata": {},
   "source": [
    "## 4. Pipeline de procesamiento de texto y clasificación con SVM\n",
    "\n",
    "El código fuente que se analiza implementa un pipeline de procesamiento de texto para clasificar reseñas de productos. El pipeline utiliza una serie de transformaciones que comienzan por la limpieza y normalización del texto, la conversión a una representación numérica mediante TF-IDF, la reducción de dimensionalidad con TruncatedSVD y finalmente, la clasificación del texto mediante un clasificador SVM lineal. Además, se incluye una parte de visualización para observar cómo se agrupan los documentos en un espacio de dos dimensiones, lo que permite una interpretación visual del comportamiento del clasificador y de la distribución de los datos.\n",
    "\n",
    "El flujo general del código es el siguiente:\n",
    "\n",
    "1. **Carga y preparación de datos:** Se crea un DataFrame con reseñas y sus etiquetas (positivo/negativo).\n",
    "2. **Definición del transformador personalizado `TextCleaner`:** Este se encarga de limpiar y normalizar el texto.\n",
    "3. **Construcción del pipeline:** Se encadenan varias etapas (limpieza, vectorización, reducción de dimensionalidad y clasificación).\n",
    "4. **División del dataset:** Se separa el conjunto de datos en entrenamiento y prueba.\n",
    "5. **Entrenamiento y evaluación:** Se entrena el pipeline completo y se obtiene un reporte de clasificación.\n",
    "6. **Visualización:** Se proyectan los textos en un espacio de dos dimensiones para visualizar la distribución de las clases.\n",
    "\n",
    "Cada una de estas etapas utiliza técnicas fundamentales en el procesamiento de texto, las cuales se explican a continuación.\n",
    "\n",
    "#### 1. Limpieza y normalización de texto\n",
    "\n",
    "#### Descripción en el código\n",
    "\n",
    "El transformador personalizado `TextCleaner` es una clase que hereda de `BaseEstimator` y `TransformerMixin` de scikit-learn, lo que le permite integrarse fácilmente en el pipeline. Dentro de su método `transform`, se realizan varias operaciones:\n",
    "\n",
    "- **Conversión a minúsculas:** Se transforma todo el texto a minúsculas para evitar discrepancias entre palabras que sean iguales salvo por diferencias en mayúsculas o minúsculas.\n",
    "- **Eliminación de caracteres especiales:** Utilizando expresiones regulares, se eliminan caracteres que no sean letras (incluyendo caracteres acentuados y la ñ) ni espacios.\n",
    "- **Tokenización:** Se divide el texto en palabras o tokens usando `nltk.word_tokenize`.\n",
    "- **Eliminación de stopwords:** Se filtran las palabras muy frecuentes (como artículos y preposiciones) que aportan poco valor semántico a la tarea.\n",
    "- **Stemming:** Se aplica un algoritmo de stemming (en este caso, el `SnowballStemmer` para español) que reduce las palabras a su raíz o forma base, ayudando a consolidar términos con significados similares.\n",
    "\n",
    "#### Conceptos relacionados\n",
    "\n",
    "**Limpieza y normalización de texto:**  \n",
    "Este proceso es crucial en NLP, ya que el texto en su forma cruda puede contener ruido (caracteres no deseados, mayúsculas, puntuación innecesaria, etc.) que afecta la calidad del análisis. La normalización busca homogeneizar el contenido para que el modelo trabaje con datos consistentes.\n",
    "\n",
    "- **Conversión a minúsculas:** Permite que \"Producto\" y \"producto\" sean tratados de manera idéntica, evitando duplicidad en la representación de términos.\n",
    "- **Eliminación de caracteres no alfabéticos:** Ayuda a eliminar símbolos, números o signos de puntuación que no contribuyen al significado semántico en muchos contextos de análisis de sentimientos o clasificación.\n",
    "- **Tokenización:** Es el proceso de dividir el texto en unidades mínimas (tokens) que serán analizadas individualmente. Una tokenización adecuada es la base para construir una representación numérica del texto.\n",
    "- **Stopwords:** Las stopwords son palabras de alta frecuencia que suelen tener poco significado por sí solas. Su eliminación reduce la dimensionalidad y mejora la calidad del modelo.\n",
    "- **Stemming:** Al reducir las palabras a su raíz, se agrupan términos derivados de una misma palabra. Por ejemplo, \"ejecutar\", \"ejecuta\" y \"ejecutado\" pueden converger a la misma raíz, lo que mejora la capacidad del modelo para identificar patrones.\n",
    "\n",
    "**Técnicas adicionales en la limpieza:**\n",
    "- **Lematización:** A diferencia del stemming, la lematización utiliza reglas lingüísticas y diccionarios para transformar las palabras a su forma canónica o lema. Esto puede preservar mejor el significado.\n",
    "- **Normalización de caracteres Unicode:** Para manejar acentos y otros caracteres especiales de forma consistente.\n",
    "- **Corrección ortográfica:** Se puede aplicar un corrector ortográfico para mejorar la calidad del texto antes de la tokenización.\n",
    "- **Eliminación de URLs y menciones:** En textos provenientes de redes sociales, es común eliminar URLs, menciones y hashtags.\n",
    "\n",
    "\n",
    "### 2. Representación TF-IDF\n",
    "\n",
    "#### Descripción en el código\n",
    "\n",
    "Después de limpiar el texto, el pipeline utiliza el transformador `TfidfVectorizer` de scikit-learn. Este componente convierte el texto en una matriz numérica en la que cada fila representa un documento y cada columna representa la importancia de un término en el documento. El parámetro `max_features=50` limita el número de términos a los 50 más importantes, lo que ayuda a controlar la dimensionalidad.\n",
    "\n",
    "### Conceptos relacionados\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency):**  \n",
    "Esta técnica es fundamental en NLP para transformar textos en vectores numéricos. Se basa en dos componentes:\n",
    "\n",
    "- **Frecuencia del término (TF):** Mide cuántas veces aparece un término en un documento.\n",
    "- **Frecuencia inversa de documentos (IDF):** Mide la importancia del término en todo el corpus. La fórmula es:\n",
    "  $$\n",
    "  \\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\log \\frac{N}{\\text{df}(t)}\n",
    "  $$\n",
    "  donde:\n",
    "  - \\( \\text{tf}(t, d) \\) es la frecuencia del término \\( t \\) en el documento \\( d \\).\n",
    "  - \\( N \\) es el número total de documentos.\n",
    "  - \\( \\text{df}(t) \\) es el número de documentos en los que aparece \\( t \\).\n",
    "\n",
    "Esta medida ayuda a atenuar el impacto de palabras muy comunes (por ejemplo, \"el\", \"la\") que, aunque frecuentes, no ofrecen información discriminativa para diferenciar entre documentos.\n",
    "\n",
    "**Técnicas adicionales en la representación:**\n",
    "- **BOW (Bag of Words):** Otra técnica básica en la que se cuenta la frecuencia de cada palabra en el documento sin tener en cuenta el orden. TF-IDF es una extensión que introduce ponderación.\n",
    "- **N-grams:** Se pueden incluir n-gramas (por ejemplo, bi-gramas o tri-gramas) en el vectorizador para capturar secuencias de palabras y contextos más ricos.\n",
    "- **Embeddings:** Técnicas como Word2Vec, GloVe o FastText transforman las palabras en vectores densos de baja dimensión que capturan relaciones semánticas, lo cual es útil para tareas de clasificación más complejas.\n",
    "\n",
    "\n",
    "### 3. Reducción de dimensionalidad con truncatedSVD\n",
    "\n",
    "#### Descripción en el código\n",
    "\n",
    "Después de la representación TF-IDF, se aplica la técnica de reducción de dimensionalidad usando `TruncatedSVD`. En este caso, se especifica `n_components=2`, lo que reduce la matriz TF-IDF a solo dos componentes principales. Esto permite visualizar la información en un espacio bidimensional.\n",
    "\n",
    "#### Conceptos relacionados\n",
    "\n",
    "**TruncatedSVD:**  \n",
    "Esta técnica es similar al análisis de componentes principales (PCA) pero está diseñada para trabajar de manera eficiente con matrices dispersas, como las generadas por TF-IDF. Al reducir la dimensionalidad:\n",
    "- Se elimina el ruido y se retienen las características más relevantes.\n",
    "- Se mejora la eficiencia computacional en modelos de alta dimensión.\n",
    "- Se facilita la interpretación y visualización de los datos.\n",
    "\n",
    "**LSA (Latent Semantic Analysis):**  \n",
    "El uso de TruncatedSVD en el contexto de NLP se asocia frecuentemente con el Análisis Semántico Latente (LSA), que busca descubrir relaciones ocultas entre términos y documentos. Esta técnica identifica patrones semánticos al agrupar términos y documentos en un espacio de menor dimensión.\n",
    "\n",
    "**Técnicas adicionales en reducción de dimensionalidad:**\n",
    "- **PCA (Principal Component Analysis):** Aunque no es ideal para datos dispersos, es ampliamente utilizado en otros contextos.\n",
    "- **t-SNE (t-distributed Stochastic Neighbor Embedding):** Es muy útil para visualizar datos de alta dimensión en dos o tres dimensiones, capturando relaciones no lineales, aunque su uso en tareas de clasificación puede ser limitado.\n",
    "- **UMAP (Uniform Manifold Approximation and Projection):** Similar a t-SNE, UMAP es utilizado para la reducción de dimensionalidad y la visualización, con la ventaja de conservar más la estructura global de los datos.\n",
    "\n",
    "\n",
    "### 4. Clasificador SVM para texto\n",
    "\n",
    "#### Descripción en el código\n",
    "\n",
    "El pipeline emplea un clasificador SVM (Support Vector Machine) con un kernel lineal. El clasificador se configura con `SVC(kernel='linear', probability=True)`, lo que indica que se utilizará un SVM lineal y se habilitará la estimación de probabilidades, lo que puede ser útil para interpretar la confianza en las predicciones.\n",
    "\n",
    "#### Conceptos relacionados\n",
    "\n",
    "**SVM (Support Vector Machine):**  \n",
    "Las máquinas de vectores de soporte son algoritmos de clasificación robustos y potentes, especialmente eficaces en espacios de alta dimensión como los que se obtienen al trabajar con TF-IDF. Entre sus ventajas se destacan:\n",
    "\n",
    "- **Generalización en espacios de alta dimensión:** Debido a la naturaleza de los datos de texto, que suelen tener muchas características (palabras), un clasificador lineal SVM puede encontrar un hiperplano que separe eficientemente las clases.\n",
    "- **Regularización:** Las SVM incorporan mecanismos de regularización que ayudan a evitar el sobreajuste, lo que es fundamental cuando se trabaja con conjuntos de datos de alta dimensión y con relativamente pocos ejemplos.\n",
    "- **Interpretabilidad:** Al usar un kernel lineal, es posible interpretar los coeficientes asociados a cada característica, lo que permite conocer qué palabras o términos influyen más en la clasificación.\n",
    "\n",
    "**Técnicas adicionales en clasificación de texto:**\n",
    "- **Regresión logística:** Es otra técnica lineal utilizada en clasificación de texto, especialmente útil cuando se requiere una interpretación probabilística directa.\n",
    "- **Random Forest y árboles de decisión:** Aunque menos comunes en textos debido a la alta dimensionalidad, pueden ser efectivos si se realiza una adecuada reducción de características o ingeniería de atributos.\n",
    "- **Modelos basados en redes neuronales:** Técnicas como CNNs, RNNs, LSTMs y modelos basados en Transformers (por ejemplo, BERT) se han vuelto muy populares en tareas de NLP, ya que capturan relaciones complejas y contextos semánticos de manera más robusta.\n",
    "- **Ensamblado de modelos (Ensemble):** Combinar diferentes clasificadores (por ejemplo, SVM, Regresión Logística y modelos de árbol) a través de técnicas de votación o stacking puede mejorar la robustez y el rendimiento en tareas de clasificación.\n",
    "\n",
    "Además, se pueden utilizar métodos de **ajuste de hiperparámetros** (por ejemplo, GridSearchCV o RandomizedSearchCV) para optimizar parámetros como el coeficiente de regularización $C$ del SVM, lo cual es crítico en la obtención de un modelo con buen desempeño.\n",
    "\n",
    "\n",
    "### 5. Visualización de clusters con la matriz reducida\n",
    "\n",
    "#### Descripción en el código\n",
    "\n",
    "Para la visualización, el código primero aplica manualmente los pasos de limpieza y vectorización para obtener la matriz TF-IDF de todos los documentos. Posteriormente, se transforma esta matriz usando el `TruncatedSVD` para obtener una representación en 2 dimensiones (almacenada en `svd_matrix`). Finalmente, se utiliza Seaborn para crear un scatter plot en el que cada punto representa una reseña, y se colorean según su etiqueta (positiva o negativa).\n",
    "\n",
    "#### Conceptos relacionados\n",
    "\n",
    "**Visualización en espacios reducidos:**  \n",
    "Visualizar datos de alta dimensión es un reto común en el análisis de texto. Al aplicar una reducción a dos dimensiones, se puede observar cómo se distribuyen y agrupan los datos de forma visual:\n",
    "\n",
    "- **Clusters y separación de clases:** Se puede inspeccionar si los documentos de la misma clase (por ejemplo, reseñas positivas) tienden a agruparse, lo que puede indicar que el modelo puede diferenciar bien entre clases.\n",
    "- **Interpretación de la varianza:** La representación en 2D permite identificar qué tan dispersos están los datos y si existen patrones que sugieran subgrupos o outliers.\n",
    "- **Herramientas de visualización:** Además de Seaborn y Matplotlib, se pueden emplear bibliotecas como Plotly para visualizaciones interactivas que permitan un análisis más dinámico de los clusters.\n",
    "\n",
    "**Técnicas adicionales en visualización de datos:**\n",
    "- **t-SNE y UMAP:** Como se mencionó anteriormente, estas técnicas de reducción de dimensionalidad son muy populares para visualizar datos en 2D o 3D, pues pueden capturar estructuras no lineales en los datos.\n",
    "- **Mapas de calor:** Para visualizar la matriz de confusión o las correlaciones entre características, lo que puede aportar información sobre la calidad del modelo.\n",
    "- **Gráficos de dispersión interactivos:** Herramientas como Plotly o Bokeh permiten crear visualizaciones interactivas que facilitan la exploración de clusters y relaciones entre datos.\n",
    "\n",
    "\n",
    "### 6. Detalle del pipeline y flujo de datos\n",
    "\n",
    "#### Estructura del pipeline\n",
    "\n",
    "El pipeline de scikit-learn se encadena de la siguiente manera:\n",
    "\n",
    "1. **`TextCleaner`:** Recibe los documentos en forma de texto crudo y devuelve una versión limpia y normalizada.\n",
    "2. **`TfidfVectorizer`:** Toma el texto limpio y lo transforma en una matriz numérica basada en la representación TF-IDF. Esto convierte cada documento en un vector en un espacio de características de tamaño limitado (en este caso, 50 dimensiones).\n",
    "3. **`TruncatedSVD`:** Reduce la dimensionalidad de la matriz TF-IDF a un espacio de 2 dimensiones. Aunque esta transformación se utiliza en el pipeline completo para la clasificación, su componente de 2 dimensiones también facilita la visualización.\n",
    "4. **`SVC`:** Utiliza los vectores transformados para entrenar un modelo SVM lineal que clasifica los documentos en dos categorías (positivo y negativo).\n",
    "\n",
    "#### División de datos y entrenamiento\n",
    "\n",
    "El código utiliza `train_test_split` para dividir el conjunto de datos en entrenamiento y prueba, manteniendo la proporción de clases mediante el parámetro `stratify`. Esto asegura que la distribución de reseñas positivas y negativas sea similar en ambos conjuntos. Luego, el pipeline se entrena con los datos de entrenamiento y se evalúa utilizando un reporte de clasificación que muestra métricas como precisión, recall y F1-score para cada clase.\n",
    "\n",
    "#### Proceso de transformación manual para visualización\n",
    "\n",
    "Para la visualización de clusters, se aplica manualmente la secuencia de transformaciones de las etapas de limpieza y vectorización a todos los datos de texto (sin dividirlos). Se transforma el texto usando el transformador `cleaner`, luego se aplica el vectorizador TF-IDF, y finalmente se reduce la dimensionalidad con TruncatedSVD. Con la matriz resultante de dos dimensiones, se utiliza Seaborn para crear un gráfico de dispersión en el que se diferencian las clases mediante el color.\n",
    "\n",
    "\n",
    "### 7. Técnicas adicionales y enriquecimiento del pipeline\n",
    "\n",
    "Además de las técnicas implementadas en el código, es posible incorporar otros métodos y mejoras en cada etapa del pipeline para abordar desafíos adicionales en el procesamiento y clasificación de textos. Algunas de estas técnicas incluyen:\n",
    "\n",
    "#### Preprocesamiento avanzado\n",
    "\n",
    "- **Lematización:** Utilizar lematizadores (por ejemplo, `WordNetLemmatizer` para inglés o equivalentes para español) puede preservar mejor el significado de las palabras en comparación con el stemming.\n",
    "- **Corrección de errores ortográficos:** Implementar algoritmos de corrección ortográfica puede mejorar la calidad del texto antes de la tokenización, especialmente en datasets que contienen errores tipográficos.\n",
    "- **Extracción de entidades nombradas:** Técnicas de reconocimiento de entidades (NER) permiten identificar y etiquetar nombres de personas, organizaciones o lugares, lo cual puede enriquecer la representación del documento.\n",
    "- **Normalización de emojis y emoticonos:** En textos provenientes de redes sociales, la conversión de emojis en descripciones textuales puede aportar información valiosa sobre el sentimiento.\n",
    "\n",
    "#### Representación del texto\n",
    "\n",
    "- **Embeddings de palabras:** Además de TF-IDF, se pueden utilizar técnicas basadas en embeddings, como Word2Vec, GloVe o FastText, que generan representaciones densas de palabras capturando relaciones semánticas y contextuales.\n",
    "- **Embeddings contextuales:** Modelos basados en Transformers (por ejemplo, BERT, RoBERTa) generan embeddings contextuales, lo que significa que la representación de una palabra varía según su contexto, mejorando la capacidad del modelo para capturar matices semánticos.\n",
    "- **Incorporación de N-gramas:** La inclusión de n-gramas en la representación TF-IDF permite capturar secuencias y relaciones entre palabras, lo cual es útil para detectar expresiones idiomáticas o combinaciones frecuentes de palabras.\n",
    "\n",
    "#### Mejora de la clasificación\n",
    "\n",
    "- **Validación cruzada:** Utilizar técnicas de validación cruzada (por ejemplo, k-fold cross-validation) permite evaluar la robustez del modelo en distintos subconjuntos de datos y ajustar hiperparámetros de manera más precisa.\n",
    "- **Optimización de hiperparámetros:** Implementar búsquedas de hiperparámetros (GridSearchCV, RandomizedSearchCV) para afinar parámetros críticos del SVM (como $C$ o el margen) puede mejorar significativamente el rendimiento.\n",
    "- **Métodos de ensamblado:** Combinar varios modelos mediante técnicas de ensamblado, como bagging o boosting, puede resultar en modelos más robustos ante la variabilidad de los datos.\n",
    "- **Regularización y selección de características:** Técnicas de regularización (como L1 o L2) ayudan a evitar el sobreajuste, mientras que métodos de selección de características pueden reducir la dimensionalidad antes de la vectorización.\n",
    "\n",
    "#### Visualización y análisis de resultados\n",
    "\n",
    "- **Matriz de confusión:** Además del reporte de clasificación, la matriz de confusión es una herramienta visual que permite identificar patrones de error entre las clases.\n",
    "- **Curvas ROC y AUC:** Estas curvas permiten evaluar la capacidad del clasificador para distinguir entre clases y ayudan a elegir umbrales óptimos.\n",
    "- **Análisis de componentes principales (PCA):** Aunque se utiliza TruncatedSVD para datos dispersos, en otros contextos se puede aplicar PCA para explorar la varianza explicada por cada componente y comprender mejor la estructura del espacio de características.\n",
    "- **Visualización interactiva:** Herramientas como Plotly o Bokeh pueden usarse para crear dashboards interactivos que faciliten la exploración dinámica de clusters y relaciones entre documentos.\n",
    "\n",
    "#### Integración con otros sistemas\n",
    "\n",
    "- **Pipeline de preprocesamiento modular:** El pipeline presentado puede integrarse en sistemas de procesamiento de datos más grandes, permitiendo la incorporación de múltiples transformaciones y la actualización dinámica de los modelos.\n",
    "- **Análisis en tiempo real:** En aplicaciones de análisis de sentimiento en redes sociales o monitorización de opiniones, el pipeline puede adaptarse para procesar flujos de datos en tiempo real.\n",
    "- **Implementación en producción:** La modularidad del pipeline facilita su despliegue en entornos de producción, donde se pueden aplicar técnicas adicionales de monitoreo y ajuste dinámico del modelo para mantener un rendimiento óptimo.\n",
    "\n",
    "#### Aspectos técnicos y consideraciones de implementación\n",
    "\n",
    "- **Eficiencia computacional:** La reducción de dimensionalidad con TruncatedSVD no solo facilita la visualización, sino que también reduce la complejidad computacional en etapas posteriores, lo que es fundamental en aplicaciones con grandes volúmenes de datos.\n",
    "- **Escalabilidad:** El uso de transformadores personalizados y pipelines de scikit-learn permite escalar el proceso a datasets más grandes y realizar paralelización en las transformaciones.\n",
    "- **Interoperabilidad con otras herramientas:** La integración de NLTK para el procesamiento lingüístico y scikit-learn para la construcción del pipeline muestra cómo combinar diversas bibliotecas de Python para obtener una solución completa en NLP.\n",
    "- **Manejo de datos no estructurados:** El enfoque presentado es extensible a otros tipos de datos no estructurados, como publicaciones en redes sociales, artículos de noticias o comentarios en foros, donde el preprocesamiento y la representación del texto son críticos para el éxito del análisis.\n",
    "\n",
    "#### Ejemplo de ampliación del Pipeline\n",
    "\n",
    "Imaginemos que se desea agregar una etapa adicional para mejorar la representación semántica del texto antes de aplicar TF-IDF. Una posible extensión del pipeline podría incluir:\n",
    "\n",
    "- **Extracción de n-gramas:** Modificar el `TfidfVectorizer` para que incluya bi-gramas y tri-gramas, lo cual se hace configurando el parámetro `ngram_range=(1,3)`. Esto permite capturar expresiones compuestas y relaciones contextuales más ricas.\n",
    "- **Incorporación de embeddings preentrenados:** Después del preprocesamiento, se podría transformar el texto utilizando embeddings preentrenados (por ejemplo, utilizando la librería `gensim` para Word2Vec) y luego combinar estas representaciones con las generadas por TF-IDF mediante técnicas de concatenación o fusión de características.\n",
    "- **Pipeline de selección de características:** Una vez obtenida la matriz TF-IDF (o la combinación de características), se podría incluir un paso de selección de características utilizando métodos como `SelectKBest` o `chi2` para eliminar términos que no contribuyen significativamente a la discriminación entre clases.\n",
    "- **Evaluación multiclase o multietiqueta:** Aunque el ejemplo se centra en una clasificación binaria, el pipeline puede adaptarse a tareas de clasificación multiclase o incluso de clasificación multietiqueta, donde cada documento puede pertenecer a múltiples categorías simultáneamente. Esto implica ajustar tanto la representación de datos como la elección del clasificador.\n",
    "\n",
    "#### Potenciales desafíos y consideraciones\n",
    "\n",
    "Durante la implementación de un pipeline de procesamiento de texto y clasificación, pueden surgir varios desafíos técnicos:\n",
    "\n",
    "- **Desequilibrio de clases:** Si el dataset presenta un desequilibrio en la cantidad de ejemplos por clase, es posible que el modelo se sesgue hacia la clase mayoritaria. Técnicas como el sobremuestreo (oversampling) o submuestreo (undersampling) y el uso de métricas adecuadas (por ejemplo, F1-score) ayudan a mitigar este problema.\n",
    "- **Ruido en los catos:** Textos con errores ortográficos, ambigüedad lingüística o uso de jerga pueden requerir técnicas adicionales de preprocesamiento y normalización.\n",
    "- **Sobreajuste (overfitting):** En conjuntos de datos pequeños, como el ejemplo simulado, el riesgo de sobreajuste es considerable. La validación cruzada y la regularización son fundamentales para obtener modelos que generalicen bien.\n",
    "- **Interpretabilidad:** Aunque los modelos lineales como el SVM son relativamente interpretables, la interpretación de los coeficientes y la correlación con los términos del vocabulario requiere una visualización y un análisis detallado, especialmente cuando se combinan varias técnicas de reducción de dimensionalidad.\n",
    "\n",
    "\n",
    "### 8. Integración de técnicas avanzadas en el pipeline\n",
    "\n",
    "Para enriquecer aún más el pipeline, se pueden integrar técnicas de vanguardia en el campo del procesamiento de lenguaje natural:\n",
    "\n",
    "- **Modelos basados en Transformers:** La incorporación de modelos como BERT, que permiten obtener embeddings contextuales para cada token, puede reemplazar o complementar la representación TF-IDF. Esto es particularmente útil en escenarios en los que el contexto y la ambigüedad de las palabras son cruciales.\n",
    "- **Fine-tuning de modelos preentrenados:** En lugar de entrenar un clasificador SVM desde cero, se puede utilizar un modelo preentrenado y ajustar sus pesos mediante fine-tuning para la tarea específica de clasificación, lo cual ha demostrado mejorar el rendimiento en diversas tareas de NLP.\n",
    "- **Técnicas de regularización avanzada:** Además de las técnicas básicas de regularización, se pueden aplicar métodos como dropout o técnicas bayesianas para mejorar la robustez del modelo frente a datos ruidosos o limitados.\n",
    "- **Explicabilidad del modelo:** Herramientas como LIME o SHAP pueden integrarse para proporcionar explicaciones locales sobre por qué un determinado texto fue clasificado de cierta manera, lo cual es esencial en aplicaciones donde la interpretabilidad es clave.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83880f9e-fa80-4c80-82f7-f62410a1a85f",
   "metadata": {},
   "source": [
    "## 5. Pipeline multisalida para regresión y clasificación simultánea\n",
    "\n",
    "\n",
    "### 1. Contexto y propósito del pipeline multisalida\n",
    "\n",
    "El código presentado se orienta a resolver un problema multisalida en el que se realizan dos predicciones simultáneas sobre el mismo conjunto de datos. En este ejemplo, se combinan dos tareas distintas:\n",
    "- **Tarea de regresión:** Predecir un valor continuo (por ejemplo, podría tratarse de un puntaje, ingreso, o cualquier métrica cuantitativa derivada de las características).\n",
    "- **Tarea de clasificación:** Determinar la pertenencia a una de dos clases (por ejemplo, clasificar en “positivo” o “negativo”, o en cualquier binario) basándose en las mismas variables predictoras.\n",
    "\n",
    "Esta estrategia permite aprovechar un único conjunto de features para abordar problemas complejos donde se requieren múltiples salidas. En muchos escenarios reales, los predictores pueden influir de manera diferente sobre cada tipo de respuesta, lo que conduce a la necesidad de aplicar preprocesamientos diferenciados y evaluar conjuntamente el rendimiento de ambos modelos.\n",
    "\n",
    "\n",
    "### 2. Generación del dataset sintético\n",
    "\n",
    "Para ilustrar el pipeline, se genera un dataset sintético utilizando tres variables (f1, f2, f3) distribuidas de manera aleatoria:\n",
    "\n",
    "- **f1:** Se genera a partir de una distribución uniforme entre 0 y 100.\n",
    "- **f2:** Se obtiene de una distribución uniforme en el rango [50, 150].\n",
    "- **f3:** Se genera a partir de una distribución normal estándar.\n",
    "\n",
    "A partir de estas características se definen dos salidas:\n",
    "\n",
    "- **y_reg (regresión):** Se define una relación lineal en la que el valor a predecir se obtiene como  \n",
    "  $$\n",
    "  y_{\\text{reg}} = 2 \\times \\text{f1} - 0.5 \\times \\text{f2} + \\text{ruido}\n",
    "  $$\n",
    "  donde el término de ruido (ruido gaussiano) introduce variabilidad y simula datos del mundo real.\n",
    "\n",
    "- **y_clf (clasificación):** Se define una regla sencilla en la que se clasifica a los datos según el signo de f3. Si f3 es mayor que cero, se asigna la clase 1 (por ejemplo, \"positivo\"); de lo contrario, la clase 0 (por ejemplo, \"negativo\").\n",
    "\n",
    "Esta generación controlada permite experimentar con dos tipos de salida que, aunque comparten las mismas variables predictoras, requieren tratamientos distintos durante el preprocesamiento y la evaluación.\n",
    "\n",
    "\n",
    "### 3. Preprocesamiento diferencial con DualPreprocessor\n",
    "\n",
    "Uno de los puntos clave en este pipeline es el **preprocesamiento diferencial para cada tarea**. En muchos problemas multisalida, las mismas características pueden tener comportamientos o escalas distintas que afectan de manera diferente a la tarea de regresión y a la de clasificación. Para abordar esto, se define una clase personalizada llamada `DualPreprocessor` que implementa dos estrategias de escalado:\n",
    "\n",
    "- **Para la tarea de regresión:** Se utiliza el **StandardScaler**, el cual estandariza las características removiendo la media y escalando a la varianza unitaria. Esta normalización es útil cuando se desea que las variables tengan una distribución aproximadamente normal, lo que favorece a modelos sensibles a la escala (como la regresión lineal).\n",
    "\n",
    "- **Para la tarea de clasificación:** Se aplica el **RobustScaler**, que escala las características utilizando la mediana y el rango intercuartílico. Este método es especialmente robusto ante outliers y distribuciones asimétricas, características que pueden afectar negativamente a algunos clasificadores.\n",
    "\n",
    "La implementación de `DualPreprocessor` hereda de `BaseEstimator` y `TransformerMixin`, permitiendo su integración en pipelines de scikit-learn. En el método `fit`, se ajustan ambos escaladores a los datos de entrada, y en el método `transform` se generan dos DataFrames:\n",
    "- **X_reg:** Resultado de aplicar el StandardScaler para la regresión.\n",
    "- **X_clf:** Resultado de aplicar el RobustScaler para la clasificación.\n",
    "\n",
    "Esta separación permite que cada tarea reciba un conjunto de features preprocesadas de forma óptima para sus necesidades, demostrando la flexibilidad en la preparación de datos para problemas multisalida.\n",
    "\n",
    "### 4. División del dataset para cada tarea\n",
    "\n",
    "El siguiente paso consiste en dividir los datos preprocesados en conjuntos de entrenamiento y prueba. Dado que se cuenta con dos conjuntos distintos de features (uno para regresión y otro para clasificación), se realiza la partición de forma independiente para cada uno:\n",
    "\n",
    "- Para la **regresión**, se utiliza `X_reg_proc` y la variable `y_reg`.\n",
    "- Para la **clasificación**, se utiliza `X_clf_proc` y la variable `y_clf`.\n",
    "\n",
    "El uso de `train_test_split` con un `random_state` fijo garantiza reproducibilidad en las divisiones. Esta separación es fundamental para poder evaluar el rendimiento de cada modelo de forma independiente y, posteriormente, analizar posibles correlaciones entre las salidas.\n",
    "\n",
    "### 5. Construcción de pipelines para regresión y clasificación\n",
    "\n",
    "Una vez definidos los conjuntos de datos, se crean dos pipelines separados, uno para cada tarea:\n",
    "\n",
    "#### Pipeline de regresión\n",
    "\n",
    "- **Modelo utilizado:** Se emplea un `LinearRegression`, que es un modelo simple y ampliamente interpretativo para problemas de regresión.\n",
    "- **Flujo:** El pipeline se conforma únicamente del estimador, ya que el preprocesamiento ya se realizó de forma diferenciada en etapas anteriores.\n",
    "\n",
    "#### Pipeline de clasificación\n",
    "\n",
    "- **Modelo utilizado:** Se utiliza un clasificador basado en Support Vector Machines (SVC) con kernel RBF (Radial Basis Function) y la opción `probability=True`. Esto permite no solo clasificar, sino también estimar la probabilidad de pertenencia a la clase positiva.\n",
    "- **Flujo:** Se encapsula el SVC dentro de un pipeline de scikit-learn, lo que facilita la integración con técnicas de búsqueda de hiperparámetros.\n",
    "\n",
    "\n",
    "### 6. Optimización de hiperparámetros para clasificación\n",
    "\n",
    "Para la tarea de clasificación se utiliza **GridSearchCV** para optimizar los hiperparámetros del clasificador SVM. En este caso, se define una grilla de parámetros:\n",
    "- **C:** Parámetro de regularización, con valores 0.1, 1.0 y 10.\n",
    "- **gamma:** Parámetro del kernel RBF, con valores `'scale'` y `'auto'`.\n",
    "\n",
    "El GridSearchCV se configura con 5 particiones en validación cruzada (cv=5) y se utiliza la métrica F1 para la evaluación (scoring='f1'). Esta métrica es especialmente útil en clasificación binaria cuando se desea equilibrar la precisión y la sensibilidad del modelo, lo que resulta crucial en contextos donde las clases pueden estar desequilibradas o donde se desea evitar tanto falsos positivos como falsos negativos.\n",
    "\n",
    "Una vez encontrado el conjunto óptimo de hiperparámetros, el mejor estimador se entrena de nuevo con el conjunto de entrenamiento de clasificación.\n",
    "\n",
    "### 7. Entrenamiento y evaluación individual de cada tarea\n",
    "\n",
    "#### Evaluación de la regresión\n",
    "\n",
    "Después de entrenar el pipeline de regresión, se evalúa su rendimiento utilizando dos métricas clásicas:\n",
    "\n",
    "- **Error cuadrático medio (MSE):** Mide la media de los errores al cuadrado entre las predicciones y los valores reales. Es sensible a outliers debido a la penalización cuadrática.\n",
    "- **Coeficiente de determinación ($R^2$):** Indica la proporción de la varianza de la variable dependiente que es explicada por el modelo. Un valor de R² cercano a 1 indica un buen ajuste.\n",
    "\n",
    "Ambas métricas permiten entender la precisión del modelo en la predicción del valor continuo, ofreciendo una perspectiva tanto del error absoluto como del ajuste global.\n",
    "\n",
    "### Evaluación de la Clasificación\n",
    "\n",
    "Para la tarea de clasificación se utiliza el **classification_report** de scikit-learn, que resume varias métricas, incluyendo:\n",
    "- **Precisión (precision):** La proporción de verdaderos positivos sobre el total de positivos predichos.\n",
    "- **Recall (sensibilidad):** La proporción de verdaderos positivos sobre el total de positivos reales.\n",
    "- **F1-score:** La media armónica entre precisión y recall, ofreciendo una medida balanceada de la exactitud del clasificador.\n",
    "\n",
    "El uso de estas métricas permite evaluar de forma integral la capacidad del clasificador para distinguir entre las dos clases.\n",
    "\n",
    "\n",
    "### 8. Análisis conjunto: Relación entre predicción de regresión y probabilidad de clase\n",
    "\n",
    "Una parte interesante del código es el análisis que cruza las dos salidas. Se estudia la relación entre la salida del modelo de regresión y la probabilidad estimada por el clasificador para la clase positiva.\n",
    "\n",
    "Para ello se obtiene la probabilidad de la clase positiva utilizando el método `predict_proba` del clasificador SVM. Luego, se genera un scatter plot en el que:\n",
    "- El eje x representa las predicciones continuas del modelo de regresión.\n",
    "- El eje y representa la probabilidad de pertenecer a la clase positiva.\n",
    "\n",
    "Este gráfico puede revelar correlaciones o patrones interesantes. Por ejemplo, podría observarse que a mayor valor de la variable continua predicha, mayor es la probabilidad de que la instancia pertenezca a la clase positiva. Dicho análisis es especialmente relevante en contextos donde se espera que las salidas de la regresión y la clasificación estén relacionadas (por ejemplo, cuando el valor continuo representa una medida de riesgo, ingreso, o puntuación que influye en la probabilidad de una decisión).\n",
    "\n",
    "\n",
    "### 9. Conceptos clave y técnicas relacionadas\n",
    "\n",
    "#### 9.1. Concepto de multisalida (multioutput)\n",
    "\n",
    "El problema de **multisalida** o **multioutput** se refiere a situaciones en las que, a partir de un mismo conjunto de predictores, se desean obtener múltiples salidas. Estas salidas pueden ser:\n",
    "- **De naturaleza continua (regresión múltiple):** Por ejemplo, predecir diferentes métricas económicas, resultados de encuestas o puntuaciones.\n",
    "- **De naturaleza categórica (clasificación múltiple):** Por ejemplo, asignar múltiples etiquetas a un documento o predecir distintos aspectos de una situación.\n",
    "- **Una combinación de ambas:** Tal como se ilustra en el ejemplo, donde se predice un valor continuo y se asigna una clase.\n",
    "\n",
    "El enfoque multisalida resulta muy útil en problemas donde las variables objetivo pueden estar correlacionadas, permitiendo que la información compartida entre tareas mejore el rendimiento general y ofrezca una visión más integrada del problema.\n",
    "\n",
    "#### 9.2. Preprocesamiento diferencial para cada tarea\n",
    "\n",
    "Como se ha descrito, no es raro que las tareas de regresión y clasificación requieran distintos tratamientos en la etapa de preprocesamiento. Por ejemplo:\n",
    "- **Escalado para regresión:** Muchas veces se prefiere la estandarización para modelos lineales, ya que se asume que los datos siguen una distribución normal.\n",
    "- **Escalado para clasificación:** Puede ser beneficioso utilizar escaladores robustos que minimicen el impacto de outliers y preserven la estructura de los datos cuando la distribución es sesgada.\n",
    "\n",
    "El transformador `DualPreprocessor` implementa esta idea dividiendo el procesamiento de los mismos datos de entrada en dos ramas, asegurando que cada tarea reciba el tratamiento adecuado sin comprometer la integridad del conjunto de features.\n",
    "\n",
    "#### 9.3. MultiOutputRegressor\n",
    "\n",
    "Aunque en el código se muestra un ejemplo en el que se tienen solo dos tareas (una de regresión y otra de clasificación) y se tratan de forma separada, la biblioteca scikit-learn ofrece la clase **MultiOutputRegressor**. Este envoltorio permite que un regressor base se aplique de manera independiente a cada una de las variables objetivo continuas en un problema multisalida.\n",
    "\n",
    "Por ejemplo, si se tuviera un problema donde se desean predecir varios valores continuos a partir de las mismas características, **MultiOutputRegressor** encapsularía un modelo (como LinearRegression, DecisionTreeRegressor, etc.) y lo aplicaría a cada variable objetivo, permitiendo un entrenamiento simultáneo. Aunque en este ejemplo se maneja una única variable de regresión, el concepto es fundamental en aplicaciones donde se tienen múltiples salidas continuas y se busca explotar las correlaciones entre ellas.\n",
    "\n",
    "#### 9.4. Métricas y evaluación conjunta\n",
    "\n",
    "En problemas multisalida es esencial evaluar cada tarea con las métricas apropiadas y, además, intentar entender la relación entre ellas. En el ejemplo se utilizan:\n",
    "\n",
    "- **Para la regresión:**  \n",
    "  - **MSE (mean squared error):** Que cuantifica el error promedio en la escala cuadrática.  \n",
    "  - **R² (Coeficiente de determinación):** Que mide la proporción de la variabilidad explicada por el modelo.\n",
    "\n",
    "- **Para la clasificación:**  \n",
    "  - **F1-score:** Una métrica que equilibra precisión y recall, siendo particularmente útil en contextos con posibles desbalances entre clases.\n",
    "  - **Accuracy y otras métricas incluidas en el classification_report:** Que proporcionan una visión global del rendimiento del clasificador.\n",
    "\n",
    "La evaluación conjunta implica no solo medir cada tarea por separado, sino también analizar cómo las salidas se relacionan. En casos complejos, se pueden definir métricas compuestas o estrategias de validación cruzada que consideren la interdependencia entre las tareas. Por ejemplo, si se observa que las predicciones de la regresión se correlacionan fuertemente con la probabilidad de clase en la clasificación, esto puede indicar una estructura subyacente en los datos que el modelo ha logrado capturar.\n",
    "\n",
    "#### 9.5. Relación entre predicción continua y probabilidad de clase\n",
    "\n",
    "El análisis final del pipeline consiste en explorar la relación entre el valor predicho por el modelo de regresión y la probabilidad asignada por el clasificador a la clase positiva. Esta comparación es particularmente interesante en escenarios donde se espera que exista una correlación entre ambas salidas. Por ejemplo:\n",
    "\n",
    "- En aplicaciones de marketing, la predicción continua podría representar el gasto estimado o la afinidad de un cliente, mientras que la clasificación indica la probabilidad de que el cliente se suscriba a una promoción o producto premium.\n",
    "- En el ámbito financiero, la salida continua podría ser una estimación de riesgo o ingreso, y la clasificación indicaría la probabilidad de que un individuo cumpla con ciertos criterios de crédito.\n",
    "\n",
    "El gráfico de dispersión resultante permite visualizar si existe una tendencia en la que a mayor valor de la predicción de regresión corresponde una mayor probabilidad de la clase positiva, lo que puede ser aprovechado para ajustar estrategias o interpretar el comportamiento del modelo de forma más integral.\n",
    "\n",
    "\n",
    "### 10. Técnicas adicionales y extensiones potenciales\n",
    "\n",
    "Para enriquecer y robustecer el pipeline multisalida presentado, es posible integrar técnicas y metodologías adicionales en diferentes fases del proceso:\n",
    "\n",
    "#### 10.1. Ampliación del preprocesamiento\n",
    "\n",
    "- **Selección de características específicas para cada tarea:**  \n",
    "  En algunos problemas, ciertas variables pueden tener mayor relevancia para la regresión y otras para la clasificación. Se pueden emplear métodos de selección de características (como SelectKBest o técnicas basadas en importancia de variables) para optimizar la entrada a cada modelo.\n",
    "\n",
    "- **Transformaciones no lineales:**  \n",
    "  Además del escalado, puede ser beneficioso aplicar transformaciones no lineales (por ejemplo, logaritmos o transformaciones de Box-Cox) a ciertas variables para mejorar la linealidad o la estabilidad de la varianza en la tarea de regresión.\n",
    "\n",
    "- **Ingeniería de variables:**  \n",
    "  La creación de nuevas variables a partir de combinaciones o interacciones entre las features originales puede aportar información adicional para ambas tareas. Por ejemplo, la relación entre f1 y f2 podría tener una interpretación distinta en cada contexto.\n",
    "\n",
    "#### 10.2. Modelos y ensamblados\n",
    "\n",
    "- **Ensamblado de modelos multisalida:**  \n",
    "  Aunque en el ejemplo se utilizan modelos separados para regresión y clasificación, es posible utilizar técnicas de ensamblado que combinen las salidas de múltiples modelos para cada tarea. Por ejemplo, el uso de bagging o boosting puede ayudar a mejorar la estabilidad y precisión en ambas tareas.\n",
    "\n",
    "- **Modelos basados en redes neuronales:**  \n",
    "  Los enfoques basados en deep learning permiten construir arquitecturas con múltiples salidas. Una red neuronal con capas compartidas y ramas específicas para cada tarea puede aprender representaciones comunes y, al mismo tiempo, especializarse en las salidas particulares de regresión y clasificación.\n",
    "\n",
    "- **Aplicación de multiOutputRegressor en escenarios complejos:**  \n",
    "  En problemas con varias salidas continuas, encapsular un regressor base dentro de un MultiOutputRegressor permite entrenar modelos que, aunque independientes, pueden aprovechar la estructura común de las variables predictoras. Esto se extiende a modelos basados en árboles, redes neuronales y otros algoritmos.\n",
    "\n",
    "#### 10.3. Optimización y validación\n",
    "\n",
    "- **Validación cruzada estratificada para clasificación:**  \n",
    "  En la clasificación, especialmente cuando se manejan datos desequilibrados, es importante utilizar técnicas de validación cruzada estratificada para garantizar que cada partición del conjunto de datos mantenga la proporción de clases.\n",
    "\n",
    "- **Optimización conjunta de hiperparámetros:**  \n",
    "  En un entorno multisalida, se pueden emplear técnicas de búsqueda conjunta (por ejemplo, mediante pipelines combinados y búsqueda en cuadrícula o aleatoria) que optimicen hiperparámetros de ambos modelos simultáneamente, considerando las interacciones entre ellos.\n",
    "\n",
    "- **Técnicas de regularización avanzada:**  \n",
    "  La regularización no solo es crucial para prevenir el sobreajuste en la regresión, sino que también puede aplicarse en la clasificación (por ejemplo, utilizando SVM con distintos tipos de regularización o incorporando penalizaciones en modelos de redes neuronales).\n",
    "\n",
    "### 10.4. Evaluación y análisis de resultados\n",
    "\n",
    "- **Métricas compuestas y evaluación integral:**  \n",
    "  Además de evaluar cada modelo por separado, se pueden definir métricas que capturen el desempeño global del sistema. Por ejemplo, se puede ponderar el F1-score y el $R^2$ para obtener una medida compuesta del rendimiento multisalida, especialmente útil en aplicaciones donde ambas salidas tienen igual importancia.\n",
    "\n",
    "- **Análisis de sensibilidad y robustez:**  \n",
    "  Realizar estudios de sensibilidad sobre los hiperparámetros y el preprocesamiento puede ayudar a identificar qué variables o transformaciones tienen mayor impacto en cada tarea. Esto puede incluir análisis de importancia de variables, validación con distintos subsets de datos y pruebas con ruido añadido.\n",
    "\n",
    "- **Visualización avanzada:**  \n",
    "  Además del scatter plot que relaciona la predicción de regresión con la probabilidad de clase, se pueden emplear gráficos de dispersión 3D, diagramas de caja y violín, o mapas de calor para analizar las correlaciones entre variables y entender mejor las características del conjunto de datos.\n",
    "\n",
    "#### 10.5. Integración y despliegue en producción\n",
    "\n",
    "- **Pipeline modular y escalable:**  \n",
    "  La construcción de pipelines separados para cada tarea, junto con preprocesamientos diferenciados, permite una integración modular. Esto es especialmente útil para entornos de producción en los que se requiere actualizar o ajustar componentes individuales sin afectar al sistema completo.\n",
    "\n",
    "- **Monitoreo en tiempo real:**  \n",
    "  En aplicaciones que operan con flujos continuos de datos, se pueden implementar estrategias de monitoreo y retroalimentación que ajusten los modelos en tiempo real. Esto es relevante en ámbitos como la predicción de tendencias, análisis de sentimiento o sistemas de recomendación, donde la información evoluciona de forma dinámica.\n",
    "\n",
    "- **Explicabilidad y transparencia:**  \n",
    "  Para garantizar la confianza en los modelos, es recomendable integrar herramientas de explicabilidad (por ejemplo, SHAP o LIME) que permitan interpretar las predicciones tanto en la tarea de regresión como en la de clasificación. Esto es fundamental en contextos regulados o donde la toma de decisiones se debe justificar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10314c00-f155-496d-a412-b3f432e770f3",
   "metadata": {},
   "source": [
    "## 6. Pipeline avanzado con stacking regressor, selección de características y transformadores personalizados\n",
    "\n",
    "### 1. Generación del dataset sintético complejo\n",
    "\n",
    "La primera parte del código se encarga de generar un conjunto de datos sintético que simula un problema real donde las relaciones entre las variables predictoras y la variable objetivo son tanto lineales como no lineales.\n",
    "\n",
    "- **Características del dataset:**\n",
    "  - **X1:** Generada a partir de una distribución exponencial con parámetro de escala igual a 2. Esta variable presenta una distribución sesgada, característica común en datos de tiempos de espera o ciertas medidas financieras.\n",
    "  - **X2:** Se genera mediante una distribución normal con media 50 y desviación estándar 10, lo cual produce una variable con distribución simétrica y concentrada alrededor de su media.\n",
    "  - **X3:** Generada a partir de una distribución uniforme en el intervalo `[0, 100]`, lo que proporciona una variabilidad constante a lo largo de su rango.\n",
    "  - **X4:** Se genera usando la distribución Gamma con forma 2.0 y escala 5.0, generando valores que pueden presentar asimetría y varianza elevada.\n",
    "\n",
    "Estas variables se combinan en un DataFrame llamado `df_features` que sirve de matriz de características para el modelo.\n",
    "\n",
    "- **Variable objetivo (y_target):**\n",
    "  \n",
    "  La variable objetivo se define mediante una combinación de transformaciones y operaciones sobre las características:\n",
    "  \n",
    "  - Se utiliza una transformación de **raíz cuadrada** sobre X1, multiplicada por 3.5.\n",
    "  - Se suma una componente lineal de X2 multiplicada por 0.8.\n",
    "  - Se introduce una interacción entre X3 y X1, multiplicada por -0.02, lo cual añade una componente no lineal y de interacción entre variables.\n",
    "  - Se suma una transformación logarítmica (utilizando `np.log1p` para evitar problemas con valores cercanos a cero) sobre X4, multiplicada por 2.\n",
    "  - Se añade ruido gaussiano con desviación estándar 5 para simular variabilidad inherente a los datos del mundo real.\n",
    "  \n",
    "  Además, para simular la presencia de valores atípicos (outliers), se selecciona aleatoriamente el 5% de los casos y se multiplica la variable objetivo por 4. Esto crea situaciones en las que algunos valores se alejan notablemente del comportamiento general, lo que es común en problemas reales y pone a prueba la robustez de las técnicas de preprocesamiento.\n",
    "\n",
    "### 2. División del dataset en entrenamiento y prueba\n",
    "\n",
    "Una vez generado el dataset, se procede a dividirlo en conjuntos de entrenamiento y prueba utilizando la función `train_test_split` de scikit-learn. En este caso, se reserva el 20% de los datos para la evaluación final del modelo, asegurando que el proceso de entrenamiento y validación se realice de forma independiente. Esta separación es fundamental para poder evaluar de manera honesta la capacidad del modelo para generalizar sobre datos no vistos.\n",
    "\n",
    "\n",
    "### 3. Creación de transformadores personalizados\n",
    "\n",
    "El código presenta varios transformadores personalizados que extienden las funcionalidades de scikit-learn mediante la herencia de `BaseEstimator` y `TransformerMixin`. A continuación se describen cada uno de ellos:\n",
    "\n",
    "#### 3a. CustomWinsorizer\n",
    "\n",
    "**Propósito:**  \n",
    "El transformador **CustomWinsorizer** se encarga de aplicar una técnica conocida como *winsorización*. Esta técnica consiste en limitar (o recortar) los valores extremos de cada variable a ciertos percentiles predefinidos, en este caso el percentil inferior (0.01) y el superior (0.99). Esta operación ayuda a mitigar el efecto de los outliers sin eliminar completamente las muestras.\n",
    "\n",
    "**Implementación:**  \n",
    "- Durante el método `fit`, si la entrada es un DataFrame, se calculan los cuantiles inferior y superior para cada columna y se almacenan en `self.lower_bounds_` y `self.upper_bounds_`.\n",
    "- En el método `transform`, se recorre cada columna y se recortan los valores para que no se encuentren por debajo del valor inferior ni por encima del valor superior obtenido en `fit`.\n",
    "\n",
    "Esta técnica resulta especialmente útil en datasets con presencia de valores atípicos, ya que reduce su impacto sin la necesidad de eliminarlos, preservando la mayor parte de la información original.\n",
    "\n",
    "#### 3b. FeatureCreator\n",
    "\n",
    "**Propósito:**  \n",
    "El transformador **FeatureCreator** tiene como objetivo generar nuevas características a partir de las existentes. Esto es parte del proceso de ingeniería de variables, que busca extraer información relevante y capturar relaciones no lineales o interacciones entre las variables originales.\n",
    "\n",
    "**Operaciones realizadas:**\n",
    "- **Transformación Logarítmica:** Se aplica una transformación logarítmica a las columnas especificadas en la lista `use_log_transform` (en este caso, la columna 'X1'). La función `np.log1p` se utiliza para manejar valores cercanos a cero de forma segura.\n",
    "- **Creación de Interacciones:** Si la bandera `create_interactions` es verdadera, se crean nuevas características que representan la interacción entre variables (por ejemplo, el producto de X1 y X3) y también se generan potencias, como el cuadrado de X1.\n",
    "\n",
    "El objetivo es que estas nuevas características puedan ayudar a los modelos a capturar relaciones complejas entre las variables predictoras que no son evidentes en el dataset original.\n",
    "\n",
    "\n",
    "#### 3c. MultiModelFeatureSelector\n",
    "\n",
    "**Propósito:**  \n",
    "El transformador **MultiModelFeatureSelector** se utiliza para seleccionar un subconjunto de características relevantes basándose en la importancia de las mismas. En lugar de utilizar un solo modelo, este transformador entrena dos modelos diferentes (RandomForestRegressor y GradientBoostingRegressor) y promedia las importancias de las características obtenidas de cada uno. Luego, selecciona aquellas características cuya importancia media supere un umbral definido (en este caso, 0.05).\n",
    "\n",
    "**Funcionamiento:**\n",
    "- Se entrena cada uno de los modelos con el conjunto de datos de entrada.\n",
    "- Se extraen las importancias de las características de cada modelo.\n",
    "- Se calcula la media de las importancias y se comparan con el umbral.\n",
    "- Las características que cumplan el criterio se almacenan en `self.selected_features_` y, durante el método `transform`, solo se devuelven esas columnas.\n",
    "\n",
    "Esta estrategia de selección de características basada en múltiples modelos puede ofrecer una mayor robustez, ya que se combinan diferentes criterios de evaluación de la relevancia de cada variable.\n",
    "\n",
    "\n",
    "#### 3d. DataFrameConverter\n",
    "\n",
    "**Propósito:**  \n",
    "El transformador **DataFrameConverter** se encarga de asegurar que los datos de entrada tengan etiquetas de columna (es decir, que sean un DataFrame). Esto es importante porque muchas de las transformaciones posteriores dependen de conocer los nombres de las columnas para poder operar correctamente.\n",
    "\n",
    "**Implementación:**\n",
    "- En el método `fit`, si no se han proporcionado columnas y la entrada ya es un DataFrame, se extraen y almacenan los nombres de las columnas.\n",
    "- En el método `transform`, si los datos no son un DataFrame, se convierten a uno utilizando las columnas previamente almacenadas.\n",
    "\n",
    "De este modo se preserva la información sobre los nombres de las características, lo cual es útil para interpretabilidad y para operaciones de selección de columnas posteriores.\n",
    "\n",
    "\n",
    "#### 3e. DataFrameStandardScaler\n",
    "\n",
    "**Propósito:**  \n",
    "Este transformador extiende la funcionalidad del `StandardScaler` de scikit-learn para que, al escalar los datos, se conserven los índices y nombres de columna del DataFrame original. Esto facilita la trazabilidad de las variables a lo largo del pipeline y asegura que, tras el escalado, los datos sigan siendo fácilmente interpretables.\n",
    "\n",
    "**Funcionamiento:**\n",
    "- Se utiliza el método `transform` del StandardScaler original para escalar los datos.\n",
    "- Luego, se reconstruye un DataFrame con los datos escalados, manteniendo los índices y nombres de las columnas originales.\n",
    "\n",
    "Esta adaptación es muy útil en pipelines complejos, donde la preservación de la estructura del DataFrame permite una integración más fluida con otros transformadores personalizados que dependen de nombres de columnas para operar.\n",
    "\n",
    "### 4. Construcción del pipeline de preprocesamiento y modelado con stacking\n",
    "\n",
    "En esta sección se define el pipeline completo que integra los transformadores personalizados junto con el modelo de stacking. Se combinan varias etapas de preprocesamiento y modelado para formar un sistema robusto y modular.\n",
    "\n",
    "#### 4a. Modelos base y meta-modelo para el stacking\n",
    "\n",
    "El **stacking regressor** es una técnica de ensamblado que combina múltiples modelos base y, a partir de sus predicciones, entrena un modelo final (meta-modelo) para producir la predicción definitiva.\n",
    "\n",
    "- **Modelos base (estimators):**\n",
    "  - **RandomForestRegressor:** Un ensamble de árboles de decisión que utiliza bagging para reducir la varianza y mejorar la estabilidad.\n",
    "  - **GradientBoostingRegressor:** Un modelo de boosting que entrena secuencialmente árboles de decisión, corrigiendo errores del modelo anterior.\n",
    "  - **SVR (support vector regressor) con kernel RBF:** Un modelo basado en máquinas de soporte, especialmente útil para capturar relaciones no lineales.\n",
    "  \n",
    "- **Meta-modelo (final_estimator):**\n",
    "  - **Ridge regression:** Un modelo de regresión lineal con regularización L2. Su papel es aprender a combinar las predicciones de los modelos base de manera óptima, aprovechando la diversidad de cada uno.\n",
    "\n",
    "El **StackingRegressor** se configura con los modelos base y el meta-modelo. La idea es que los modelos base generen predicciones que sirvan como nuevas características, las cuales son luego utilizadas por el Ridge para realizar la predicción final.\n",
    "\n",
    "\n",
    "#### 4b. Integración de las etapas en un pipeline\n",
    "\n",
    "El pipeline se construye utilizando la clase `Pipeline` de scikit-learn y consta de las siguientes etapas:\n",
    "\n",
    "1. **Imputer:**  \n",
    "   Se utiliza un `SimpleImputer` con estrategia de la media para rellenar posibles valores faltantes en el dataset. La imputación es crucial en problemas reales para evitar que valores nulos afecten el rendimiento del modelo.\n",
    "\n",
    "2. **DataFrameConverter:**  \n",
    "   Se aplica para asegurar que los datos mantengan su estructura de DataFrame y se conserven las etiquetas de las columnas. Esto es especialmente importante para que los siguientes transformadores personalizados operen correctamente.\n",
    "\n",
    "3. **CustomWinsorizer (Winsor):**  \n",
    "   Se utiliza para recortar los valores extremos en cada característica, reduciendo la influencia de outliers en el modelado.\n",
    "\n",
    "4. **FeatureCreator (Featcreator):**  \n",
    "   Se generan nuevas características a partir de las existentes. En el ejemplo, se aplica una transformación logarítmica a la variable 'X1' y se crean características de interacción (por ejemplo, el producto de X1 y X3) y potencias (como X1²).\n",
    "\n",
    "5. **DataFrameStandardScaler (Scaler):**  \n",
    "   Se escala el dataset resultante para que todas las variables tengan media cero y desviación estándar uno, lo cual es fundamental para muchos algoritmos de aprendizaje, especialmente aquellos basados en distancias o que son sensibles a la escala de los datos.\n",
    "\n",
    "6. **MultiModelFeatureSelector (Feature_selector):**  \n",
    "   Se realiza una selección de características utilizando dos modelos diferentes para evaluar la importancia de las variables. Solo se mantienen aquellas que superan un umbral de importancia (en este caso, 0.05).\n",
    "\n",
    "7. **StackingRegressor (Stack):**  \n",
    "   Finalmente, se incorpora el modelo de stacking, que combina las predicciones de los modelos base para generar la predicción final utilizando el meta-modelo Ridge.\n",
    "\n",
    "Esta estructura modular permite que cada etapa del pipeline se pueda ajustar, validar y optimizar de forma independiente, al mismo tiempo que se integra de manera coherente en un sistema completo.\n",
    "\n",
    "\n",
    "### 5. Búsqueda de hiperparámetros con GridSearchCV\n",
    "\n",
    "Una vez definido el pipeline, se procede a la optimización de hiperparámetros mediante **GridSearchCV**. Este proceso implica:\n",
    "\n",
    "- **Definición del espacio de búsqueda:**  \n",
    "  Se crea un diccionario (`param_grid`) con los hiperparámetros a optimizar. En este ejemplo se ajustan:\n",
    "  - El número de estimadores del RandomForestRegressor (`stack__rf__n_estimators`) con posibles valores 50 y 80.\n",
    "  - El número de estimadores del GradientBoostingRegressor (`stack__gb__n_estimators`) con los mismos valores.\n",
    "  - El parámetro de regularización (alpha) del meta-modelo Ridge (`stack__final_estimator__alpha`) con valores 0.1, 1.0 y 10.\n",
    "  \n",
    "- **Configuración de la validación cruzada:**  \n",
    "  Se utiliza una validación cruzada de 5 particiones (cv=5) para evaluar la robustez del modelo en diferentes subconjuntos del conjunto de entrenamiento. El scoring se define en función del error cuadrático medio negativo (`neg_mean_squared_error`), que es una métrica común en problemas de regresión.\n",
    "\n",
    "- **Paralelización:**  \n",
    "  Se emplea `n_jobs=-1` para aprovechar todos los núcleos disponibles en la máquina y acelerar el proceso de búsqueda.\n",
    "\n",
    "Una vez ejecutada la búsqueda, se imprime el conjunto de hiperparámetros que produjo la mejor puntuación y se muestra el valor del MSE negativo obtenido.\n",
    "\n",
    "\n",
    "### 6. Evaluación del modelo en el conjunto de prueba\n",
    "\n",
    "Tras ajustar los hiperparámetros y seleccionar el mejor modelo, se evalúa el rendimiento del pipeline en el conjunto de prueba. Se calculan y se muestran varias métricas:\n",
    "\n",
    "- **Mean squared error (MSE):**  \n",
    "  Mide la media de los errores al cuadrado entre las predicciones y los valores reales. Es sensible a valores atípicos y ofrece una idea clara del error promedio en la misma escala de la variable objetivo.\n",
    "\n",
    "- **Mean absolute error (MAE):**  \n",
    "  Proporciona el error medio absoluto, lo que ayuda a interpretar el error en unidades originales y es menos sensible a outliers que el MSE.\n",
    "\n",
    "- **$R²$ (Coeficiente de determinación):**  \n",
    "  Indica la proporción de varianza explicada por el modelo. Un valor cercano a 1 sugiere un buen ajuste, mientras que valores bajos indican que el modelo explica poco de la variabilidad de los datos.\n",
    "\n",
    "\n",
    "### 7. Análisis de residuos y visualización\n",
    "\n",
    "Una parte importante en la evaluación de modelos de regresión es el análisis de residuos. El código incluye varias visualizaciones para interpretar el comportamiento del modelo:\n",
    "\n",
    "- **Gráfico de residuos vs. Predicciones:**  \n",
    "  Se genera un scatter plot en el que se representan los residuos (diferencia entre el valor real y la predicción) en función de las predicciones. La línea horizontal en cero (dibujada en rojo y con estilo discontinúo) ayuda a identificar patrones, sesgos o heterocedasticidad en los errores.\n",
    "\n",
    "- **Histograma de residuos:**  \n",
    "  Se utiliza un histograma para observar la distribución de los residuos. Idealmente, se espera que esta distribución sea aproximadamente normal y centrada en cero, lo que indicaría que los errores del modelo son aleatorios y no sistemáticos.\n",
    "\n",
    "- **Gráfico de valores reales vs. Predicciones:**  \n",
    "  Se crea un scatter plot comparando los valores reales con las predicciones del modelo, junto con la línea ideal $y = x$ que se traza en rojo. Este gráfico facilita la identificación de desviaciones y permite evaluar visualmente la precisión del modelo en diferentes rangos de la variable objetivo.\n",
    "\n",
    "- **Selección de características:**  \n",
    "  Al final se imprime la lista de características seleccionadas por el transformador **MultiModelFeatureSelector**. Esto permite conocer qué variables han sido consideradas relevantes tras el proceso de selección basado en modelos y averiguar, a partir de las importancias medias, cuáles contribuyen de manera significativa a la predicción.\n",
    "\n",
    "\n",
    "### 8. Consideraciones adicionales y técnicas complementarias\n",
    "\n",
    "El pipeline presentado es un ejemplo avanzado que combina múltiples técnicas. Sin embargo, existen diversas estrategias adicionales que pueden enriquecer este enfoque:\n",
    "\n",
    "#### 8a. Transformaciones adicionales y enriquecimiento de características\n",
    "\n",
    "- **Ingeniería de variables avanzada:**  \n",
    "  Se pueden generar nuevas características a partir de interacciones complejas, no solo multiplicando variables, sino también aplicando transformaciones polinómicas de mayor orden o combinando transformaciones no lineales (por ejemplo, funciones trigonométricas o exponenciales).\n",
    "\n",
    "- **Reducción de dimensionalidad:**  \n",
    "  Aunque en este ejemplo se realiza una selección de características, en otros casos puede ser útil aplicar técnicas como el Análisis de Componentes Principales (PCA) o t-SNE para transformar el espacio de características a uno de menor dimensión, reduciendo el ruido y la colinealidad entre variables.\n",
    "\n",
    "- **Integración de datos externos:**  \n",
    "  La incorporación de variables derivadas de fuentes externas (por ejemplo, datos temporales, indicadores macroeconómicos o variables geoespaciales) puede mejorar la capacidad predictiva del modelo.\n",
    "\n",
    "#### 8b. Modelos de ensamblado y estrategias de stacking\n",
    "\n",
    "- **Diversificación de modelos base:**  \n",
    "  La elección de los modelos base en el stacking puede extenderse a otros algoritmos, como modelos basados en redes neuronales o máquinas de soporte vectorial con distintos kernels. La diversidad en los modelos base puede mejorar la capacidad del meta-modelo para combinar las fortalezas de cada uno.\n",
    "\n",
    "- **Validación y robustez del stacking:**  \n",
    "  Es posible implementar estrategias de validación específicas para el stacking, como validación cruzada anidada o técnicas de bagging sobre el conjunto de predicciones de los modelos base, lo cual puede ayudar a mejorar la estabilidad del meta-modelo.\n",
    "\n",
    "- **Stacking en conjuntos multisalida:**  \n",
    "  Si el problema tuviera múltiples salidas continuas, se podría extender el stacking a un entorno multisalida, combinando los resultados de varios modelos para cada una de las variables objetivo, aprovechando la interrelación entre ellas.\n",
    "\n",
    "#### 8c. Optimización y selección de hiperparámetros\n",
    "\n",
    "- **Búsqueda aleatoria (RandomizedSearchCV):**  \n",
    "  En lugar de GridSearchCV, se puede utilizar RandomizedSearchCV para explorar un espacio de hiperparámetros más amplio en menor tiempo, lo que resulta especialmente útil cuando el número de combinaciones posibles es muy alto.\n",
    "\n",
    "- **Optimización Bayesiana:**  \n",
    "  Técnicas de optimización Bayesiana (por ejemplo, usando librerías como Hyperopt o BayesianOptimization) pueden ofrecer una búsqueda más eficiente del espacio de hiperparámetros, ajustando el modelo de manera inteligente en función de los resultados previos.\n",
    "\n",
    "#### 8d. Pipeline modular y escalable\n",
    "\n",
    "- **Uso de ColumnTransformer:**  \n",
    "  Cuando se tienen variables de distintos tipos o se desea aplicar preprocesamientos diferentes a subconjuntos de columnas, se puede utilizar `ColumnTransformer` de scikit-learn para definir pipelines separados dentro del mismo modelo global.\n",
    "\n",
    "- **Integración con modelos de deep learning:**  \n",
    "  Aunque este ejemplo se centra en modelos tradicionales de machine learning, la estructura modular del pipeline facilita la integración de modelos basados en deep learning, en los cuales se pueden incorporar capas personalizadas para el procesamiento de características.\n",
    "\n",
    "#### 8e. Análisis de errores y explicabilidad\n",
    "\n",
    "- **Análisis de importancia de variables:**  \n",
    "  La técnica implementada en el **MultiModelFeatureSelector** ya proporciona una idea de la importancia de las variables. Sin embargo, se pueden utilizar herramientas como SHAP o LIME para profundizar en la explicación de las predicciones del modelo, lo que resulta particularmente útil en aplicaciones en las que la interpretabilidad es clave.\n",
    "\n",
    "- **Diagnóstico de residuos:**  \n",
    "  El análisis de residuos realizado mediante gráficos de dispersión e histogramas puede complementarse con pruebas estadísticas (por ejemplo, test de normalidad o análisis de heterocedasticidad) para evaluar la validez de los supuestos del modelo y, de ser necesario, ajustar el preprocesamiento o el modelado.\n",
    "\n",
    "### 9. Comentarios sobre técnicas avanzadas y posibles extensiones\n",
    "\n",
    "El pipeline presentado integra varias técnicas avanzadas, pero existen muchas estrategias adicionales que pueden ser consideradas en un entorno de producción o en investigaciones más profundas:\n",
    "\n",
    "- **Pipeline con componentes paralelos:**  \n",
    "  Se pueden implementar pipelines que combinen transformadores en paralelo (por ejemplo, utilizando `FeatureUnion` o `ColumnTransformer`), permitiendo aplicar distintos preprocesamientos simultáneamente a diferentes subconjuntos de variables. Esto es especialmente útil cuando se trabaja con variables de distintos tipos (numéricas, categóricas, textuales) que requieren preprocesamientos muy específicos.\n",
    "\n",
    "- **Stacking para problemas de series temporales:**  \n",
    "  La técnica de stacking no se limita a problemas de regresión o clasificación tradicionales; puede extenderse a problemas de series temporales, donde se combinan modelos que capturan diferentes patrones de tendencia y estacionalidad.\n",
    "\n",
    "- **Integración con técnicas de regularización avanzada:**  \n",
    "  Además del modelo Ridge utilizado como meta-modelo, se pueden explorar otros algoritmos con regularización, como Lasso o ElasticNet, para mejorar la interpretabilidad y reducir la sobreajuste en el meta-modelo.\n",
    "\n",
    "- **Análisis de sensibilidad de hiperparámetros:**  \n",
    "  Implementar métodos de análisis de sensibilidad para identificar cómo afectan las variaciones en los hiperparámetros a la performance final del modelo puede guiar mejor la optimización y la selección de parámetros críticos.\n",
    "\n",
    "- **Monitoreo y actualización del pipeline:**  \n",
    "  En entornos productivos, es fundamental implementar estrategias de monitoreo del rendimiento del modelo y actualizar periódicamente el pipeline en función de cambios en la distribución de datos o la aparición de nuevos patrones.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
