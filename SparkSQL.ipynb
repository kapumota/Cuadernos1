{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introducción a SparkSQL**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno cubre las operaciones básicas de Apache SparkSQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL es un módulo de Spark para el procesamiento de datos estructurados. Se utiliza para consultar datos estructurados dentro de programas de Spark, utilizando SQL o una API de DataFrame familiar.\n",
    "\n",
    "Después de completar este laboratorio, podrás:\n",
    "\n",
    "* Cargar un archivo de datos en un DataFrame.\n",
    "* Crear una vista de tabla para el DataFrame.\n",
    "* Ejecutar consultas SQL básicas y agregar datos en la vista de la tabla.\n",
    "* Crear un UDF de Pandas para realizar operaciones a nivel de columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este cuaderno, utilizaremos Python y Spark (PySpark). Estas bibliotecas deberían estar instaladas en tu entorno local. Pandas es un paquete popular para ciencia de datos en Python. En este cuaderno, usaremos Pandas para cargar un archivo CSV desde el disco a un DataFrame de Pandas en memoria. PySpark es la API de Spark para Python. \n",
    "\n",
    "En este cuaderno, usaremos PySpark para inicializar el contexto de Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install findspark\n",
    "#!pip install pyarrow==0.15.1 \n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1 - Sesión de Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea e inicializa la sesión de Spark necesaria para cargar los DataFrames y operar sobre ellos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1: Creación de la sesión y el contexto de Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una clase de contexto de Spark\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creando una sesión de Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ejemplo básico de DataFrames en Python con Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2: Inicializar la sesión de Spark\n",
    "\n",
    "Para trabajar con DataFrames, solo necesitamos verificar que la instancia de la sesión de Spark ha sido creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - Carga de datos y creación de una vista de tabla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, primero leerás un archivo CSV en un DataFrame de Pandas y luego lo convertirás en un DataFrame de Spark.\n",
    "\n",
    "Pandas es una biblioteca utilizada para la manipulación y el análisis de datos. Ofrece estructuras de datos y operaciones para crear y manipular objetos de Series y DataFrames. Los datos pueden importarse desde diversas fuentes, como matrices de Numpy, diccionarios de Python y archivos CSV. Pandas permite manipular, organizar y mostrar los datos.\n",
    "\n",
    "Para crear un DataFrame de Spark, cargaremos un DataFrame externo llamado `mtcars`. Este DataFrame incluye 32 observaciones sobre 11 variables:\n",
    "\n",
    "| Índice | Nombre de columna | Unidades/Descripción |\n",
    "| :---: | :--- | :--- |\n",
    "|[, 1] | mpg | Millas por galón |\n",
    "|[, 2] | cyl | Número de cilindros |\n",
    "|[, 3] | disp | Cilindrada (pulg. cúbicas) |  \n",
    "|[, 4] | hp  | Potencia bruta (caballos de fuerza) |\n",
    "|[, 5] | drat | Relación del eje trasero |\n",
    "|[, 6] | wt | Peso (lb/1000) |\n",
    "|[, 7] | qsec | Tiempo de 1/4 de milla |\n",
    "|[, 8] | vs  | V/S |\n",
    "|[, 9] | am | Transmisión (0 = automática, 1 = manual) |\n",
    "|[,10] | gear | Número de marchas hacia adelante |\n",
    "|[,11] | carb | Número de carburadores |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 1: Cargar datos en un DataFrame de Pandas**\n",
    "\n",
    "Pandas tiene una función conveniente para cargar datos CSV desde una URL directamente en un DataFrame de Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el archivo usando la función `read_csv` de Pandas\n",
    "mtcars = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/mtcars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsualiza algunos registros\n",
    "mtcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.rename( columns={'Unnamed: 0':'name'}, inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 2: Cargar datos en un DataFrame de Spark**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la función `createDataFrame` para cargar los datos en un DataFrame de Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(mtcars) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Veamos el esquema del DataFrame de Spark cargado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 3: Renombrar la columna existente \"vs\" a \"versus\" y asignar el nuevo DataFrame a la variable \"sdf_new\"**\n",
    "\n",
    "La función `withColumnRenamed()` se usa para renombrar los nombres de las columnas existentes. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_new = sdf.withColumnRenamed(\"vs\", \"versus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución de la función anterior no modifica el DataFrame original `sdf`, sino que crea un nuevo DataFrame `sdf_new` con la columna renombrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 4: Ver el nuevo DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa cómo `vs` ahora ha sido renombrado a `versus` en este DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 5: Crear una vista de tabla**\n",
    "Crear una vista de tabla en Spark SQL es necesario para ejecutar consultas SQL programáticamente en un DataFrame. Una vista es una tabla temporal para ejecutar consultas SQL. Una vista temporal proporciona un alcance local dentro de la sesión actual de Spark. En este ejemplo, creamos una vista temporal usando la función `createTempView()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createTempView(\"cars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 3 - Ejecutar consultas SQL y agregar datos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos una vista de tabla, podemos ejecutar consultas similares a las que haríamos en una tabla SQL. Realizaremos operaciones similares a las del notebook de DataFrames. Sin embargo, la diferencia aquí es que utilizamos directamente consultas SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra toda la tabla\n",
    "spark.sql(\"SELECT * FROM cars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra una columna específica\n",
    "spark.sql(\"SELECT mpg FROM cars\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta básica de filtrado para determinar qué autos tienen un alto kilometraje y bajo número de cilindros\n",
    "spark.sql(\"SELECT * FROM cars WHERE mpg > 20 AND cyl < 6\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el método where para obtener la lista de autos cuyo millaje por galón es menor a 18\n",
    "sdf.where(sdf['mpg'] < 18).show(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega datos y agrupar por cilindros\n",
    "spark.sql(\"SELECT count(*), cyl from cars GROUP BY cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 4 - Crear un UDF de Pandas para aplicar una operación a nivel de columna**\n",
    "Apache Spark se ha convertido en el estándar de facto para el procesamiento de big data. Para permitir que los científicos de datos aprovechen el valor del big data, Spark agregó una API de Python en la versión 0.7, con soporte para funciones definidas por el usuario (UDF). Estas funciones operan fila por fila y, por lo tanto, sufren una sobrecarga de serialización e invocación. Como resultado, muchos pipelines de datos definen UDFs en Java y Scala y luego las invocan desde Python.\n",
    "\n",
    "Los Pandas UDFs, construidos sobre Apache Arrow, brindan lo **mejor de ambos mundos**: la capacidad de definir UDFs de alto rendimiento y baja sobrecarga completamente en Python. En este ejemplo simple, crearemos un **Pandas UDF Escalar** para convertir la columna `wt` de unidades imperiales (1000 libras) a unidades métricas (toneladas métricas).\n",
    "\n",
    "Además, los UDFs pueden registrarse e invocarse en SQL de forma nativa al registrar una función Python regular utilizando el decorador `@pandas_udf()`. Luego, aplicaremos este UDF a nuestra columna `wt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 1: Importar bibliotecas y registrar un UDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa la función Pandas UDF\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"float\")\n",
    "def convert_wt(s: pd.Series) -> pd.Series:\n",
    "    # Fórmula para convertir de unidades imperiales a toneladas métricas\n",
    "    return s * 0.45\n",
    "\n",
    "spark.udf.register(\"convert_weight\", convert_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tarea 2: Aplicar el UDF a la vista de tabla**\n",
    "\n",
    "Ahora podemos aplicar la función `convert_weight` a la columna `wt` en la tabla `cars`. Esto se hace fácilmente usando la consulta SQL que se muestra a continuación. En este ejemplo, mostramos tanto el peso original (en 1000 libras) como el peso convertido (en toneladas métricas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT *, wt AS weight_imperial, convert_weight(wt) as weight_metric FROM cars\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5 - Combinación de DataFrames basada en una condición específica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1-Comprendiendo la operación JOIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el DataFrame de muestra 1\n",
    "\n",
    "data = [(\"A101\", \"John\"), (\"A102\", \"Peter\"), (\"A103\", \"Charlie\")] \n",
    "columns = [\"emp_id\", \"emp_name\"]\n",
    "dataframe_1 = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el DataFrame de muestra 2\n",
    "\n",
    "data = [(\"A101\", \"John\"), (\"A102\", \"Peter\"), (\"A103\", \"Charlie\")] \n",
    "columns = [\"emp_id\", \"emp_name\"]\n",
    "dataframe_1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "data = [(\"A101\", 3250), (\"A102\", 6735), (\"A103\", 8650)] \n",
    "columns = [\"emp_id\", \"salary\"] \n",
    "dataframe_2 = spark.createDataFrame(data, columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un nuevo DataFrame \"combined_df\" realizando un inner join\n",
    "\n",
    "combined_df = dataframe_1.join(dataframe_2, on=\"emp_id\", how=\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra los datos en combined_df como una lista de Row.\n",
    "\n",
    "combined_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2 - Rellenar los valores faltantes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un DataFrame de muestra 1 con algunos valores faltantes\n",
    "\n",
    "data = [(\"A101\", 1000), (\"A102\", 2000), (\"A103\",None)]\n",
    "columns = [\"emp_id\", \"salary\"]\n",
    "dataframe_1 = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verás que se lanza un error porque el DataFrame contiene un valor nulo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que en el tercer registro del DataFrame \"dataframe_1\", la columna “salary” contiene un valor nulo (\"na\"). Se puede rellenar con un valor utilizando la función `fillna()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellena el valor faltante de salary con un valor específico\n",
    "\n",
    "filled_df = dataframe_1.fillna({\"salary\": 3000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas prácticas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 1-Operaciones básicas en SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra todas las filas de autos de la marca Mercedez en la vista `cars` que creamos anteriormente. Los autos de Mercedez tienen el prefijo \"Merc\" en la columna de nombre del auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz doble clic **aquí** para obtener una pista.\n",
    "\n",
    "<!-- La pista está abajo:\n",
    "\n",
    "La palabra clave `like` en SQL se usa para identificar patrones. \n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz doble clic **aquí** para ver la solución.\n",
    "\n",
    "<!-- La respuesta está abajo:\n",
    "\n",
    "spark.sql(\"SELECT * FROM cars where name like 'Merc%'\").show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 2- Funciones definidas por el usuario (UDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook, creamos una UDF para convertir el peso de unidades imperiales a unidades métricas. Ahora, en este ejercicio, por favor crea una UDF en pandas para convertir la columna `mpg` a `kmpl` (kilómetros por litro). Puedes usar el factor de conversión de 0.425.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque de código para que los estudiantes respondan\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"float\")\n",
    "def convert_mileage(s: pd.Series) -> pd.Series:\n",
    "    # La fórmula para convertir de imperial a unidades métricas\n",
    "    return s * 0.425\n",
    "\n",
    "spark.udf.register(\"convert_mileage\", convert_mileage)\n",
    "\n",
    "spark.sql(\"SELECT *, mpg AS mpg, convert_mileage(mpg) as kmpl FROM cars\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz doble clic **aquí** para ver la solución.\n",
    "\n",
    "<!-- La respuesta está abajo:\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"float\")\n",
    "def convert_mileage(s: pd.Series) -> pd.Series:\n",
    "    # La fórmula para convertir de imperial a unidades métricas\n",
    "    return s * 0.425\n",
    "\n",
    "spark.udf.register(\"convert_mileage\", convert_mileage)\n",
    "\n",
    "spark.sql(\"SELECT *, mpg AS mpg, convert_mileage(mpg) as kmpl FROM cars\").show()\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "prev_pub_hash": "8ec3be29ce32c39c05949ea46748689b11f79dc4d34b91da7b7febcecd5def37"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
