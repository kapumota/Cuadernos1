{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b3b5f1-f442-46ba-a691-7f2e520b0c67",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bfbde-aedf-42fe-8e9a-6aa9db647ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías esenciales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Verificación de la disponibilidad de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo usado:\", device)\n",
    "\n",
    "\n",
    "## 1. Grafo computacional dinámico\n",
    "# Ejemplo simple de grafo computacional dinámico\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x * 3       # Operación multiplicativa\n",
    "z = y ** 2      # Operación de potencia\n",
    "z.backward()    # Propagación hacia atrás: cálculo de gradientes\n",
    "\n",
    "print(\"Valor de x:\", x.item())\n",
    "print(\"Gradiente de x:\", x.grad.item())\n",
    "# Se espera que el gradiente sea calculado como d(z)/d(x) = 2*y*dy/dx = 2*(3x)*3 = 18 cuando x=2\n",
    "\n",
    "## 2. Visualización de grafos computacionales\n",
    "\n",
    "# Descomentar la siguiente línea si torchviz no está instalado:\n",
    "!pip install torchviz\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Ejemplo para visualizar el grafo computacional\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x * 3\n",
    "z = y ** 2\n",
    "dot = make_dot(z, params={'x': x})\n",
    "# Se guarda la visualización en un archivo 'grafo_computacional.png'\n",
    "dot.render(\"grafo_computacional\", format=\"png\")\n",
    "print(\"Grafo computacional guardado en 'grafo_computacional.png'\")\n",
    "\n",
    "\n",
    "## 3. Tensores y propiedades avanzadas\n",
    "\n",
    "# Creación de un tensor simple en CPU\n",
    "tensor_cpu = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "print(\"Tensor en CPU:\")\n",
    "print(tensor_cpu)\n",
    "\n",
    "# Mover el tensor a GPU si está disponible\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor_cpu.to('cuda')\n",
    "    print(\"Tensor en GPU:\")\n",
    "    print(tensor_gpu)\n",
    "else:\n",
    "    print(\"GPU no disponible. Se continuará trabajando en CPU.\")\n",
    "\n",
    "\n",
    "## 4. Técnicas de slicing y reshape\n",
    "# Creación de un tensor de ejemplo con 20 elementos\n",
    "tensor = torch.arange(1, 21).reshape(4, 5)\n",
    "print(\"Tensor original (4x5):\")\n",
    "print(tensor)\n",
    "\n",
    "# Slicing: extraer las dos primeras filas y tres columnas\n",
    "subtensor = tensor[:2, :3]\n",
    "print(\"Subtensor obtenido mediante slicing (2x3):\")\n",
    "print(subtensor)\n",
    "\n",
    "# Cambio de forma usando reshape: transformar a una matriz de 5x4\n",
    "tensor_reshaped = tensor.reshape(5, 4)\n",
    "print(\"Tensor redimensionado a (5x4):\")\n",
    "print(tensor_reshaped)\n",
    "\n",
    "## 5. Autograd y extensibilidad\n",
    "\n",
    "# Ejemplo básico con autograd\n",
    "a = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0], requires_grad=True)\n",
    "c = a * b + a ** 2\n",
    "c.backward()\n",
    "\n",
    "print(\"Gradiente de a (calculado automáticamente):\", a.grad.item())\n",
    "print(\"Gradiente de b (calculado automáticamente):\", b.grad.item())\n",
    "\n",
    "# Ejemplo de función autograd personalizada\n",
    "class MiFuncionAutograd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Guardamos el tensor de entrada para usarlo en backward\n",
    "        ctx.save_for_backward(input)\n",
    "        # Operación forward: elevar al cuadrado el input\n",
    "        return input ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Recuperamos el tensor de entrada almacenado\n",
    "        input, = ctx.saved_tensors\n",
    "        # Cálculo del gradiente: derivada de input**2 es 2*input\n",
    "        grad_input = grad_output * 2 * input\n",
    "        return grad_input\n",
    "\n",
    "# Uso de la función personalizada\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "y = MiFuncionAutograd.apply(x)\n",
    "y.backward()\n",
    "print(\"Gradiente de x con función personalizada:\", x.grad.item())\n",
    "\n",
    "\"\"\"\n",
    "## 6. Algoritmos de backpropagation\n",
    "\n",
    "El algoritmo de backpropagation es fundamental en el entrenamiento de redes neuronales, ya que permite ajustar los parámetros del modelo mediante el cálculo de gradientes. El proceso involucra:\n",
    "1. **Forward pass:** Calcular la salida del modelo a partir de la entrada.\n",
    "2. **Cálculo de la pérdida:** Evaluar qué tan lejos está la predicción del valor esperado.\n",
    "3. **Backward pass:** Calcular los gradientes de la pérdida respecto a cada parámetro utilizando la regla de la cadena.\n",
    "4. **Actualización de parámetros:** Utilizar un optimizador para actualizar los pesos en función de los gradientes calculados.\n",
    "\"\"\"\n",
    "\n",
    "# Definición de una red neuronal simple\n",
    "class RedSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RedSimple, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)  # Capa lineal: 2 entradas -> 1 salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador\n",
    "modelo = RedSimple()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=0.01)\n",
    "\n",
    "# Datos de entrenamiento (entrada y salida deseada)\n",
    "entradas = torch.tensor([[1.0, 2.0],\n",
    "                         [2.0, 3.0],\n",
    "                         [3.0, 4.0]])\n",
    "salidas = torch.tensor([[3.0],\n",
    "                        [5.0],\n",
    "                        [7.0]])\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()          # Reiniciar gradientes\n",
    "    predicciones = modelo(entradas)\n",
    "    loss = criterion(predicciones, salidas)\n",
    "    loss.backward()                # Backward pass: cálculo de gradientes\n",
    "    optimizer.step()               # Actualización de parámetros\n",
    "\n",
    "print(\"Pérdida final tras entrenamiento (RedSimple con SGD):\", loss.item())\n",
    "\n",
    "\n",
    "## 7. Máscaras (Masks)\n",
    "\n",
    "# Ejemplo de enmascaramiento en secuencias\n",
    "secuencias = torch.tensor([[1, 2, 0, 0],\n",
    "                           [3, 4, 5, 0]])\n",
    "mascara = (secuencias != 0)  # True donde el valor no es 0\n",
    "print(\"Secuencias originales:\")\n",
    "print(secuencias)\n",
    "print(\"Máscara generada (True indica valores no nulos):\")\n",
    "print(mascara)\n",
    "\n",
    "# Uso de la máscara para extraer únicamente los elementos válidos\n",
    "elementos_filtrados = secuencias[mascara]\n",
    "print(\"Elementos filtrados utilizando la máscara:\")\n",
    "print(elementos_filtrados)\n",
    "\n",
    "## 8. Position-wise feed-forward networks\n",
    "\n",
    "# Definición de la position-wise feed-forward network\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Primera transformación seguida de una activación ReLU\n",
    "        out = F.relu(self.fc1(x))\n",
    "        # Aplicación de dropout y segunda transformación lineal\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "# Parámetros para el ejemplo\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "# Ejemplo: se asume que la entrada 'x' tiene forma [batch_size, seq_len, d_model]\n",
    "x = torch.randn(10, 20, d_model)\n",
    "output = ffn(x)\n",
    "print(\"Forma de la salida de la position-wise feed-forward network:\", output.shape)\n",
    "\n",
    "\n",
    "## 9. Manejo de batches (división de conjunto de datos)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Creación de un dataset sintético\n",
    "datos = torch.randn(100, 10)           # 100 muestras, 10 características cada una\n",
    "etiquetas = torch.randint(0, 2, (100,))  # 100 etiquetas binarias\n",
    "\n",
    "dataset = TensorDataset(datos, etiquetas)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Iteración sobre los batches\n",
    "for batch_idx, (batch_datos, batch_etiquetas) in enumerate(loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"  Datos:\", batch_datos.shape)\n",
    "    print(\"  Etiquetas:\", batch_etiquetas.shape)\n",
    "    # Aquí se podría incluir el proceso de forward y backward de un modelo\n",
    "\n",
    "## 10. Optimizadores\n",
    "\n",
    "# Reutilizamos la clase RedSimple definida anteriormente\n",
    "modelo = RedSimple()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(modelo.parameters(), lr=0.001)\n",
    "\n",
    "# Datos de ejemplo\n",
    "entradas = torch.tensor([[1.0, 2.0],\n",
    "                         [2.0, 3.0],\n",
    "                         [3.0, 4.0]])\n",
    "salidas = torch.tensor([[3.0],\n",
    "                        [5.0],\n",
    "                        [7.0]])\n",
    "\n",
    "# Ciclo de entrenamiento simple\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    predicciones = modelo(entradas)\n",
    "    loss = criterion(predicciones, salidas)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Pérdida final usando Adam:\", loss.item())\n",
    "\n",
    "\n",
    "## 11. Label Smoothing\n",
    "def label_smoothing_loss(pred, target, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Calcula la pérdida con label smoothing.\n",
    "    \n",
    "    Parámetros:\n",
    "      - pred: tensor con logits de predicción (sin aplicar softmax)\n",
    "      - target: tensor con índices de la clase correcta\n",
    "      - smoothing: factor de suavizado (por defecto 0.1)\n",
    "    \n",
    "    La función crea una distribución \"suavizada\" en la que se asigna la mayor probabilidad a la clase correcta y se distribuye el resto entre las demás clases.\n",
    "    \"\"\"\n",
    "    num_classes = pred.size(1)\n",
    "    # Crear una distribución de etiquetas suavizadas\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(pred)\n",
    "        true_dist.fill_(smoothing / (num_classes - 1))\n",
    "        # Asignar la probabilidad correspondiente a la etiqueta correcta\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - smoothing)\n",
    "    # Cálculo de la pérdida (entropía cruzada suavizada)\n",
    "    return torch.mean(torch.sum(-true_dist * F.log_softmax(pred, dim=1), dim=1))\n",
    "\n",
    "# Ejemplo de uso del label smoothing en un batch de predicciones\n",
    "predicciones = torch.randn(3, 5)  # 3 muestras, 5 clases\n",
    "etiquetas = torch.tensor([1, 0, 4])\n",
    "loss_ls = label_smoothing_loss(predicciones, etiquetas, smoothing=0.1)\n",
    "print(\"Pérdida con label smoothing:\", loss_ls.item())\n",
    "\n",
    "## 12. Batch normalization\n",
    "# Ejemplo de batch normalization para tensores de 1 dimensión (características)\n",
    "batch_norm = nn.BatchNorm1d(10)  # Normalización para 10 características\n",
    "x = torch.randn(16, 10)         # 16 muestras, 10 características cada una\n",
    "x_normalized = batch_norm(x)\n",
    "print(\"Salida tras aplicar Batch Normalization:\")\n",
    "print(x_normalized)\n",
    "\n",
    "## 13. Dropout y dropout variacional\n",
    "# Ejemplo básico de Dropout en una capa lineal\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "x = torch.randn(5, 10)\n",
    "x_dropout = dropout(x)\n",
    "print(\"Salida después de aplicar Dropout (p=0.5):\")\n",
    "print(x_dropout)\n",
    "\n",
    "# Ejemplo sencillo de dropout variacional aplicado a una red recurrente\n",
    "class RNNConDropoutVariacional(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(RNNConDropoutVariacional, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        output, hidden = self.rnn(x)\n",
    "        # Aplicar dropout variacional: la misma máscara se aplica a todas las salidas\n",
    "        output = self.dropout(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Ejemplo de uso de la RNN con dropout variacional\n",
    "rnn_model = RNNConDropoutVariacional(input_size=10, hidden_size=20, num_layers=2, dropout=0.5)\n",
    "x_seq = torch.randn(8, 15, 10)  # 8 muestras, secuencia de longitud 15, 10 características\n",
    "output, hidden = rnn_model(x_seq)\n",
    "print(\"Forma de la salida de la RNN con dropout variacional:\", output.shape)\n",
    "\n",
    "## 14. Skip connections y residual connections\n",
    "# Implementación de un bloque residual simple\n",
    "class BloqueResidual(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(BloqueResidual, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features, in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identidad = x         # Guardamos la entrada original para la conexión residual\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # Suma de la entrada original y la transformación\n",
    "        out += identidad\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Ejemplo de uso del bloque residual\n",
    "x = torch.randn(5, 10)  # 5 muestras, 10 características cada una\n",
    "bloque = BloqueResidual(10)\n",
    "salida_residual = bloque(x)\n",
    "print(\"Forma de la salida del bloque residual:\", salida_residual.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2483a37-5497-4585-a022-51add42dfe0c",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### Ejercicio 1: Propiedades básicas y avanzadas de los tensores\n",
    "\n",
    "**Objetivo:**  \n",
    "- Familiarizarse con la creación, manipulación y operación sobre tensores en PyTorch, comprendiendo tanto propiedades básicas (dimensiones, tipos, dispositivo) como operaciones avanzadas (broadcasting, indexación avanzada, manipulación de formas).\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Definir tensores de distintas dimensiones y tipos de datos.\n",
    "- Realizar operaciones aritméticas (suma, multiplicación, exponenciación) y comprobar las reglas de broadcasting.\n",
    "- Explorar propiedades como la mutabilidad, la conversión entre dispositivos (CPU/GPU) y el uso de métodos que permiten modificar la forma sin copiar datos (por ejemplo, métodos para transponer o permutar ejes).\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Qué diferencias existen entre un tensor en CPU y uno en GPU en términos de eficiencia computacional?\n",
    "- ¿Cómo afecta el broadcasting en operaciones element-wise y qué precauciones se deben tomar?\n",
    "- ¿Qué ventajas ofrece la manipulación de formas (reshape) respecto a la copia de datos en memoria?\n",
    "\n",
    "\n",
    "#### Ejercicio 2: Construcción y visualización del grafo computacional dinámico\n",
    "\n",
    "**Objetivo:**  \n",
    "- Comprender el concepto de grafo computacional dinámico y la importancia del seguimiento de operaciones para la diferenciación automática.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Diseñar una secuencia de operaciones con tensores que involucren operaciones aritméticas y funciones de activación.\n",
    "- Habilitar la construcción del grafo computacional mediante la asignación de `requires_grad=True` en los tensores.\n",
    "- Investigar y describir un método para visualizar este grafo (por ejemplo, mediante una herramienta de visualización) y explicar la relevancia de cada nodo y sus conexiones en el contexto del cálculo del gradiente.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Cómo se construye el grafo en tiempo real y qué significa que sea “dinámico”?\n",
    "- ¿Qué información ofrece la visualización y cómo se relaciona con la cadena de derivadas en backpropagation?\n",
    "- ¿Qué ventajas ofrece este enfoque en comparación con los métodos de diferenciación simbólica?\n",
    "\n",
    "\n",
    "#### Ejercicio 3: Técnicas de slicing, reshape y manipulación de datos en tensores\n",
    "\n",
    "**Objetivo:**  \n",
    "- Practicar la extracción y reorganización de datos a partir de tensores, fundamental para la preparación y el preprocesamiento de datos en IA.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Partir de un tensor de dimensiones conocidas y aplicar técnicas de slicing para extraer sub-tensores específicos.\n",
    "- Cambiar la forma de un tensor utilizando técnicas de reshape, observando cómo se reordenan los elementos.\n",
    "- Analizar la diferencia entre operaciones que generan vistas versus aquellas que crean nuevos tensores, y discutir la importancia de esta distinción en términos de memoria y rendimiento.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿En qué situaciones es preferible utilizar slicing en lugar de copiar datos?\n",
    "- ¿Qué consideraciones se deben tener al redimensionar datos para que sean compatibles con un modelo?\n",
    "- ¿Cómo se gestionan los errores comunes al cambiar la forma de los tensores?\n",
    "\n",
    "#### Ejercicio 4: Autograd - diferenciación automática y funciones personalizadas\n",
    "\n",
    "**Objetivo:**  \n",
    "- Profundizar en el mecanismo de diferenciación automática de PyTorch y aprender a implementar funciones personalizadas que definan sus propias reglas de forward y backward.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Describir el proceso de cálculo del gradiente mediante la regla de la cadena en un escenario simple.\n",
    "- Diseñar una función teórica que realice una operación matemática no estándar y que requiera definir manualmente la función de backward.\n",
    "- Comparar el resultado de la diferenciación automática con el cálculo manual del gradiente, identificando posibles discrepancias o casos especiales.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Por qué es necesario almacenar información en el contexto (ctx) durante el forward?\n",
    "- ¿Cuáles son los desafíos y beneficios de crear una función personalizada en comparación con las operaciones predefinidas?\n",
    "- ¿Cómo se puede validar que la implementación del backward es correcta?\n",
    "\n",
    "#### Ejercicio 5: Extensibilidad de PyTorch mediante módulos y funciones personalizadas\n",
    "\n",
    "**Objetivo:**  \n",
    "- Explorar la capacidad de PyTorch para definir módulos y funciones que integren algoritmos de backpropagation, permitiendo la creación de arquitecturas de red personalizadas.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Elaborar un diseño teórico para un módulo personalizado que incluya capas lineales, activaciones y una función de pérdida específica.\n",
    "- Discutir cómo se integran estas funciones personalizadas en el ciclo de entrenamiento (forward, cálculo de pérdida, backward y actualización de parámetros).\n",
    "- Proponer mejoras o modificaciones al módulo que puedan facilitar la interpretación del gradiente o la eficiencia del entrenamiento.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Qué ventajas ofrece la creación de módulos personalizados en términos de modularidad y reutilización del código?\n",
    "- ¿Cómo se garantiza que la función de pérdida personalizada se integre adecuadamente en el proceso de backpropagation?\n",
    "- ¿Qué problemas podrían surgir al extender las capacidades de PyTorch y cómo se podrían solucionar?\n",
    "\n",
    "\n",
    "#### Ejercicio 6: Uso y aplicación de máscaras en datos secuenciales\n",
    "\n",
    "**Objetivo:**  \n",
    "- Comprender el uso de máscaras para filtrar información irrelevante o nula en datos secuenciales, fundamental en tareas de procesamiento de lenguaje natural y secuencias.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Analizar un escenario en el que se disponga de secuencias con elementos de relleno (padding) y definir una estrategia para identificar y excluir dichos elementos.\n",
    "- Discutir la implementación teórica de una máscara que permita extraer únicamente los elementos válidos de la secuencia.\n",
    "- Evaluar el impacto de la aplicación de la máscara en el entrenamiento y en la predicción de modelos secuenciales.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Por qué es crucial eliminar o ignorar elementos de relleno en modelos secuenciales?\n",
    "- ¿Cómo se puede garantizar que la máscara se aplique de manera consistente durante el entrenamiento y la inferencia?\n",
    "- ¿Qué otros escenarios de datos podrían beneficiarse del uso de máscaras?\n",
    "\n",
    "\n",
    "#### Ejercicio 7: Implementación de position-wise feed-forward networks\n",
    "\n",
    "**Objetivo:**  \n",
    "- Investigar el funcionamiento y la aplicación de las Position-wise Feed-Forward Networks, esenciales en arquitecturas Transformer.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Describir el flujo de datos en una red feed-forward que se aplica de forma independiente a cada posición de una secuencia.\n",
    "- Analizar la función de cada componente: capas lineales, activación (por ejemplo, ReLU) y dropout.\n",
    "- Evaluar, de forma teórica, cómo esta arquitectura permite el procesamiento paralelo de secuencias y mejora la eficiencia del modelo.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Qué beneficios ofrece el procesamiento posición por posición en comparación con otras arquitecturas?\n",
    "- ¿Cómo se integra esta red dentro de un modelo Transformer y qué impacto tiene en el rendimiento?\n",
    "- ¿Qué limitaciones podrían existir al utilizar este enfoque en contextos de datos muy heterogéneos?\n",
    "\n",
    "\n",
    "#### Ejercicio 8: Manejo de batches y uso de optimizadores en el entrenamiento\n",
    "\n",
    "**Objetivo:**  \n",
    "- Comprender la importancia del manejo de batches en el entrenamiento de modelos y la influencia de diferentes optimizadores en la convergencia del modelo.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Diseñar un procedimiento teórico para dividir un conjunto de datos en batches y discutir cómo afecta el tamaño de batch al cálculo del gradiente.\n",
    "- Comparar las características y comportamiento de al menos dos optimizadores (por ejemplo, Adam y SGD) en un escenario de entrenamiento.\n",
    "- Reflexionar sobre la elección del optimizador adecuado para distintos tipos de modelos y conjuntos de datos.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Qué ventajas y desventajas tiene el uso de batches pequeños versus batches grandes?\n",
    "- ¿Cómo afecta la tasa de aprendizaje la convergencia de cada optimizador?\n",
    "- ¿Qué estrategias se pueden implementar para mejorar la estabilidad del entrenamiento en función del optimizador elegido?\n",
    "\n",
    "\n",
    "#### Ejercicio 9: Aplicación de técnicas de label smoothing\n",
    "\n",
    "**Objetivo:**  \n",
    "- Explorar la técnica de label smoothing para suavizar las etiquetas en problemas de clasificación, reduciendo la sobreconfianza del modelo y mejorando la generalización.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Definir, en términos teóricos, qué es label smoothing y cómo se modifica la distribución de probabilidad de las etiquetas.\n",
    "- Comparar los efectos de entrenar un modelo con y sin label smoothing en un escenario de clasificación.\n",
    "- Analizar posibles casos en los que el label smoothing puede ayudar a evitar el sobreajuste y mejorar la robustez del modelo.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Qué ventajas ofrece suavizar las etiquetas en comparación con utilizar etiquetas “hard”?\n",
    "- ¿Cómo se debe ajustar el parámetro de suavizado para evitar la pérdida de información crítica?\n",
    "- ¿Qué impacto tiene esta técnica en la interpretación de las salidas del modelo?\n",
    "\n",
    "\n",
    "#### Ejercicio 10: Integración de batch normalization y dropout en redes profundas\n",
    "\n",
    "**Objetivo:**  \n",
    "- Investigar cómo la normalización de batches y el dropout pueden mejorar la estabilidad y el rendimiento del entrenamiento en redes profundas.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Describir la función de Batch Normalization y cómo normaliza las activaciones de cada capa durante el entrenamiento.\n",
    "- Analizar el papel del Dropout en la reducción del sobreajuste, explicando el mecanismo de “apagado” aleatorio de neuronas.\n",
    "- Diseñar, a nivel conceptual, una arquitectura profunda que combine ambas técnicas y discutir cómo interactúan para mejorar la convergencia.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Cómo afecta la normalización en cada capa a la propagación del gradiente en redes muy profundas?\n",
    "- ¿Qué aspectos se deben considerar al combinar Batch Normalization y Dropout en una misma arquitectura?\n",
    "- ¿En qué situaciones podría ser contraproducente aplicar ambas técnicas simultáneamente?\n",
    "\n",
    "\n",
    "#### Ejercicio 11: Uso de dropout variacional en arquitecturas recurrentes\n",
    "\n",
    "**Objetivo:**  \n",
    "- Comprender la implementación y los beneficios del dropout variacional en redes recurrentes, asegurando que la máscara de dropout sea coherente a lo largo de la secuencia.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Explicar la diferencia entre el dropout estándar y el dropout variacional en el contexto de redes recurrentes.\n",
    "- Describir, teóricamente, cómo se aplica una máscara de dropout que se mantenga constante en cada paso temporal.\n",
    "- Evaluar las ventajas que aporta esta técnica en términos de retención de información y estabilidad en la propagación a través del tiempo.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Por qué es importante mantener una máscara consistente en arquitecturas recurrentes?\n",
    "- ¿Qué impacto tiene el dropout variacional en la capacidad del modelo para capturar dependencias a largo plazo?\n",
    "- ¿Cuáles son los desafíos al ajustar la tasa de dropout en este contexto?\n",
    "\n",
    "\n",
    "#### Ejercicio 12: Implementación de skip connection y conexiones residuales\n",
    "\n",
    "**Objetivo:**  \n",
    "- Analizar y aplicar el concepto de conexiones residuales para facilitar el entrenamiento de modelos profundos y mitigar el problema del desvanecimiento del gradiente.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Describir el concepto de skip connections y cómo permiten la propagación directa de la información.\n",
    "- Plantear, de manera teórica, la implementación de un bloque residual que incluya transformaciones intermedias y la suma de la entrada original.\n",
    "- Discutir las ventajas y posibles inconvenientes de utilizar conexiones residuales en diferentes tipos de arquitecturas (por ejemplo, redes convolucionales vs. redes recurrentes).\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Cómo ayudan las conexiones residuales a mejorar la propagación del gradiente en redes muy profundas?\n",
    "- ¿Qué diferencias existen entre implementar skip connections de forma simple y usar bloques residuales más complejos?\n",
    "- ¿En qué escenarios podría ser especialmente beneficioso incluir estas conexiones en un modelo?\n",
    "\n",
    "\n",
    "#### Ejercicio 13: Exploración de otras técnicas avanzadas en PyTorch para IA\n",
    "\n",
    "**Objetivo:**  \n",
    "- Investigar y analizar otras técnicas avanzadas que ofrece PyTorch y que resultan útiles en aplicaciones de inteligencia artificial.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Investigar y documentar al menos dos técnicas adicionales, por ejemplo:\n",
    "  - **Atención multicabecera:** Analizar cómo se implementa la atención en modelos de procesamiento de lenguaje y visión, y discutir su impacto en la eficiencia del modelo.\n",
    "  - **Embeddings y representación de datos:** Estudiar el uso de capas de embeddings para representar datos categóricos o secuenciales de forma densa.\n",
    "  - **Learning rate schedulers:** Explorar estrategias para ajustar dinámicamente la tasa de aprendizaje durante el entrenamiento y su efecto en la convergencia.\n",
    "  - **Data augmentation para imágenes o secuencias:** Discutir técnicas para aumentar el tamaño y la diversidad de los datos de entrenamiento.\n",
    "- Elaborar un informe teórico que incluya la definición, el funcionamiento y el impacto de cada técnica en modelos de IA.\n",
    "\n",
    "**Preguntas:**  \n",
    "- ¿Cómo mejora cada técnica la capacidad de un modelo para generalizar a nuevos datos?\n",
    "- ¿Qué consideraciones se deben tener al implementar estas técnicas en entornos de producción?\n",
    "- ¿Cómo se pueden combinar estas técnicas con las ya estudiadas para construir modelos más robustos y eficientes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c92ab-96ee-4450-86ce-2cd971eadcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
