{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b073d2c-7163-4f82-86bd-0fa4adcf901f",
   "metadata": {},
   "source": [
    "### 1. Pipeline con transformadores personalizados y regresión lineal (Ridge)\n",
    "\n",
    "**Aspectos avanzados:**\n",
    "\n",
    "1. **Diseño de pipelines y composición de transformadores**  \n",
    "   - Un `Pipeline` encapsula un flujo de preprocesamiento y modelado, lo que garantiza la aplicación consistente de cada paso.  \n",
    "   - La clase `ColumnTransformer` posibilita aplicar transformaciones específicas a subconjuntos de columnas, permitiendo un preprocesamiento diferenciado (numérico vs. categórico).\n",
    "\n",
    "2. **Transformaciones de variables (LogTransformer y RobustScaler)**  \n",
    "   - La **transformación logarítmica** (`LogTransformer`) es útil cuando las variables presentan distribución fuertemente asimétrica (colas largas). Reducir la asimetría favorece métodos que asumen distribución más cercana a la normalidad y disminuye el efecto de outliers.  \n",
    "   - El **escalado robusto** (`RobustScaler`) utiliza medidas robustas de tendencia central (mediana) y dispersión (IQR). A diferencia de la estandarización clásica (que utiliza media y desviación estándar), este escalado es menos sensible a valores atípicos (outliers).\n",
    "\n",
    "3. **Creación de características polinómicas (PolynomialFeatures)**  \n",
    "   - Añadir términos polinómicos de grado 2 captura relaciones no lineales entre las variables. En problemas complejos, estos términos pueden ayudar al modelo lineal a incrementar su poder de ajuste (aunque se debe vigilar el sobreajuste).  \n",
    "   - La regularización (ver siguiente punto) es clave para controlar el aumento de dimensionalidad que implica esta expansión polinómica.\n",
    "\n",
    "4. **Regresión lineal regularizada (Ridge)**  \n",
    "   - Ridge (o **regresión L2**) penaliza la suma de los cuadrados de los coeficientes.  \n",
    "   - Matemáticamente, la función de coste para Ridge se expresa como:  \n",
    "     $$\n",
    "     \\min_{\\beta} \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij})^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "     $$\n",
    "     donde $\\alpha$ controla la fuerza de la regularización.  \n",
    "   - La regularización Ridge reduce la varianza del estimador, ayudando a mitigar el sobreajuste, a costa de aumentar ligeramente el sesgo.\n",
    "\n",
    "5. **Búsqueda de hiperparámetros (GridSearchCV) y validación cruzada**  \n",
    "   - La estrategia **grid search** recorre de forma exhaustiva un conjunto discreto de valores de hiperparámetros, con **validación cruzada** (CV) en cada combinación para estimar la métrica de rendimiento.  \n",
    "   - Esto asegura que el mejor conjunto de parámetros no quede sobreajustado al conjunto de entrenamiento original, sino que se seleccione con base en múltiples particiones.\n",
    "\n",
    "6. **Análisis de residuos y métricas**  \n",
    "   - Las métricas como MSE, MAE y R2 informan sobre la bondad del ajuste en distintos sentidos:  \n",
    "     - **MSE** penaliza más fuertemente grandes errores (debido al cuadrado).  \n",
    "     - **MAE** es una métrica más robusta a outliers (errores grandes no se penalizan de manera cuadrática).  \n",
    "     - **R2** mide la proporción de varianza explicada, con valores cercanos a 1 indicando mejor ajuste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13777d51-070b-45ee-9903-f3515bc0bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, PolynomialFeatures, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Generar un dataset sintético\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "X_num = pd.DataFrame({\n",
    "    'feature1': np.random.exponential(scale=2, size=n_samples),\n",
    "    'feature2': np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "})\n",
    "X_cat = pd.DataFrame({\n",
    "    'category': np.random.choice(['A', 'B', 'C'], size=n_samples)\n",
    "})\n",
    "y = 3*X_num['feature1'] + 0.5*X_num['feature2'] + np.random.normal(scale=2, size=n_samples)\n",
    "\n",
    "data = pd.concat([X_num, X_cat], axis=1)\n",
    "data['target'] = y\n",
    "\n",
    "# Transformador personalizado para aplicar logaritmo a variables asimétricas\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables=None, column_names=None):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "          variables: lista de nombres de columna a transformar.\n",
    "          column_names: lista de nombres de columna, en caso de que la entrada sea un array.\n",
    "        \"\"\"\n",
    "        self.variables = variables\n",
    "        self.column_names = column_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Si X es DataFrame, almacenar sus columnas\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.column_names_ = X.columns\n",
    "        else:\n",
    "            if self.column_names is not None:\n",
    "                self.column_names_ = self.column_names\n",
    "            else:\n",
    "                raise ValueError(\"Si X no es DataFrame, debe proporcionar 'column_names'.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convertir a DataFrame si es necesario\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.column_names_)\n",
    "        else:\n",
    "            X = X.copy()\n",
    "        for var in self.variables:\n",
    "            # Agregar un pequeño valor para evitar log(0)\n",
    "            X[var] = np.log1p(X[var])\n",
    "        return X\n",
    "\n",
    "# Definir las variables numéricas y categóricas\n",
    "numeric_features = ['feature1', 'feature2']\n",
    "categorical_features = ['category']\n",
    "\n",
    "# Pipeline para variables numéricas\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # SimpleImputer devuelve un array, por lo que se pasan los nombres de columnas\n",
    "    ('log', LogTransformer(variables=['feature1'], column_names=numeric_features)),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
    "])\n",
    "\n",
    "# Pipeline para variables categóricas\n",
    "# Se utiliza squeeze() para convertir un DataFrame de una columna a una Series.\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot',  FunctionTransformer(lambda X: pd.get_dummies(X.squeeze()), validate=False))\n",
    "])\n",
    "\n",
    "# Preprocesamiento de columnas\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Pipeline final con modelo Ridge\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "# División de datos\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.1, 1.0, 10.0],\n",
    "    # Se ajusta el grado polinómico en el paso 'poly' del pipeline numérico\n",
    "    'preprocessor__num__poly__degree': [1, 2]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid.best_params_)\n",
    "\n",
    "# Evaluación\n",
    "y_pred = grid.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"R2 Score: {r2:.3f}\")\n",
    "\n",
    "# Análisis de residuos\n",
    "residuos = y_test - y_pred\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_pred, residuos, alpha=0.6)\n",
    "plt.hlines(0, min(y_pred), max(y_pred), colors='red')\n",
    "plt.xlabel(\"Predicciones\")\n",
    "plt.ylabel(\"Residuos\")\n",
    "plt.title(\"Análisis de residuos\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32431ddd-88ca-41ed-8e97-24a6866d4ef0",
   "metadata": {},
   "source": [
    "### 2. Pipeline para clasificación con KNN y SVM en datos desbalanceados\n",
    "\n",
    "**Aspectos avanzados:**\n",
    "\n",
    "1. **Desbalance de clases y métricas acordes**  \n",
    "   - Cuando la proporción de clases está muy desbalanceada, la exactitud (accuracy) puede ser engañosa. Por ello, se usan métricas como F1-score (combina precisión y exhaustividad) y AUC-ROC (Evalúa la tasa de verdaderos positivos contra la de falsos positivos).\n",
    "\n",
    "2. **Técnicas de sobremuestreo (SMOTE)**  \n",
    "   - **SMOTE (Synthetic Minority Over-sampling Technique)** genera sintéticamente nuevas instancias para la clase minoritaria.  \n",
    "   - A diferencia del sobre-muestreo aleatorio puro, SMOTE reduce el sobreajuste al crear ejemplos intermedios entre muestras existentes de la clase minoritaria.\n",
    "\n",
    "3. **K-Nearest neighbors (KNN)**  \n",
    "   - KNN no tiene fase de entrenamiento explícita; la clasificación se basa en la vecindad de cada punto.  \n",
    "   - El parámetro `n_neighbors` controla la complejidad: valores pequeños → alta varianza (más sobreajuste); valores grandes → alta sesgo (suavización excesiva).\n",
    "\n",
    "4. **Máquinas de vectores de soporte (SVM)**  \n",
    "   - El objetivo de la SVM es maximizar el **margen** que separa las clases.  \n",
    "   - La versión **kernelizada** (por ejemplo, RBF) proyecta los datos a un espacio de mayor dimensión para encontrar un hiperplano separador no lineal.  \n",
    "   - El hiperparámetro `C` controla la penalización al error de clasificación. Un `C` grande busca minimizar la clasificación errónea (riesgo de sobreajuste); un `C` pequeño tolera más errores en aras de un margen mayor (riesgo de subajuste).\n",
    "\n",
    "5. **Curvas ROC y matriz de confusión**  \n",
    "   - La **Curva ROC** (Receiver Operating Characteristic) evalúa la sensibilidad (TPR) frente a la especificidad (1-FPR) para diferentes umbrales de decisión.  \n",
    "   - La **matriz de confusión** provee una visión clara de cómo el modelo confunde la clase positiva con la negativa, facilitando un análisis detallado de errores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9750f-aa79-4624-8b70-f2e97a596fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Generar datos sintéticos desbalanceados\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "X_class = pd.DataFrame({\n",
    "    'feat1': np.random.normal(0, 1, n_samples),\n",
    "    'feat2': np.random.normal(5, 2, n_samples)\n",
    "})\n",
    "# Clase minoritaria: 10%, mayoritaria: 90%\n",
    "y_class = np.where(np.random.rand(n_samples) < 0.1, 1, 0)\n",
    "\n",
    "# Transformador personalizado para ingeniería de características\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convertir a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=['feat1', 'feat2'])  # Asegurar nombres correctos\n",
    "        \n",
    "        # Crear una nueva feature: relación entre feat1 y feat2\n",
    "        X['feat_ratio'] = X['feat1'] / (X['feat2'] + 1e-5)\n",
    "        # Crear feature de interacción\n",
    "        X['feat_interaction'] = X['feat1'] * X['feat2']\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Pipeline de preprocesamiento\n",
    "preprocessor_cls = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('engineer', FeatureEngineer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Función para balancear datos con SMOTE\n",
    "def balance_data(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "\n",
    "# Preprocesamiento completo\n",
    "X_processed = preprocessor_cls.fit_transform(X_class)\n",
    "X_balanced, y_balanced = balance_data(pd.DataFrame(X_processed), y_class)\n",
    "\n",
    "# Dividir datos balanceados\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear pipelines para KNN y SVM\n",
    "pipeline_knn = Pipeline([\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "pipeline_svm = Pipeline([\n",
    "    ('classifier', SVC(probability=True))\n",
    "])\n",
    "\n",
    "# Definir grids de hiperparámetros\n",
    "param_grid_knn = {\n",
    "    'classifier__n_neighbors': [3, 5, 7],\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "param_grid_svm = {\n",
    "    'classifier__C': [0.5, 1.0, 5.0],\n",
    "    'classifier__kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "grid_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=5, scoring='f1')\n",
    "grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring='f1')\n",
    "\n",
    "grid_knn.fit(X_train_cls, y_train_cls)\n",
    "grid_svm.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "print(\"Mejores hiperparámetros KNN:\", grid_knn.best_params_)\n",
    "print(\"Mejores hiperparámetros SVM:\", grid_svm.best_params_)\n",
    "\n",
    "# Evaluación en test\n",
    "for name, model in zip(['KNN', 'SVM'], [grid_knn, grid_svm]):\n",
    "    y_pred = model.predict(X_test_cls)\n",
    "    y_proba = model.predict_proba(X_test_cls)[:,1]\n",
    "    f1 = f1_score(y_test_cls, y_pred)\n",
    "    auc_val = roc_auc_score(y_test_cls, y_proba)\n",
    "    print(f\"{name} - F1-score: {f1:.3f}, AUC: {auc_val:.3f}\")\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test_cls, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Matriz de confusión - {name}\")\n",
    "    plt.xlabel(\"Predicción\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n",
    "    # Curva ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_cls, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('Tasa Falso Positivo')\n",
    "    plt.ylabel('Tasa Verdadero Positivo')\n",
    "    plt.title(f'Curva ROC - {name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea34449-ade7-49b4-ae76-e817d6c8aa22",
   "metadata": {},
   "source": [
    "### 3. Pipeline con árboles de decisión y ensamble\n",
    "\n",
    "**Aspectos avanzados:**\n",
    "\n",
    "1. **Selección de características basada en correlación**  \n",
    "   - El transformador personalizado `CorrelationSelector` elimina aquellas variables que no superan un umbral de correlación medio, estrategia útil para disminuir dimensionalidad y ruido.  \n",
    "   - Se podría combinar con otras técnicas (p. ej., selección basada en información mutua), pero el enfoque de correlación es un primer filtro simple y efectivo en muchos casos.\n",
    "\n",
    "2. **Manejo de outliers mediante Winsorización**  \n",
    "   - La **Winsorización** recorta los valores extremos a determinados percentiles. A diferencia de, por ejemplo, eliminar outliers, la Winsorización conserva todas las muestras y reduce la influencia de valores atípicos muy altos/bajos.  \n",
    "   - Esto puede ser crítico en modelos basados en árboles, que son robustos en la partición del espacio, pero también en métodos lineales o basados en distancia.\n",
    "\n",
    "3. **Modelos de árboles**  \n",
    "   - **Árbol de decisión**: un modelo altamente interpretable, pero con tendencia al sobreajuste. Requiere parámetros como `max_depth`, `min_samples_split`, etc.  \n",
    "   - **Random Forest**: combina múltiples árboles entrenados en muestras bootstrap con subconjuntos aleatorios de características. Reduce la varianza del modelo a costa de perder algo de interpretabilidad individual.  \n",
    "   - **Gradient Boosting**: secuencialmente entrena “árboles débiles” que corrigen errores de predecesores. Tiende a sobreajustar menos que los árboles base y puede converger a soluciones muy precisas con tuning adecuado de `learning_rate` y `n_estimators`.\n",
    "\n",
    "4. **Importancia de características**  \n",
    "   - En modelos de árboles, la importancia de variables (basada en la reducción de impureza o de la función de pérdida) provee una medida de cuán relevante es cada predictor en la decisión.  \n",
    "   - Sin embargo, hay que matizar que la importancia de características en árboles puede verse inflada para variables con muchos valores únicos o para variables altamente correlacionadas entre sí.\n",
    "\n",
    "5. **Validación cruzada anidada (Nested CV)**  \n",
    "   - Usada frecuentemente en problemas con muchos hiperparámetros para prevenir sobreajuste en la selección de modelos.  \n",
    "   - Envuelve un **loop externo** de particionado de datos y un **loop interno** de búsqueda de hiperparámetros. Así, el conjunto de datos de prueba en la validación externa permanece intacta con respecto de la selección de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebe40e-676c-42a2-bf34-5dc05302ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Generar dataset sintético para clasificación\n",
    "np.random.seed(2021)\n",
    "n_samples = 800\n",
    "X_dt = pd.DataFrame({\n",
    "    'age': np.random.randint(20, 70, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'score': np.random.uniform(300, 850, n_samples)\n",
    "})\n",
    "y_dt = np.where(X_dt['score'] > 600, 1, 0)\n",
    "\n",
    "# Transformador personalizado para selección de features por correlación\n",
    "class CorrelationSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.1):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convertir X a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "        corr = np.abs(np.corrcoef(X.T))\n",
    "        self.features_ = X.columns[(corr.mean(axis=0) > self.threshold)].tolist()  # Convertir a lista\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convertir X a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.features_)\n",
    "            \n",
    "        return X[self.features_]\n",
    "\n",
    "\n",
    "# Transformador para Winsorización de outliers\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.05, upper_quantile=0.95):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convertir X a DataFrame si es un ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "        self.lower_bounds_ = X.quantile(self.lower_quantile)\n",
    "        self.upper_bounds_ = X.quantile(self.upper_quantile)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Convertir a DataFrame si es necesario\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.lower_bounds_.index)\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = np.clip(X[col], self.lower_bounds_[col], self.upper_bounds_[col])\n",
    "        return X\n",
    "\n",
    "\n",
    "# Pipeline de preprocesamiento para datos numéricos\n",
    "numeric_pipeline_dt = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('winsorizer', Winsorizer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', CorrelationSelector(threshold=0.2))\n",
    "])\n",
    "\n",
    "# Definir modelos de árboles\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Pipeline completo para cada modelo\n",
    "pipelines = {}\n",
    "for name, model in models.items():\n",
    "    pipelines[name] = Pipeline([\n",
    "        ('preprocessor', numeric_pipeline_dt),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid_dt = {\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 5, 10]\n",
    "}\n",
    "param_grid_gb = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grids = {\n",
    "    'DecisionTree': GridSearchCV(pipelines['DecisionTree'], param_grid_dt, cv=5, scoring='f1'),\n",
    "    'RandomForest': GridSearchCV(pipelines['RandomForest'], param_grid_rf, cv=5, scoring='f1'),\n",
    "    'GradientBoosting': GridSearchCV(pipelines['GradientBoosting'], param_grid_gb, cv=5, scoring='f1')\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "for name, grid in grids.items():\n",
    "    grid.fit(X_dt, y_dt)\n",
    "    print(f\"{name} - Mejor Score: {grid.best_score_:.3f} | Mejores Hiperparámetros: {grid.best_params_}\")\n",
    "    # Obtener la importancia de las variables, si aplica\n",
    "    best_model = grid.best_estimator_.named_steps['classifier']\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_names = grid.best_estimator_.named_steps['preprocessor'].named_steps['selector'].features_\n",
    "        importances = best_model.feature_importances_\n",
    "        df_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        df_imp = df_imp.sort_values(by='importance', ascending=False)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(x='importance', y='feature', data=df_imp)\n",
    "        plt.title(f\"Importancia de Variables - {name}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84ae4c-7ff7-4b52-8dc3-5b2a3b96866e",
   "metadata": {},
   "source": [
    "#### 4. Pipeline de procesamiento de texto y clasificación con SVM\n",
    "\n",
    "**Aspectos avanzados:**\n",
    "\n",
    "1. **Limpieza y normalización de texto**  \n",
    "   - El transformador `TextCleaner` ejecuta pasos de normalización: conversión a minúsculas, eliminación de caracteres no alfanuméricos, tokenización, eliminación de stopwords y stemming. Cada paso reduce ruido y mejora la señal semántica.\n",
    "\n",
    "2. **Representación TF-IDF**  \n",
    "   - **TF-IDF** (Term Frequency-Inverse Document Frequency) pondera la importancia de cada palabra en el documento considerando su frecuencia global.  \n",
    "   - Formalmente, se calcula:  \n",
    "     $$\n",
    "     \\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\log \\frac{N}{\\text{df}(t)}\n",
    "     $$\n",
    "     donde $\\text{tf}(t, d)$ es la frecuencia del término $t$ en el documento $d$, $N$ es el total de documentos y $\\text{df}(t)$ es el número de documentos que contienen $t$.  \n",
    "   - Esto atenúa el efecto de palabras muy comunes (ej. \"el\", \"la\" en español) que no discriminan bien entre documentos.\n",
    "\n",
    "3. **Reducción de dimensionalidad con TruncatedSVD**  \n",
    "   - Análogo al **PCA** pero aplicado a matrices dispersas de conteos/TF-IDF. Reduce la dimensionalidad manteniendo la mayor varianza de los datos y acelerando el entrenamiento de modelos en espacios de alta dimensión.  \n",
    "   - En el contexto de procesamiento de lenguaje natural (NLP), se acerca a la idea de **LSA (Latent Semantic Analysis)**, que captura correlaciones semánticas entre términos.\n",
    "\n",
    "4. **Clasificador SVM para texto**  \n",
    "   - Las SVM lineales son muy comunes en NLP gracias a que, en espacios de alta dimensión, un modelo lineal con regularización tiende a funcionar muy bien y ser eficiente.  \n",
    "   - La elección de `kernel= linear` suele facilitar la interpretación (los coeficientes se asocian a palabras o conceptos del vocabulario).\n",
    "\n",
    "5. **Visualización de clusters con la matriz reducida**  \n",
    "   - Al proyectar TF-IDF a 2 dimensiones con SVD, puede verse cómo documentos con similar contenido semántico se agrupan en el plano.  \n",
    "   - Esto ayuda a **interpretar** si la clase positiva/negativa se separa adecuadamente en ese espacio reducido (aunque no sea el mismo espacio final donde entrena el modelo SVM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e2c57-ef14-43ba-90d1-4475bf2add02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Descarga de recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Datos de ejemplo: reseñas de producto (simulados)\n",
    "text_data = pd.DataFrame({\n",
    "    'review': [\n",
    "        \"El producto es excelente, superó mis expectativas.\",\n",
    "        \"Muy malo, no funciona como se describe.\",\n",
    "        \"La calidad es buena, pero el precio es alto.\",\n",
    "        \"Deficiente, me decepcionó la compra.\",\n",
    "        \"Es un producto decente, cumple lo básico.\",\n",
    "        \"Increíble, lo recomiendo a todos.\",\n",
    "        \"No me gustó, esperaba más calidad.\",\n",
    "        \"Buen producto, excelente relación calidad-precio.\",\n",
    "        \"Terrible, se rompió al poco tiempo de uso.\",\n",
    "        \"Muy recomendable, superó mis expectativas.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]  # 1: positivo, 0: negativo\n",
    "})\n",
    "\n",
    "# Transformador personalizado para limpieza de texto\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, language='spanish'):\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.stemmer = SnowballStemmer(language)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        cleaned = []\n",
    "        for doc in X:\n",
    "            # Convertir a minúsculas y quitar caracteres especiales\n",
    "            doc = doc.lower()\n",
    "            doc = re.sub(r'[^a-záéíóúñ\\s]', '', doc)\n",
    "            tokens = nltk.word_tokenize(doc)\n",
    "            tokens = [self.stemmer.stem(word) for word in tokens if word not in self.stop_words]\n",
    "            cleaned.append(\" \".join(tokens))\n",
    "        return cleaned\n",
    "\n",
    "# Pipeline para procesamiento de texto y clasificación\n",
    "pipeline_text = Pipeline([\n",
    "    ('cleaner', TextCleaner()),\n",
    "    ('tfidf', TfidfVectorizer(max_features=50)),\n",
    "    ('svd', TruncatedSVD(n_components=2)),\n",
    "    ('svm', SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "# División de datos (usando 'stratify' para asegurar la representación proporcional de clases)\n",
    "X_text = text_data['review']\n",
    "y_text = text_data['label']\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    X_text, y_text, test_size=0.3, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline_text.fit(X_train_text, y_train_text)\n",
    "y_pred_text = pipeline_text.predict(X_test_text)\n",
    "\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_test_text, y_pred_text, zero_division=1))\n",
    "\n",
    "# Visualización de clusters en el espacio reducido\n",
    "# Se aplica el transformador 'cleaner' y 'tfidf' manualmente para toda la data\n",
    "tfidf_matrix = pipeline_text.named_steps['tfidf'].transform(\n",
    "    pipeline_text.named_steps['cleaner'].transform(X_text)\n",
    ")\n",
    "svd_matrix = pipeline_text.named_steps['svd'].transform(tfidf_matrix)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=svd_matrix[:, 0], y=svd_matrix[:, 1], hue=y_text, palette='viridis', s=100)\n",
    "plt.title(\"Clusters en el espacio reducido (SVD)\")\n",
    "plt.xlabel(\"Componente 1\")\n",
    "plt.ylabel(\"Componente 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2010762-1ade-46fb-a029-9dd90046f3de",
   "metadata": {},
   "source": [
    "#### 5. Pipeline multisalida para regresión y clasificación simultánea\n",
    "\n",
    "**Aspectos avanzados:**\n",
    "\n",
    "1. **Concepto de multisalida (multioutput)**  \n",
    "   - Un problema multisalida combina, en un solo conjunto de predictores, una tarea de regresión y otra de clasificación (o múltiples regresiones/clasificaciones).  \n",
    "   - Un ejemplo real: predecir la edad (regresión) y el rango etario (clasificación) a partir de una misma serie de características biométricas.\n",
    "\n",
    "2. **Preprocesamiento diferencial para cada tarea**  \n",
    "   - En el ejemplo, un transformador personalizado (`DualPreprocessor`) aplica un escalado distinto (StandardScaler vs. RobustScaler) para las dos tareas.  \n",
    "   - Esto ilustra cómo puede diseñarse una lógica que permita *dividir* el espacio de features en dos enfoques de preprocesamiento (uno para la regresión y otro para la clasificación), preservando la consistencia interna del Pipeline.\n",
    "\n",
    "3. **MultiOutputRegressor**  \n",
    "   - Cuando se tienen múltiples variables objetivo de tipo continuo, `MultiOutputRegressor` envuelve un regressor base para manejar cada uno.  \n",
    "   - Aunque aquí se muestra un caso sencillo (una sola variable de regresión y otra de clasificación), el esquema MultiOutput es especialmente útil al manejar varios targets correlacionados (p. ej., predecir ventas de distintos productos).\n",
    "\n",
    "4. **Métricas y evaluación conjunta**  \n",
    "   - Se entrena y evalúa simultáneamente la parte de regresión y la de clasificación. Cada una requiere sus propias métricas: MSE, R2 para regresión y accuracy/F1 para clasificación.  \n",
    "   - Esto evidencia la importancia de diseñar un método de partición apropiado (como `train_test_split` duplicado o mayor sofisticación con validación cruzada estratificada, etc.) para garantizar evaluaciones justas en ambas tareas.\n",
    "\n",
    "5. **Relación entre predicción continua y probabilidad de clase**  \n",
    "   - Es interesante analizar cómo la salida regresiva (p. ej., un valor continuo de renta, puntaje u otra métrica) se relaciona con la probabilidad de pertenecer a la clase positiva.  \n",
    "   - En problemas más complejos, pueden hallarse correlaciones o patrones en cómo cierta variable continua se correlaciona con la probabilidad de un suceso (por ejemplo, a mayor ingreso, mayor probabilidad de comprar un producto premium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96b0bc-81e3-4258-af55-d46c5a180e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generar dataset sintético multisalida\n",
    "np.random.seed(0)\n",
    "n_samples = 600\n",
    "X_multi = pd.DataFrame({\n",
    "    'f1': np.random.uniform(0, 100, n_samples),\n",
    "    'f2': np.random.uniform(50, 150, n_samples),\n",
    "    'f3': np.random.normal(0, 1, n_samples)\n",
    "})\n",
    "# Tarea de regresión: predecir un valor continuo\n",
    "y_reg = 2*X_multi['f1'] - 0.5*X_multi['f2'] + np.random.normal(0, 5, n_samples)\n",
    "# Tarea de clasificación: clasificar en dos clases basadas en una función de X\n",
    "y_clf = np.where(X_multi['f3'] > 0, 1, 0)\n",
    "\n",
    "# Transformador personalizado para preprocesamiento diferencial\n",
    "class DualPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler_reg = StandardScaler()\n",
    "        self.scaler_clf = RobustScaler()\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler_reg.fit(X)\n",
    "        self.scaler_clf.fit(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_reg = pd.DataFrame(self.scaler_reg.transform(X), columns=X.columns)\n",
    "        X_clf = pd.DataFrame(self.scaler_clf.transform(X), columns=X.columns)\n",
    "        return X_reg, X_clf\n",
    "\n",
    "# Aplicar el transformador\n",
    "dual_preproc = DualPreprocessor()\n",
    "X_reg_proc, X_clf_proc = dual_preproc.fit_transform(X_multi)\n",
    "\n",
    "# Dividir datos para ambas tareas\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg_proc, y_reg, test_size=0.3, random_state=42)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf_proc, y_clf, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear pipelines para ambas tareas\n",
    "pipeline_reg = Pipeline([\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "pipeline_clf = Pipeline([\n",
    "    ('classifier', SVC(kernel='rbf', probability=True))\n",
    "])\n",
    "\n",
    "# Definir grids de hiperparámetros para clasificación\n",
    "param_grid_clf = {\n",
    "    'classifier__C': [0.1, 1.0, 10],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_clf = GridSearchCV(pipeline_clf, param_grid_clf, cv=5, scoring='f1')\n",
    "grid_clf.fit(X_clf_train, y_clf_train)\n",
    "print(\"Mejores hiperparámetros para clasificación:\", grid_clf.best_params_)\n",
    "\n",
    "# Entrenar regresión y clasificación\n",
    "pipeline_reg.fit(X_reg_train, y_reg_train)\n",
    "grid_clf.best_estimator_.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Evaluación de regresión\n",
    "y_reg_pred = pipeline_reg.predict(X_reg_test)\n",
    "print(\"Evaluación de regresión:\")\n",
    "print(\"MSE:\", mean_squared_error(y_reg_test, y_reg_pred))\n",
    "print(\"R2:\", r2_score(y_reg_test, y_reg_pred))\n",
    "\n",
    "# Evaluación de clasificación\n",
    "y_clf_pred = grid_clf.best_estimator_.predict(X_clf_test)\n",
    "print(\"Evaluación de clasificación:\")\n",
    "print(classification_report(y_clf_test, y_clf_pred))\n",
    "\n",
    "# Análisis conjunto: relación entre la predicción de regresión y la probabilidad de la clase positiva\n",
    "y_clf_proba = grid_clf.best_estimator_.predict_proba(X_clf_test)[:,1]\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_reg_pred, y_clf_proba, alpha=0.7)\n",
    "plt.xlabel(\"Predicción de regresión\")\n",
    "plt.ylabel(\"Probabilidad de clase positiva\")\n",
    "plt.title(\"Relación entre regresión y clasificación\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf479dd-38b7-49fc-84fd-365d121d245d",
   "metadata": {},
   "source": [
    "### 6. Pipeline avanzado con stacking regressor, selección de características y transformadores personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d318a-6932-48a9-a803-111df37e2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Generar un dataset sintético complejo con componentes lineales y no lineales\n",
    "# --------------------------------------------------------------------------------\n",
    "np.random.seed(2025)\n",
    "n_samples = 1000\n",
    "\n",
    "# Variables con diferentes distribuciones\n",
    "X1 = np.random.exponential(scale=2, size=n_samples)\n",
    "X2 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "X3 = np.random.uniform(0, 100, n_samples)\n",
    "X4 = np.random.gamma(shape=2.0, scale=5.0, size=n_samples)\n",
    "\n",
    "# Combinamos en un DataFrame\n",
    "df_features = pd.DataFrame({\n",
    "    'X1': X1, \n",
    "    'X2': X2, \n",
    "    'X3': X3, \n",
    "    'X4': X4\n",
    "})\n",
    "\n",
    "# Creamos una variable objetivo compleja con interacciones no lineales y ruido\n",
    "y_target = 3.5 * np.sqrt(X1) + 0.8 * X2 - 0.02 * X3 * X1 + 2 * np.log1p(X4) + np.random.normal(scale=5, size=n_samples)\n",
    "\n",
    "# Introducimos valores atípicos en el 5% de los casos\n",
    "mask_outliers = np.random.choice([False, True], size=n_samples, p=[0.95, 0.05])\n",
    "y_target[mask_outliers] *= 4\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Dividir en entrenamiento y prueba\n",
    "# --------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. Creación de transformadores personalizados\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# 3a. CustomWinsorizer modificado para preservar DataFrame\n",
    "class CustomWinsorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, quantile_lower=0.01, quantile_upper=0.99):\n",
    "        self.quantile_lower = quantile_lower\n",
    "        self.quantile_upper = quantile_upper\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.lower_bounds_ = X.quantile(self.quantile_lower)\n",
    "            self.upper_bounds_ = X.quantile(self.quantile_upper)\n",
    "        else:\n",
    "            X_df = pd.DataFrame(X)\n",
    "            self.lower_bounds_ = X_df.quantile(self.quantile_lower)\n",
    "            self.upper_bounds_ = X_df.quantile(self.quantile_upper)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        X_clipped = X.copy()\n",
    "        for col in X_clipped.columns:\n",
    "            lower = self.lower_bounds_[col]\n",
    "            upper = self.upper_bounds_[col]\n",
    "            X_clipped[col] = X_clipped[col].clip(lower=lower, upper=upper)\n",
    "        return X_clipped\n",
    "\n",
    "# 3b. FeatureCreator para generar características adicionales\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_log_transform=['X1'], create_interactions=True):\n",
    "        self.use_log_transform = use_log_transform\n",
    "        self.create_interactions = create_interactions\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Aplicar transformaciones logarítmicas a ciertas columnas\n",
    "        for col in self.use_log_transform:\n",
    "            X[f'log_{col}'] = np.log1p(X[col])\n",
    "        # Crear interacciones y potencias\n",
    "        if self.create_interactions:\n",
    "            if 'X1' in X.columns and 'X3' in X.columns:\n",
    "                X['X1_X3'] = X['X1'] * X['X3']\n",
    "                X['X1^2'] = X['X1'] ** 2\n",
    "        return X\n",
    "\n",
    "# 3c. MultiModelFeatureSelector para la selección de características\n",
    "class MultiModelFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.05, n_estimators=50, random_state=42):\n",
    "        self.threshold = threshold\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = [\n",
    "            RandomForestRegressor(n_estimators=self.n_estimators, random_state=self.random_state),\n",
    "            GradientBoostingRegressor(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        ]\n",
    "        importances = []\n",
    "        self.features_ = X.columns\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "            importances.append(model.feature_importances_)\n",
    "        mean_importances = np.mean(importances, axis=0)\n",
    "        self.selected_features_ = self.features_[mean_importances > self.threshold]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.selected_features_]\n",
    "\n",
    "# 3d. DataFrameConverter para asegurar que los datos tengan etiquetas de columna\n",
    "class DataFrameConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.columns is None:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                self.columns = X.columns\n",
    "            else:\n",
    "                raise ValueError(\"No se proporcionaron columnas y X no es DataFrame.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X\n",
    "        return pd.DataFrame(X, columns=self.columns)\n",
    "\n",
    "# 3e. DataFrameStandardScaler que conserva DataFrame y sus etiquetas\n",
    "class DataFrameStandardScaler(StandardScaler, TransformerMixin):\n",
    "    def transform(self, X, copy=None):\n",
    "        # Escalar los datos usando el método de StandardScaler\n",
    "        X_scaled = super().transform(X)\n",
    "        # Si X es DataFrame, reconstruirlo con los mismos índices y columnas\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "        else:\n",
    "            return X_scaled\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. Crear un pipeline de preprocesamiento y modelado con Stacking\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Modelos base para el Stacking\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),\n",
    "    ('svr', SVR(kernel='rbf'))\n",
    "]\n",
    "\n",
    "# Meta-modelo final\n",
    "final_estimator = Ridge()\n",
    "\n",
    "# Definir el StackingRegressor\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator\n",
    ")\n",
    "\n",
    "# Construir el pipeline asegurando que se preserven las etiquetas de columna\n",
    "pipeline_stacking = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('dataframe_converter', DataFrameConverter(columns=X_train.columns)),\n",
    "    ('winsor', CustomWinsorizer(quantile_lower=0.01, quantile_upper=0.99)),\n",
    "    ('featcreator', FeatureCreator(use_log_transform=['X1'], create_interactions=True)),\n",
    "    ('scaler', DataFrameStandardScaler()),\n",
    "    ('feature_selector', MultiModelFeatureSelector(threshold=0.05, n_estimators=30)),\n",
    "    ('stack', stacking_reg)\n",
    "])\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Búsqueda de hiperparámetros con GridSearchCV\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "param_grid = {\n",
    "    'stack__rf__n_estimators': [50, 80],\n",
    "    'stack__gb__n_estimators': [50, 80],\n",
    "    'stack__final_estimator__alpha': [0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_stacking,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor puntuación (MSE negativo): {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Evaluación en el conjunto de prueba\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Métricas en Test ---\")\n",
    "print(f\"MSE  : {mse:.3f}\")\n",
    "print(f\"MAE  : {mae:.3f}\")\n",
    "print(f\"R^2  : {r2:.3f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Análisis de residuos y visualizaciones\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "residuos = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_pred, residuos, alpha=0.6)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicciones\")\n",
    "plt.ylabel(\"Residuos\")\n",
    "plt.title(\"Análisis de residuos - Stacking Regressor\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(residuos, bins=30, alpha=0.7)\n",
    "plt.title(\"Distribución de residuos\")\n",
    "plt.xlabel(\"Residuo\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \n",
    "         color='red', linestyle='--')  # Línea ideal y=x\n",
    "plt.xlabel(\"Valores reales\")\n",
    "plt.ylabel(\"Predicciones\")\n",
    "plt.title(\"Valores reales vs. Predicciones\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizar las features seleccionadas\n",
    "selector = best_model.named_steps['feature_selector']\n",
    "print(\"\\nFeatures seleccionadas:\")\n",
    "print(selector.selected_features_.tolist())\n",
    "\n",
    "print(\"\\nLas importancias medias utilizadas en la selección determinaron las features finales.\")\n",
    "print(\"Para más detalles, inspeccione manualmente los estimators en 'best_model.named_steps['stack'].estimators_'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf932fd-0d86-48a1-8079-e8eaca53293f",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### Ejercicio 1: Construcción de un pipeline con preprocesamiento mixto\n",
    "\n",
    "**Objetivo:**  \n",
    "Crear un pipeline que procese datos con variables numéricas y categóricas.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Utiliza un dataset (puede ser sintético o real) que contenga tanto variables numéricas como categóricas.  \n",
    "- Implementa un pipeline que incluya:  \n",
    "  - Imputación de valores faltantes en variables numéricas y categóricas.  \n",
    "  - Aplicación de un escalador robusto para variables numéricas.  \n",
    "  - Codificación one-hot para variables categóricas (puedes usar `OneHotEncoder` o una función personalizada).  \n",
    "- Realiza una búsqueda de hiperparámetros con `GridSearchCV` para ajustar un modelo de regresión (por ejemplo, Ridge o Lasso).  \n",
    "- Evalúa el rendimiento utilizando métricas como MSE y R2, y visualiza los residuos.\n",
    "\n",
    "#### Ejercicio 2: Desarrollo de un transformador personalizado para Winsorización  \n",
    "**Objetivo:**  \n",
    "Implementar y validar un transformador personalizado que realice Winsorización para el manejo de outliers.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Crea una clase que herede de `BaseEstimator` y `TransformerMixin`.  \n",
    "- La clase deberá recortar los valores de cada característica por debajo y por encima de percentiles definidos (por ejemplo, 5% y 95%).  \n",
    "- Integra este transformador en un pipeline junto con un escalador y un modelo (por ejemplo, RandomForestRegressor).  \n",
    "- Entrena el pipeline en un conjunto de datos sintético con outliers intencionales y compara el rendimiento del modelo con y sin Winsorización.\n",
    "\n",
    "\n",
    "#### Ejercicio 3: Pipeline con ingeniería de características y stacking regressor  \n",
    "**Objetivo:**  \n",
    "Combinar transformaciones personalizadas con un modelo de ensamblaje (Stacking) en un pipeline.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Genera un dataset sintético que contenga relaciones no lineales y algunas interacciones entre las variables.  \n",
    "- Crea un transformador personalizado que genere nuevas variables (por ejemplo, transformaciones logarítmicas, interacciones o términos polinómicos).  \n",
    "- Construye un pipeline que incluya:  \n",
    "  - Imputación de datos.  \n",
    "  - Transformación personalizada de características.  \n",
    "  - Escalado de datos.  \n",
    "  - Un `StackingRegressor` que combine, al menos, tres modelos base (por ejemplo, RandomForest, SVR y GradientBoosting) y un meta-modelo lineal.  \n",
    "- Realiza una búsqueda de hiperparámetros utilizando `GridSearchCV` para ajustar parámetros en los modelos base y el meta-modelo.  \n",
    "- Evalúa el pipeline en un conjunto de prueba, calculando métricas como MSE, MAE y R2, y visualiza los resultados.\n",
    "\n",
    "\n",
    "#### Ejercicio 4: Comparación de pipelines y evaluación de modelos  \n",
    "**Objetivo:**  \n",
    "Comparar diferentes enfoques de preprocesamiento y modelos en un mismo conjunto de datos.\n",
    "\n",
    "**Instrucciones:**  \n",
    "- Selecciona un dataset que pueda ser utilizado para regresión o clasificación.  \n",
    "- Construye al menos dos pipelines distintos, por ejemplo:  \n",
    "  - Pipeline A: Con preprocesamiento básico (imputación, escalado) y un modelo simple (como regresión lineal o SVM).  \n",
    "  - Pipeline B: Con transformadores personalizados (por ejemplo, winsorización, ingeniería de características) y un modelo más complejo (por ejemplo, un ensamble o un StackingRegressor/Classifier).  \n",
    "- Usa `GridSearchCV` para ajustar los hiperparámetros de ambos pipelines.  \n",
    "- Evalúa el rendimiento de cada pipeline con métricas apropiadas y realiza un análisis comparativo (por ejemplo, mediante validación cruzada, análisis de residuos, curvas ROC en el caso de clasificación, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d520a2-dfed-4ed5-9ce0-a1adb65055a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
