{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed28af3-82a6-4ae4-b001-1a2659c21c6f",
   "metadata": {},
   "source": [
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5943e55-c912-418f-ba56-06be4dc0ff84",
   "metadata": {},
   "source": [
    "\n",
    "**1. Implementación de perceptrón multicapa desde cero**  \n",
    "\n",
    "*Enunciado:*  \n",
    "Desarrolla una red neuronal feedforward simple utilizando únicamente NumPy. La red debe incluir al menos una capa oculta, implementar la función de activación sigmoide (o ReLU), y utilizar el algoritmo de backpropagation para actualizar los pesos. Incorpora además técnicas de regularización, como el dropout o regularización L2, para prevenir el sobreajuste.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Funciones de activación y sus derivadas\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Datos de entrenamiento (por ejemplo, operación XOR)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Inicialización de parámetros\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 3)    # Capa oculta: 3 neuronas\n",
    "b1 = np.zeros((1, 3))\n",
    "W2 = np.random.randn(3, 1)    # Capa de salida: 1 neurona\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epocas = 10000\n",
    "\n",
    "for epoca in range(epocas):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # Cálculo del error y retropropagación\n",
    "    error = y - a2\n",
    "    delta2 = error * sigmoid_deriv(z2)\n",
    "    error_hidden = np.dot(delta2, W2.T)\n",
    "    delta1 = error_hidden * sigmoid_deriv(z1)\n",
    "    \n",
    "    # Actualización de pesos (con posible incorporación de L2: agregar término lambda*W)\n",
    "    W2 += learning_rate * np.dot(a1.T, delta2)\n",
    "    b2 += learning_rate * np.sum(delta2, axis=0, keepdims=True)\n",
    "    W1 += learning_rate * np.dot(X.T, delta1)\n",
    "    b1 += learning_rate * np.sum(delta1, axis=0, keepdims=True)\n",
    "    \n",
    "    # (Opcional) Aplicar dropout en a1 durante el entrenamiento\n",
    "\n",
    "if epoca % 1000 == 0:\n",
    "    print(f\"epoca {epoca}, Error: {np.mean(np.abs(error))}\")\n",
    "\n",
    "print(\"Predicciones finales:\", a2)\n",
    "```\n",
    "\n",
    "\n",
    "**2. Optimización en redes feedforward con técnicas avanzadas**  \n",
    "*Enunciado:*  \n",
    "Crea una red neuronal de múltiples capas utilizando PyTorch y experimenta con diferentes optimizadores (por ejemplo, Adam, RMSProp) y funciones de activación (ReLU, LeakyReLU, etc.). Realiza un seguimiento del desempeño en cada caso y comenta las diferencias en la convergencia.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definición del modelo\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "modelo = FeedforwardNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(modelo.parameters(), lr=0.001)\n",
    "\n",
    "# Datos de ejemplo\n",
    "x = torch.randn(100, 10)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoca in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = modelo(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoca % 10 == 0:\n",
    "        print(f'epoca {epoca+1}, Loss: {loss.item()}')\n",
    "```\n",
    "\n",
    "\n",
    "**3. Construcción de una red neuronal básica sin librerías de deep learning**  \n",
    "*Enunciado:*  \n",
    "Implementa una red neuronal para clasificación utilizando únicamente operaciones de NumPy, sin apoyarte en frameworks como TensorFlow o PyTorch. La red deberá incluir propagación hacia adelante, cálculo manual del gradiente y actualización de pesos mediante descenso del gradiente.  \n",
    "\n",
    "\n",
    "\n",
    "**4. Implementación manual de una CNN básica**  \n",
    "*Enunciado:*  \n",
    "Programa desde cero las operaciones de convolución y pooling para construir una red neuronal convolucional simple. El ejercicio debe incluir la implementación de una función de convolución 2D, una función de pooling (por ejemplo, max pooling) y la integración de estas operaciones en una red capaz de clasificar imágenes simples (por ejemplo, del dataset MNIST).  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def conv2d(image, kernel):\n",
    "    # Suponiendo imagen cuadrada y kernel de tamaño 3x3\n",
    "    kernel = np.flipud(np.fliplr(kernel))  # rotación del kernel\n",
    "    output = np.zeros_like(image)\n",
    "    image_padded = np.pad(image, pad_width=1, mode='constant')\n",
    "    for x in range(image.shape[0]):\n",
    "        for y in range(image.shape[1]):\n",
    "            output[x, y] = (kernel * image_padded[x:x+3, y:y+3]).sum()\n",
    "    return output\n",
    "\n",
    "def max_pooling(image, size=2, stride=2):\n",
    "    output_shape = (image.shape[0] // size, image.shape[1] // size)\n",
    "    pooled = np.zeros(output_shape)\n",
    "    for i in range(0, image.shape[0], stride):\n",
    "        for j in range(0, image.shape[1], stride):\n",
    "            pooled[i//stride, j//stride] = np.max(image[i:i+size, j:j+size])\n",
    "    return pooled\n",
    "\n",
    "# Ejemplo de uso:\n",
    "image = np.random.rand(5, 5)\n",
    "kernel = np.array([[1, 0, -1],\n",
    "                   [1, 0, -1],\n",
    "                   [1, 0, -1]])\n",
    "conv_result = conv2d(image, kernel)\n",
    "pool_result = max_pooling(conv_result)\n",
    "print(\"Convolución:\\n\", conv_result)\n",
    "print(\"Max Pooling:\\n\", pool_result)\n",
    "```\n",
    "\n",
    "\n",
    "**5. Replicación de la arquitectura LeNet**  \n",
    "*Enunciado:*  \n",
    "Utilizando PyTorch, implementa la arquitectura clásica LeNet para clasificación de imágenes (por ejemplo, en MNIST). Asegúrate de definir las capas convolucionales, de pooling y las capas totalmente conectadas de acuerdo con la arquitectura original.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Capa convolucional 1: de 1 canal a 6 canales, kernel de 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Capa convolucional 2: de 6 a 16 canales\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Capas totalmente conectadas\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "\n",
    "**6. Desarrollo de una versión simplificada de AlexNet**  \n",
    "*Enunciado:*  \n",
    "Implementa una versión simplificada de AlexNet utilizando PyTorch. El modelo debe incluir varias capas convolucionales con funciones de activación ReLU, capas de pooling y una sección de clasificador totalmente conectado, incorporando técnicas de regularización como dropout.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**7. Implementación de modelos inspirados en VGG y ResNet**  \n",
    "\n",
    "*Enunciado:*  \n",
    "\n",
    "Diseña dos modelos:  \n",
    "\n",
    "- Uno inspirado en la arquitectura VGG (por ejemplo, VGG16 simplificado) que utilice bloques de convolución y pooling secuenciales.  \n",
    "- Otro basado en ResNet (por ejemplo, ResNet18) que incorpore bloques residuales para facilitar el entrenamiento en arquitecturas profundas.  \n",
    "\n",
    "Compara el desempeño de ambos modelos en una tarea de clasificación de imágenes.\n",
    "\n",
    "*Código de referencia (VGG simplificado):*\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Se pueden agregar más bloques similares para aumentar la profundidad\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 16 * 16, 4096),  # Ajustar según el tamaño de entrada\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "*Código de referencia (ResNet18 simplificado):*\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = [BasicBlock(in_channels, out_channels, stride, downsample)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "\n",
    "**8. Desarrollo de una RNN básica para series temporales**  \n",
    "\n",
    "*Enunciado:*  \n",
    "Implementa una red neuronal recurrente simple utilizando PyTorch para predecir series temporales. La red debe manejar secuencias de datos y abordar problemas asociados al desvanecimiento o explosión del gradiente (por ejemplo, mediante la normalización de gradientes).  \n",
    "\n",
    "*Código de referencia:*\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        # Se utiliza la última salida de la secuencia\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "modelo = SimpleRNN(input_size=1, hidden_size=10, output_size=1)\n",
    "print(modelo)\n",
    "```\n",
    "\n",
    "**9. Aplicación de RNN en procesamiento de lenguaje natural**  \n",
    "\n",
    "*Enunciado:*  \n",
    "Diseña una RNN que procese secuencias de texto para tareas de clasificación (por ejemplo, análisis de sentimientos o detección de spam). El modelo debe incluir una capa de embedding para transformar las palabras a vectores y luego procesar la secuencia con una capa recurrente.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "modelo = TextRNN(vocab_size=5000, embed_size=128, hidden_size=64, num_classes=2)\n",
    "print(modelo)\n",
    "```\n",
    "\n",
    "**10. Creación de una red LSTM desde cero**\n",
    "\n",
    "*Enunciado:*  \n",
    "\n",
    "Implementa una red LSTM utilizando PyTorch. La red debe incluir la definición de la capa LSTM y una capa de salida que se utilice para tareas de predicción en series temporales o clasificación de secuencias. Explica el rol de las puertas (entrada, olvido y salida).  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "modelo = LSTMModel(input_size=10, hidden_size=50, num_layers=2, num_classes=1)\n",
    "print(modelo)\n",
    "```\n",
    "\n",
    "\n",
    "**11. Implementación y comparación de una red GRU**  \n",
    "\n",
    "*Enunciado:*  \n",
    "Desarrolla una red GRU para la predicción o clasificación de secuencias, utilizando PyTorch. Compara su rendimiento y eficiencia computacional con una red LSTM (usando arquitecturas similares y el mismo conjunto de datos).  \n",
    "\n",
    "*Código de referencia:*\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "modelo = GRUModelo(input_size=10, hidden_size=50, num_layers=2, num_classes=1)\n",
    "print(modelo)\n",
    "```\n",
    "\n",
    "\n",
    "**12. Desarrollo de una RNN bidireccional para análisis de sentimientos**  \n",
    "\n",
    "*Enunciado:*  \n",
    "\n",
    "Crea una red neuronal recurrente bidireccional utilizando PyTorch para una tarea de análisis de sentimientos. El modelo debe incluir una capa de embedding, una capa recurrente bidireccional y una capa totalmente conectada para la clasificación final. Se debe resaltar la ventaja de procesar la información en ambas direcciones.  \n",
    "\n",
    "*Código de referencia:*\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # La capa RNN bidireccional duplica el tamaño del estado oculto\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "modelo = BiRNN(vocab_size=5000, embed_size=128, hidden_size=64, num_classes=2)\n",
    "print(modelo)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd747d-a83a-4d5b-b91d-9d53679b6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
