{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077d5229-bb26-4c66-8d4c-7806a3faa744",
   "metadata": {},
   "source": [
    "### Introducción a los autoencoders\n",
    "\n",
    "Los autoencoders son modelos de redes neuronales diseñados para aprender representaciones compactas y útiles de los datos. Se basan en la idea de entrenar una red para que, a partir de una entrada, genere una reconstrucción lo más fiel posible a la misma. La estructura típica de un autoencoder se compone de dos partes principales: el codificador y el decodificador.\n",
    "\n",
    "#### Estructura y funcionamiento básico\n",
    "\n",
    "- **Codificador:**  \n",
    "  El objetivo del codificador es transformar la entrada $x$ en una representación de menor dimensión, también denominada \"código\" o \"embedding\". Este proceso se puede expresar matemáticamente como:\n",
    "  \n",
    "  $$\n",
    "  z = f_{\\theta}(x)\n",
    "  $$\n",
    "  \n",
    "  donde $f_{\\theta} $ representa una función no lineal parametrizada por $\\theta $ (los pesos y sesgos de la red), y $ z$ es la representación latente.\n",
    "\n",
    "- **Decodificador:**  \n",
    "  La función del decodificador es reconstruir la entrada original a partir del código $z$, aproximando así $ x$ por medio de:\n",
    "  \n",
    "  $$\n",
    "  \\hat{x} = g_{\\phi}(z)\n",
    "  $$\n",
    "  \n",
    "  donde $ g_{\\phi} $ es otra función no lineal parametrizada por $ \\phi $. La meta es que la salida $ \\hat{x} $ sea lo más similar posible a la entrada $ x $.\n",
    "\n",
    "- **Función de pérdida:**  \n",
    "  El entrenamiento de un autoencoder se basa en la minimización de la diferencia entre la entrada y su reconstrucción. Comúnmente se utiliza una función de pérdida como el error cuadrático medio (MSE) para problemas continuos o la entropía cruzada para datos categóricos, de manera que:\n",
    "  \n",
    "  $$\n",
    "  L(x, \\hat{x}) = \\|x - \\hat{x}\\|^2 \\quad \\text{o} \\quad L(x, \\hat{x}) = - \\sum_{i} x_i \\log (\\hat{x}_i)\n",
    "  $$\n",
    "  \n",
    "  La función de pérdida se optimiza respecto a los parámetros $\\theta$ y $ \\phi $ de la red.\n",
    "\n",
    "#### Objetivos y ventajas de utilizar autoencoders\n",
    "\n",
    "El principal objetivo de los autoencoders es aprender una representación comprimida de los datos que conserve la información esencial para reconstruirlos. Entre las ventajas se destacan:\n",
    "\n",
    "- **Reducción de dimensionalidad:**  \n",
    "  Al aprender una codificación de menor dimensión, los autoencoders permiten obtener una representación compacta que facilita el almacenamiento y procesamiento de los datos, similar a técnicas clásicas como el Análisis de Componentes Principales (PCA), pero con la ventaja de capturar relaciones no lineales.\n",
    "\n",
    "- **Eliminación de ruido:**  \n",
    "  Mediante el entrenamiento, los autoencoders pueden aprender a eliminar el ruido presente en los datos, obteniendo reconstrucciones más limpias y útiles para tareas de preprocesamiento.\n",
    "\n",
    "- **Aprendizaje no supervisado:**  \n",
    "  Al no requerir etiquetas en el proceso de entrenamiento, los autoencoders se sitúan en el ámbito del aprendizaje no supervisado, lo que resulta especialmente útil en situaciones en las que los datos etiquetados son escasos o inexistentes.\n",
    "\n",
    "- **Preentrenamiento de redes profundas:**  \n",
    "  Los autoencoders también se utilizan como etapa de preentrenamiento para redes neuronales profundas, permitiendo inicializar los pesos de manera que se capture la estructura subyacente de los datos antes de afinar la red en una tarea supervisada.\n",
    "\n",
    "#### Variantes de autoencoders\n",
    "\n",
    "Existen diversas variantes que se han propuesto para ampliar la capacidad y aplicabilidad de los autoencoders, como los autoencoders denoising (que aprenden a reconstruir datos limpios a partir de entradas corruptas) y los autoencoders esparsos (que introducen restricciones para fomentar una representación latente dispersa). Estas variantes abren la puerta a aplicaciones en diferentes dominios y mejoran la calidad de las representaciones aprendidas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6528886-4d58-48d0-84ec-3e8e1e87b93f",
   "metadata": {},
   "source": [
    "### Autoencoders variacionales (VAE)\n",
    "\n",
    "Los autoencoders variacionales representan una extensión de los autoencoders clásicos que incorpora un marco probabilístico en el proceso de codificación. Esta aproximación no solo permite obtener una representación comprimida, sino que también facilita la generación de nuevos datos a partir de la distribución aprendida en el espacio latente.\n",
    "\n",
    "#### Motivación y fundamento teórico\n",
    "\n",
    "El objetivo de los VAE es modelar la distribución subyacente de los datos de manera tal que, a partir de un muestreo en el espacio latente, se puedan generar nuevos ejemplos que pertenezcan al mismo dominio. En lugar de aprender una función determinista que mapea $ x $ a $ z $, se aprende una distribución probabilística $q_{\\phi}(z|x)$ que describe la probabilidad de que un dato $ x $ se asocie a un vector latente $ z $.\n",
    "\n",
    "- **Distribución latente:**  \n",
    "  Se asume comúnmente que la variable latente $z$ sigue una distribución normal multivariada $\\mathcal{N}(0, I) $. Durante el entrenamiento, el codificador aprende a aproximar $q_{\\phi}(z|x)$ mediante parámetros que definen una media $ \\mu $ y una varianza $ \\sigma^2 $ (o log-varianza).\n",
    "\n",
    "- **Reparametrización:**  \n",
    "  Un aspecto clave en el entrenamiento de los VAE es el truco de reparametrización, el cual permite que el muestreo de $ z $ sea diferenciable. En lugar de muestrear $z$ directamente, se reparametriza como:\n",
    "  \n",
    "  $$\n",
    "  z = \\mu(x) + \\sigma(x) \\cdot \\epsilon\n",
    "  $$\n",
    "  \n",
    "donde $ \\epsilon \\sim \\mathcal{N}(0, I) $ es una variable de ruido extraída de una distribución normal estándar. Esto permite que el gradiente fluya a través de la media y la varianza durante el proceso de entrenamiento.\n",
    "\n",
    "#### Función de pérdida en VAE\n",
    "\n",
    "La función de pérdida en los VAE combina dos términos fundamentales:\n",
    "\n",
    "1. **Término de reconstrucción:**  \n",
    "   Este término mide cuán bien el decodificador es capaz de reconstruir la entrada a partir del código $ z $. La forma más habitual es utilizar una función de pérdida similar a la de los autoencoders clásicos, como el error cuadrático medio (MSE) o la entropía cruzada, dependiendo del tipo de datos.\n",
    "\n",
    "2. **Término de regularización (KL Divergence):**  \n",
    "   Se introduce una penalización basada en la divergencia de Kullback-Leibler (KL) entre la distribución aproximada $ q_{\\phi}(z|x) $ y la distribución prior $ p(z) $ (usualmente $ \\mathcal{N}(0, I) $). Este término se expresa como:\n",
    "   \n",
    "   $$\n",
    "   D_{KL}\\left( q_{\\phi}(z|x) \\,\\|\\, p(z) \\right)\n",
    "   $$\n",
    "   \n",
    "y su objetivo es forzar a la distribución latente a aproximarse a la distribución normal, lo que permite realizar muestreos coherentes durante la generación de nuevos datos.\n",
    "\n",
    "La función de pérdida total se puede resumir de la siguiente manera:\n",
    "\n",
    "$$\n",
    "L(x, \\hat{x}) = \\mathbb{E}_{q_{\\phi}(z|x)}\\left[ \\text{ReconLoss}(x, \\hat{x}) \\right] + D_{KL}\\left( q_{\\phi}(z|x) \\,\\|\\, p(z) \\right)\n",
    "$$\n",
    "\n",
    "Este balance entre el término de reconstrucción y la divergencia KL es esencial para que el modelo no solo aprenda a reconstruir los datos, sino que también estructure el espacio latente de manera que permita la generación de ejemplos nuevos y coherentes.\n",
    "\n",
    "#### Arquitectura de un VAE\n",
    "\n",
    "La arquitectura de un Autoencoder Variacional es similar en estructura a la de un autoencoder clásico, pero con algunas modificaciones importantes:\n",
    "\n",
    "- **Codificador probabilístico:**  \n",
    "  El codificador en un VAE no produce un vector único $ z $ sino dos vectores: $ \\mu(x) $ y $ \\log \\sigma^2(x) $, que definen la distribución $ q_{\\phi}(z|x) $.\n",
    "\n",
    "- **Muestreo reparametrizado:**  \n",
    "  Con el truco de reparametrización, se genera el vector latente $ z $ a partir de la distribución aprendida, lo que permite la diferenciabilidad durante el entrenamiento.\n",
    "\n",
    "- **Decodificador generativo:**  \n",
    "  El decodificador toma el vector $ z $ muestreado y genera una reconstrucción $ \\hat{x} $ de la entrada. Este proceso implica aprender la distribución condicional $ p_{\\theta}(x|z) $.\n",
    "\n",
    "La arquitectura completa permite que, una vez entrenado, el modelo genere nuevos datos extrayendo muestras de la distribución prior $ p(z) $ y pasándolas a través del decodificador. Esto habilita tareas de generación de datos, interpolación en el espacio latente y exploración de la estructura interna de los datos.\n",
    "\n",
    "#### Entrenamiento y optimización de VAE\n",
    "\n",
    "El entrenamiento de un VAE se realiza mediante métodos de optimización estándar (como descenso del gradiente estocástico o Adam), donde se minimiza la función de pérdida descrita anteriormente. La inclusión de la divergencia KL no solo actúa como regularizador, sino que además estructura el espacio latente, facilitando la interpolación y generación de ejemplos.\n",
    "\n",
    "- **Balance entre reconstrucción y regularización:**  \n",
    "  Durante el entrenamiento, es común observar que un balance adecuado entre el término de reconstrucción y la penalización KL es crucial. Un peso excesivo en la divergencia KL puede hacer que el modelo se enfoque en ajustar el espacio latente a la distribución prior, a expensas de una mala reconstrucción; mientras que un énfasis demasiado fuerte en la reconstrucción puede llevar a un sobreajuste, perdiendo la capacidad generativa.\n",
    "\n",
    "- **Estrategias de warm-up:**  \n",
    "  Algunas técnicas implican comenzar con un peso menor para el término KL e incrementarlo gradualmente durante las primeras épocas de entrenamiento. Esto permite que el modelo se enfoque inicialmente en la reconstrucción y luego integre progresivamente la regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba099a-5046-46db-b85a-769836269299",
   "metadata": {},
   "source": [
    "### Aplicaciones en generación de datos y reducción de dimensionalidad\n",
    "\n",
    "Los autoencoders y, en particular, los autoencoders variacionales han abierto nuevas posibilidades en el ámbito del procesamiento de datos. Entre sus aplicaciones más relevantes se encuentran la generación de datos y la reducción de dimensionalidad.\n",
    "\n",
    "#### Generación de datos\n",
    "\n",
    "Una de las características más innovadoras de los VAE es su capacidad para generar nuevos ejemplos que siguen la distribución de los datos de entrenamiento.\n",
    "\n",
    "- **Generación de imágenes:**  \n",
    "  Al entrenar un VAE con un conjunto de imágenes, el modelo aprende una representación latente que captura características importantes como formas, texturas y patrones de color. Una vez entrenado, es posible muestrear del espacio latente y utilizar el decodificador para generar imágenes nuevas que, aunque no sean réplicas exactas de las imágenes de entrenamiento, comparten características similares. Esta técnica se ha utilizado en aplicaciones artísticas, en la síntesis de rostros humanos y en la creación de contenido digital.\n",
    "\n",
    "- **Interpolación en el espacio latente:**  \n",
    "  Otra aplicación interesante es la interpolación entre dos puntos del espacio latente. Dado dos vectores latentes correspondientes a dos imágenes distintas, se puede generar una secuencia de vectores intermedios que, al ser decodificados, producen una transición suave entre las dos imágenes originales. Esto es útil en animación, transformación de estilos y exploración de variaciones en los datos.\n",
    "\n",
    "- **Generación de texto y música:**  \n",
    "  Aunque el uso más extendido de los VAE se ha dado en el ámbito de las imágenes, también se han adaptado para la generación de secuencias en dominios como el texto y la música. En estos casos, el decodificador aprende a generar secuencias que respetan la estructura y la coherencia del lenguaje o del ritmo musical, permitiendo aplicaciones en la creación de contenido literario o composiciones musicales automatizadas.\n",
    "\n",
    "- **Aplicaciones en ciencia y medicina:**  \n",
    "  En campos como la bioinformática, los VAE se han utilizado para generar datos sintéticos que ayudan a simular escenarios en los que los datos reales son difíciles de obtener o para ampliar conjuntos de datos en estudios clínicos. La generación de datos sintéticos puede servir para validar modelos o para realizar análisis exploratorios en entornos de alta dimensionalidad.\n",
    "\n",
    "#### Reducción de dimensionalidad\n",
    "\n",
    "Otra aplicación fundamental de los autoencoders es la reducción de dimensionalidad, ya que el espacio latente aprendido representa una versión comprimida y, en muchos casos, desprovista de redundancias de los datos originales.\n",
    "\n",
    "- **Comparación con métodos tradicionales:**  \n",
    "  Mientras que técnicas clásicas como PCA se basan en transformaciones lineales, los autoencoders permiten capturar relaciones no lineales, lo cual resulta especialmente útil en conjuntos de datos complejos. La representación latente puede preservar estructuras intrínsecas que son difíciles de detectar mediante métodos lineales.\n",
    "\n",
    "- **Visualización de datos:**  \n",
    "  En aplicaciones de análisis exploratorio, es común reducir la dimensionalidad a dos o tres dimensiones para facilitar la visualización de la distribución de los datos. Un autoencoder bien entrenado puede extraer características que permiten identificar clusters, tendencias y outliers, proporcionando insights valiosos en el análisis de grandes volúmenes de información.\n",
    "\n",
    "- **Preprocesamiento para tareas supervisadas:**  \n",
    "  La reducción de dimensionalidad mediante autoencoders se utiliza frecuentemente como una etapa de preprocesamiento en pipelines de aprendizaje supervisado. Al reducir la dimensionalidad, se disminuye la complejidad del modelo final y se mejora la eficiencia del entrenamiento, manteniendo las características más relevantes para la tarea de clasificación o regresión.\n",
    "\n",
    "- **Detección de anomalías:**  \n",
    "  Al entrenar un autoencoder con datos considerados “normales”, se puede utilizar la capacidad del modelo para reconstruir estos datos. Las entradas que no puedan ser reconstruidas de manera precisa pueden considerarse anomalías. Esta técnica se aplica en la detección de fraudes, en la monitorización de sistemas industriales y en la identificación de comportamientos inusuales en series temporales.\n",
    "\n",
    "#### Integración de modelos variacionales en aplicaciones prácticas\n",
    "\n",
    "La combinación de la capacidad generativa y la reducción de dimensionalidad en los VAE permite diseñar sistemas robustos para múltiples aplicaciones:\n",
    "\n",
    "- **Sistemas de recomendación y filtrado colaborativo:**  \n",
    "  La representación latente extraída por un VAE puede capturar las preferencias y características subyacentes de los usuarios y productos. Esto facilita la generación de recomendaciones personalizadas en plataformas de comercio electrónico y servicios de streaming, mejorando la experiencia del usuario.\n",
    "\n",
    "- **Compresión y transmisión de datos:**  \n",
    "  En escenarios donde la transmisión de grandes volúmenes de datos es costosa o limitada, la capacidad de comprimir la información en un espacio latente de menor dimensión resulta valiosa. Los VAE pueden utilizarse para codificar datos de alta resolución en representaciones más compactas que se pueden transmitir y luego reconstruir en el destino con una pérdida mínima de calidad.\n",
    "\n",
    "- **Aprendizaje no supervisado en grandes conjuntos de datos:**  \n",
    "  La naturaleza no supervisada de los autoencoders variacionales los hace adecuados para el análisis de datos en entornos donde no se dispone de etiquetas. Esta capacidad permite descubrir patrones ocultos y relaciones complejas en datos de sensores, imágenes, texto y otros dominios, facilitando el desarrollo de sistemas de inteligencia artificial más autónomos.\n",
    "\n",
    "- **Transferencia de conocimientos y fine-tuning:**  \n",
    "  Las representaciones aprendidas por un VAE pueden servir como punto de partida para tareas supervisadas, permitiendo la transferencia de conocimiento a modelos que se ajustan a tareas específicas. Esto es especialmente útil en situaciones donde el etiquetado de datos es costoso o cuando se requiere una adaptación rápida a nuevos dominios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082451c-9b92-4723-899c-80e384ecbe74",
   "metadata": {},
   "source": [
    "La integración de estos modelos en aplicaciones prácticas ha llevado a innovaciones en sistemas de recomendación, compresión de datos, análisis de series temporales y detección de anomalías, entre otros campos. La capacidad de los VAE para capturar las características esenciales de los datos y transformar esa información en un espacio latente de menor dimensión ha demostrado ser una herramienta poderosa para enfrentar desafíos en diversos sectores, desde la informática y el comercio electrónico hasta la medicina y la ingeniería.\n",
    "\n",
    "La versatilidad de los autoencoders variacionales se refleja en su capacidad para adaptarse a diferentes tipos de datos y en su integración en sistemas híbridos, donde se combinan con otras técnicas de aprendizaje profundo para potenciar el rendimiento y la interpretabilidad. Este enfoque permite construir modelos que no solo son capaces de aprender representaciones significativas, sino que también pueden generalizar y generar nuevos ejemplos, abriendo la puerta a aplicaciones innovadoras en áreas emergentes de la inteligencia artificial.\n",
    "\n",
    "La continua evolución de estas técnicas, junto con la integración de avances en la optimización y la escalabilidad, promete ampliar aún más las posibilidades de los autoencoders y los modelos variacionales en el futuro. El desarrollo de métodos que permitan una mayor eficiencia computacional y una mejor interpretación de las representaciones latentes seguirá impulsando la aplicación de estos modelos en escenarios de alta complejidad y gran volumen de datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
