{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca91171c-58ba-437c-a1da-0cce67ac1345",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "\n",
    "#### 1. Agente de aprendizaje por refuerzo profundo con PPO en OpenAI Gym\n",
    "\n",
    "**Descripción:**  \n",
    "\n",
    "Implementa un agente que utilice el algoritmo Proximal Policy Optimization (PPO) para resolver un entorno de OpenAI Gym (por ejemplo, \"CartPole-v1\"). El ejercicio consiste en configurar el entorno, entrenar al agente y evaluar su desempeño.\n",
    "\n",
    "**Código de referencia (usando `stable-baselines3`):**\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Crear el entorno\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Inicializar el agente PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "# Evaluar el agente\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "env.close()\n",
    "```\n",
    "\n",
    "\n",
    "#### 2. Algoritmo basado en DPO (Direct Preference Optimization)\n",
    "\n",
    "**Descripción:**  \n",
    "Crea un algoritmo en el que se compare la salida de un modelo para pares de entradas y se optimice directamente la política en función de las preferencias obtenidas (simuladas o reales). Se debe implementar la función de pérdida que minimice la diferencia entre las salidas preferidas y las no preferidas.\n",
    "\n",
    "**Código de referencia (pseudocódigo con PyTorch):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir una red simple (por ejemplo, para generar una salida a partir de una entrada)\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Función de pérdida basada en preferencias (simulada)\n",
    "def dpo_loss(preferred_output, non_preferred_output):\n",
    "    # Queremos que la salida preferida sea mayor que la no preferida\n",
    "    # Se puede usar un margen\n",
    "    margin = 1.0\n",
    "    loss = torch.mean(torch.clamp(margin - (preferred_output - non_preferred_output), min=0))\n",
    "    return loss\n",
    "\n",
    "# Parámetros de ejemplo\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "model = PolicyNet(input_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Ejemplo de datos simulados\n",
    "x_preferred = torch.randn(32, input_dim)\n",
    "x_non_preferred = torch.randn(32, input_dim)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output_pref = model(x_preferred)\n",
    "    output_nonpref = model(x_non_preferred)\n",
    "    loss = dpo_loss(output_pref, output_nonpref)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n",
    "```\n",
    "\n",
    "#### 3. Pipeline de RLHF para afinar un modelo de lenguaje\n",
    "\n",
    "**Descripción:**  \n",
    "Crea un pipeline que integre la retroalimentación humana para afinar un modelo de lenguaje. Se deberá recopilar (o simular) feedback para generar datos de preferencia, entrenar un modelo de recompensa y luego ajustar la política del modelo generativo usando un algoritmo de RL (por ejemplo, PPO).\n",
    "\n",
    "**Código de referencia (fragmento con Hugging Face y pseudocódigo):**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar un modelo preentrenado (por ejemplo, GPT-2)\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Simular generación de respuestas para una entrada dada\n",
    "input_text = \"El futuro de la inteligencia artificial es\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50, do_sample=True)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Respuesta generada:\", generated_text)\n",
    "\n",
    "# SUPUESTAMENTE, se recolectan comparaciones de pares de respuestas (esto se simula)\n",
    "# En la práctica, se usarían datos etiquetados por humanos.\n",
    "# Luego, se entrena un modelo de recompensa y se utiliza PPO para ajustar el modelo generativo.\n",
    "# Aquí se muestra un pseudocódigo para la optimización:\n",
    "\"\"\"\n",
    "for each training step:\n",
    "    generate multiple responses for an input\n",
    "    collect human preferences (or simulated preferences)\n",
    "    compute reward based on preferences using the reward model\n",
    "    update the generative model parameters using PPO based on the computed rewards\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### 4. Modelo de difusión para generación de imágenes\n",
    "\n",
    "**Descripción:**  \n",
    "Diseña e implementa un modelo de difusión que transforme gradualmente una muestra de ruido gaussiano en una imagen realista. El ejercicio implica definir el proceso de forward diffusion (añadir ruido progresivamente) y el proceso inverso de denoising.\n",
    "\n",
    "**Código de referencia (pseudocódigo usando PyTorch):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ejemplo de un bloque simple de denoising\n",
    "class DenoisingBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(DenoisingBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + x\n",
    "\n",
    "# Definir un modelo de difusión simple\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, channels, num_steps):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.denoising_blocks = nn.ModuleList([DenoisingBlock(channels) for _ in range(num_steps)])\n",
    "    \n",
    "    def forward(self, noisy_image):\n",
    "        x = noisy_image\n",
    "        for block in self.denoising_blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "# Proceso de muestreo\n",
    "def sample_image(model, shape, num_steps):\n",
    "    # Comenzar con ruido gaussiano\n",
    "    x = torch.randn(shape)\n",
    "    for step in range(num_steps):\n",
    "        # En cada paso, aplicar el bloque de denoising\n",
    "        x = model(x)\n",
    "    return x\n",
    "\n",
    "# Parámetros de ejemplo\n",
    "channels = 3\n",
    "num_steps = 5\n",
    "model = DiffusionModel(channels, num_steps)\n",
    "\n",
    "# Generar una imagen (por ejemplo, 64x64 RGB)\n",
    "generated_image = sample_image(model, (1, channels, 64, 64), num_steps)\n",
    "print(\"Imagen generada con forma:\", generated_image.shape)\n",
    "```\n",
    "\n",
    "#### 5. Fine-Tuning de un LLM basado en GPT usando Hugging Face\n",
    "\n",
    "**Descripción:**  \n",
    "Realiza el fine-tuning de un modelo de lenguaje basado en GPT (por ejemplo, GPT-2) en un corpus específico. El ejercicio incluye la carga del modelo y tokenizer, la preparación del dataset y la ejecución del entrenamiento.\n",
    "\n",
    "**Código de referencia (usando `transformers` y `datasets`):**\n",
    "\n",
    "```python\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el modelo y tokenizer preentrenados\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Cargar un dataset de ejemplo (puede ser uno personalizado)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Configurar argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Inicializar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Ejecutar el fine-tuning\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "#### 6. Análisis comparativo de LLM: GPT, BERT y T5\n",
    "\n",
    "**Descripción:**  \n",
    "Implementa un ejercicio que evalúe el desempeño de distintos LLM (por ejemplo, GPT-2, BERT y T5) en tareas como clasificación, generación y comprensión de texto. Se debe comparar la precisión, la coherencia y la eficiencia de cada modelo.\n",
    "\n",
    "**Código de referencia (usando Hugging Face pipelines):**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Inicializar pipelines para cada modelo\n",
    "gpt_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "bert_pipeline = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "t5_pipeline = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "# Ejemplo de generación con GPT-2\n",
    "print(\"GPT-2 Generación:\")\n",
    "print(gpt_pipeline(\"El futuro de la inteligencia artificial es\", max_length=50))\n",
    "\n",
    "# Ejemplo de enmascaramiento con BERT\n",
    "print(\"\\nBERT Fill-Mask:\")\n",
    "print(bert_pipeline(\"La capital de Francia es [MASK].\"))\n",
    "\n",
    "# Ejemplo de transformación con T5\n",
    "print(\"\\nT5 Text-to-Text:\")\n",
    "print(t5_pipeline(\"translate English to French: The house is wonderful.\"))\n",
    "```\n",
    "\n",
    "\n",
    "#### 7. Implementación de un sistema RAG (Retrieval Augmented Generation)\n",
    "\n",
    "**Descripción:**  \n",
    "Desarrolla un sistema que integre un modelo de recuperación de información con un modelo generativo. El ejercicio debe mostrar cómo, dado un input (por ejemplo, una pregunta), el sistema recupera documentos relevantes y utiliza esa información para generar una respuesta detallada.\n",
    "\n",
    "**Código de referencia (usando Hugging Face RAG):**\n",
    "\n",
    "```python\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "\n",
    "# Cargar el tokenizer y el modelo RAG\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever)\n",
    "\n",
    "# Definir una pregunta\n",
    "question = \"¿Cuál es la capital de Francia?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "# Generar una respuesta\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Respuesta generada:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "\n",
    "#### 8. Optimización y destilación de un LLM\n",
    "\n",
    "**Descripción:**  \n",
    "Crea un ejercicio avanzado en el que se reduzca el tamaño de un LLM preentrenado (por ejemplo, BERT o GPT-2) mediante técnicas de destilación y cuantización. Evalúa el desempeño del modelo reducido en tareas de NLP.\n",
    "\n",
    "**Código de referencia (usando Hugging Face Transformers y DistilBERT como ejemplo):**\n",
    "\n",
    "```python\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments, DistilBertConfig\n",
    "\n",
    "# Cargar el modelo y tokenizer de DistilBERT (modelo destilado de BERT)\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Cargar un dataset de ejemplo\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Configurar y ejecutar el entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Se puede aplicar técnicas adicionales de cuantización usando librerías como ONNX Runtime o Hugging Face Optimum\n",
    "```\n",
    "\n",
    "#### 9. Uso del modelo LLama y comparación en Hugging Face\n",
    "\n",
    "**Descripción:**  \n",
    "Implementa un proyecto en el que utilices el modelo LLama (o una versión similar disponible en Hugging Face) para la generación de texto. El ejercicio debe incluir la carga del modelo, generación de texto a partir de un prompt y la comparación del desempeño frente a otros LLM.\n",
    "\n",
    "**Código de referencia (usando Hugging Face, asumiendo que existe un modelo LLama):**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Cargar el modelo LLama (reemplazar \"model_llama\" por el identificador correcto)\n",
    "model_name = \"facebook/llama-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Generar texto a partir de un prompt\n",
    "prompt = \"La tecnología en el siglo XXI ha transformado\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100, do_sample=True)\n",
    "print(\"Texto generado:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "\n",
    "#### 10. Sistema combinado: optimización, destilación y RLHF para afinar un LLM\n",
    "\n",
    "**Descripción:**  \n",
    "Construye un sistema que integre optimización de modelos, técnicas de destilación y RLHF para afinar un LLM. El ejercicio consiste en crear un pipeline que recolecte retroalimentación (real o simulada), destile el conocimiento de un modelo grande y optimice la política mediante RL para lograr una mayor alineación con criterios humanos.\n",
    "\n",
    "**Código de referencia (pseudocódigo que integra componentes):**\n",
    "\n",
    "```python\n",
    "# Pseudocódigo integrador\n",
    "\n",
    "# Paso 1: Cargar el modelo preentrenado grande (por ejemplo, GPT-2) y recopilar retroalimentación\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_large = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# Simular generación de texto y retroalimentación\n",
    "prompt = \"Explique los beneficios de la inteligencia artificial en la medicina.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model_large.generate(**inputs, max_length=100)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Aquí se recopilaría la retroalimentación humana (simulada para este ejercicio)\n",
    "feedback = \"La respuesta es muy completa y precisa.\"\n",
    "\n",
    "# Paso 2: Entrenar un modelo de recompensa basado en la retroalimentación (pseudocódigo)\n",
    "# reward_model = EntrenarModeloDeRecompensa(generated_text, feedback)\n",
    "\n",
    "# Paso 3: Aplicar destilación para crear un modelo más pequeño (modelo estudiante)\n",
    "from transformers import DistilGPT2LMHeadModel\n",
    "model_student = DistilGPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Paso 4: Optimizar la política del modelo estudiante utilizando RL (por ejemplo, PPO)\n",
    "# Se integra un ciclo de entrenamiento que utiliza la recompensa estimada para ajustar el modelo\n",
    "\"\"\"\n",
    "for each training_step:\n",
    "    generar múltiples salidas con model_student\n",
    "    evaluar cada salida usando reward_model\n",
    "    calcular la ventaja y actualizar model_student usando PPO\n",
    "\"\"\"\n",
    "\n",
    "# Este pseudocódigo ilustra la integración de RLHF, destilación y optimización.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d5c66-9407-48d9-b6b5-c28635244168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
